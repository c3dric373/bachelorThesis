{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting and preprocessing the data\n",
    "First we get the dataset online, then apply subsampling, then divide the dataset in equally long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 s, sys: 662 ms, total: 20.6 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "import random\n",
    "from itertools import dropwhile\n",
    "\n",
    "def sampling(dataset,threshold=1e-4, min_count=5):\n",
    "    \n",
    "    # Count occurences of each word in the dataset \n",
    "    word_counts = Counter(dataset)  \n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in dataset if random.random() < (1 - p_drop[word]) and word_counts[word]>min_count]\n",
    "    return train_words\n",
    "\n",
    "\"Transforms a list of words to a list of sentences with length=len_sen\"\n",
    "def words_to_sentences(words):\n",
    "    new_ds = []\n",
    "    len_sen = 20\n",
    "    for i in range(0, len(words), len_sen):\n",
    "        y = [words[i:i + len_sen]]\n",
    "        new_ds.extend(y)\n",
    "    return new_ds\n",
    "    \n",
    "\n",
    "# Get dataset online\n",
    "dataset = api.load('text8')\n",
    "\n",
    "# Convert to list of words\n",
    "text8_ds = []\n",
    "for x in dataset: \n",
    "    for y in x:\n",
    "        text8_ds.append(y)\n",
    "        \n",
    "# Subsampling\n",
    "text8_ds = sampling(text8_ds)\n",
    "\n",
    "# New dataset with sentences of length=20\n",
    "text8_dataset = words_to_sentences(text8_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8421394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text8_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, spatial \n",
    "import csv, numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import spatial \n",
    "#IMPORT DATA\n",
    "def get_wordsim_data():\n",
    "    wordsim_data = [] \n",
    "    with open('./data/wordsim/set1.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for row in reader: \n",
    "            wordsim_data.append(row[0].split(',')[0:3])\n",
    "    del wordsim_data[0]\n",
    "    with open('./data/wordsim/set2.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for i,row in enumerate(reader):\n",
    "            if i!=0:\n",
    "                wordsim_data.append(row[0].split(',')[0:3])\n",
    "\n",
    "    wordsim_vocab = set()\n",
    "    for x in wordsim_data:\n",
    "        wordsim_vocab.add(x[0])\n",
    "        wordsim_vocab.add(x[1])\n",
    "    return wordsim_data\n",
    "\n",
    "#len(wordsim_vocab.intersection(text8_dataset_first_sentence.vocab))\n",
    "def wordsim_task(dict_emb):\n",
    "    wordsim_data = get_wordsim_data()\n",
    "    scores = []\n",
    "    distances = []\n",
    "    for task in wordsim_data: \n",
    "        if (task[0] in dict_emb.keys() ) and (task[1] in dict_emb.keys()):\n",
    "            scores.append(float(task[2]))\n",
    "            distances.append(spatial.distance.cosine(dict_emb[task[0]], dict_emb[task[1]]))\n",
    "            \n",
    "            \n",
    "    #return stats.zscore(np.array([x[1] for x in out],dtype=float))\n",
    "    return np.corrcoef(scores,distances)\n",
    "\n",
    "#print(wordsim_task(gensim_emb))\n",
    "#wordsim_task(dict_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0,0)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        pos_u = pos_u.view(-1).to(device)\n",
    "        pos_v = pos_v.to(device)\n",
    "        neg_v = neg_v.to(device)\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, power=0.75,ctx_window=2):\n",
    "        self.LEN_SEN =20\n",
    "        #assert( all(len(sentence)== self.LEN_SEN) for sentence in dataset)\n",
    "        self.ctx_window = ctx_window\n",
    "        self.dataset = dataset[0:len(dataset)-2]\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.vocab_size = int()\n",
    "        self.vocab = set()\n",
    "        self.create_vocab()\n",
    "        #self.pairs = self.generate_pairs()\n",
    "        #self.key_pairs = self.generate_key_pairs(self.pairs)\n",
    "        self.power = power        \n",
    "        self.neg_table = self.make_neg_table(self.power)\n",
    "        #self.len = self.__len__()\n",
    "\n",
    "        \n",
    "    def generate_pairs(self):\n",
    "        print(\"Generating pairs\")\n",
    "        pairs = []\n",
    "        for sentence in self.dataset:\n",
    "            for i,word in enumerate(sentence):\n",
    "                for j in range(1,self.ctx_window+1):\n",
    "                    if(i+j<len(sentence)):\n",
    "                        pairs.append((word,sentence[i+j]))\n",
    "                    if((i-j)>=0):\n",
    "                        pairs.append((word,sentence[i-j]))\n",
    "\n",
    "        return pairs\n",
    "        \n",
    "    def __len__(self):          \n",
    "        len_dataset = len(self.dataset)     \n",
    "        center_pairs = ((self.LEN_SEN - self.ctx_window*2)*self.ctx_window*2) \n",
    "        border_pairs = sum([self.ctx_window + i for i in range(self.ctx_window)])*2\n",
    "        len_sen_without_last = (center_pairs + border_pairs)* (len_dataset-1)\n",
    "        \n",
    "        # The last sentence does not has the same length as the other ones, hence it's length needs to be computed otherwise\n",
    "        len_last_sen = len(self.dataset[(len_dataset-1)])\n",
    "        pairs_last_sen = 0\n",
    "        for j in range(len_last_sen):\n",
    "            if(j<self.ctx_window):\n",
    "                pairs_last_sen += (j+self.ctx_window)\n",
    "            elif( j>= len_last_sen - self.ctx_window):\n",
    "                pairs_last_sen += (len_last_sen-1-j+self.ctx_window)\n",
    "            else:\n",
    "                pairs_last_sen += (2*self.ctx_window)\n",
    "        return len_sen_without_last + pairs_last_sen\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    def get_neg_samples(self, count, batch_size):\n",
    "        return torch.tensor(np.random.choice(list(self.idx2word.keys()),size=(batch_size)*count,replace=True,p=self.neg_table)).view(batch_size,-1)\n",
    "   \n",
    "    \"\"\" Defines the probability of choosing a negative sampling, set empiraccaly by mikolov\"\"\"\n",
    "    def make_neg_table(self, power):\n",
    "        pow_frequency = np.array([self.word_count[self.idx2word[i]] for i in range(len(self.word_count))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "        \n",
    "\n",
    "    def generate_key_pairs(self,pairs):\n",
    "        print(\"Generating key_pairs\")\n",
    "        key_pairs = []\n",
    "        for x,y in pairs:\n",
    "            key_pairs.append((self.word2idx.get(x),self.word2idx.get(y)))\n",
    "        print(\"finished creating key_pairs\")\n",
    "        return key_pairs\n",
    "    \n",
    "    \"\"\"\"Creating vocabulary and creating dictionary with a one to one mapping int to word\"\"\"\n",
    "    def create_vocab(self):\n",
    "        print(\"Creating vocab\")\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "            for word in sentence:\n",
    "                self.word_count[word] += 1\n",
    "                self.vocab.add(word)\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "             \n",
    "    def __getitem__(self, idx):\n",
    "        #Getting the number of pairs per sentence\n",
    "        border_pairs = sum([self.ctx_window + i for i in range(self.ctx_window)])*2\n",
    "        center_pairs = ((self.LEN_SEN - self.ctx_window*2)*self.ctx_window*2)\n",
    "        n_pairs_in_sen = border_pairs + center_pairs\n",
    "        id_sen = int(idx/n_pairs_in_sen)\n",
    "        sen  = self.dataset[id_sen]\n",
    "        pair_id_in_sen = idx - id_sen*(n_pairs_in_sen)\n",
    "        counter = 0 \n",
    "        for i,word in enumerate(sen):\n",
    "            for j in range(1,self.ctx_window+1):\n",
    "                if(i+j< len(sen)):\n",
    "                    if(counter == pair_id_in_sen):\n",
    "                        #print(word)\n",
    "                        #print(i+j)\n",
    "                        if (self.word2idx[word],self.word2idx[sen[i+j]]) is None:\n",
    "                            pdb.set_trace()\n",
    "                        return(self.word2idx[word],self.word2idx[sen[i+j]])\n",
    "                    counter+=1\n",
    "                    \n",
    "                if(i-j>=0):\n",
    "                    if(counter == pair_id_in_sen):\n",
    "                        #print(word)\n",
    "                        if(self.word2idx[word],self.word2idx[sen[i-j]]) is None:\n",
    "                            pdb.set_trace()\n",
    "                        return(self.word2idx[word],self.word2idx[sen[i-j]])\n",
    "                    counter+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "import numbers\n",
    "\n",
    "class W2V():\n",
    "    def __init__(self, data,dim=100, neg_samples=10, alpha=0.4, iterations=20, batch_size=2000, \n",
    "                 shuffle=True,use_cuda=True,workers=1,momentum=0,nesterov=False,step_size=1,gamma=1):\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "        self.ctxw = self.data.ctx_window\n",
    "        self.neg_samples = neg_samples\n",
    "        self.use_cuda = use_cuda\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.ws_list = []\n",
    "        self.loss_list = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab), self.dim)\n",
    "        self.model.to(device)\n",
    "    \n",
    "        print(device)\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha, momentum=momentum,nesterov=nesterov)\n",
    "        #self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
    "        #self.optimizer = torch.optim.Adagrad(self.model.parameters(), lr=alpha)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "\n",
    "        self.iterations = iterations\n",
    " \n",
    "    def train_with_loader(self,save_embedding=True):\n",
    "        loader = DataLoader(self.data, self.batch_size, self.shuffle, num_workers=self.workers)\n",
    "        print('starting training')\n",
    "        tenth = int(len(loader)/10)\n",
    "\n",
    "        self.time=0\n",
    "        no_improvement = 0\n",
    "        best_score = -1\n",
    "        prev_score = -1\n",
    "        for epoch in range(self.iterations):\n",
    "\n",
    "            percent = 0\n",
    "            start = time.time()\n",
    "            processed_batches = 0 \n",
    "            pairs = 0\n",
    "            cum_loss = 0 \n",
    "            avg_loss =0\n",
    "            best_loss = 10 \n",
    "            \n",
    "            for i,(pos_u,pos_v) in enumerate(loader):\n",
    "                if(i%tenth == 0 ):\n",
    "                    end = time.time()\n",
    "                    hours, rem = divmod(end-start, 3600)\n",
    "                    minutes, seconds = divmod(rem, 60)\n",
    "                    time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "                    if(processed_batches!=0):\n",
    "                        avg_loss = cum_loss / processed_batches\n",
    "                    print(\"0%\" + \"=\" *(int(percent/10))+ str(percent) +\"%, \" + time_since_start + \", cum_loss = {}\".format(cum_loss),end=\"\\r\" )\n",
    "                    percent+=10\n",
    "                    \n",
    "                neg_v = self.data.get_neg_samples(self.neg_samples,pos_v.size()[0])\n",
    "                pos_v = pos_v.view(len(neg_v),-1)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                cum_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                pairs += len(pos_u)\n",
    "                processed_batches += 1\n",
    "                \n",
    "            print(\"\\n{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "            avg_loss = cum_loss / processed_batches\n",
    "            print(\" {0:d} {1:d} batches, pairs {2:d}, cum loss: {3:.5f}\".format(i,processed_batches, pairs,cum_loss))\n",
    "            self.loss_list.append(cum_loss)\n",
    "            self.time = time_since_start\n",
    "            self.model = self.model.to(cpu)\n",
    "            score = -1*(wordsim_task(self.get_embedding())[0][1])\n",
    "            if(score < best_score):\n",
    "                best_score = score\n",
    "            print(\"Current score on wordsim Task: {}\".format(score))\n",
    "            self.ws_list.append(score)\n",
    "            self.model = self.model.to(gpu)\n",
    "            \n",
    "            \n",
    "            if(score > best_score ):\n",
    "                best_score = score\n",
    "            \n",
    "            if(score - prev_score < 0.0009):\n",
    "                no_improvement += 1\n",
    "                \n",
    "            if(no_improvement == 2 or score > 0.66):\n",
    "                print(\"No improvement in word similarity early stoppage\")\n",
    "                self.iterations = epoch\n",
    "                break\n",
    "                \n",
    "            \n",
    "            prev_score = score \n",
    "        \n",
    "        if(save_embedding):\n",
    "            self.save_embedding()\n",
    "            \n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.idx2word)):\n",
    "            embedding_dict[self.data.idx2word[i]]= embedding[i]\n",
    "        return embedding_dict\n",
    "    \n",
    "    def save_embedding(self, with_loss=True):\n",
    "        print('ntm')\n",
    "        # Creating filename\n",
    "        optim = \"Optim\" + str(self.optimizer).split(\" \")[0] + \"_\"\n",
    "        filename = \"dict_emb_\" +  optim + \"_\".join([x + str(y) for x,y in vars(self).items() if isinstance(y, numbers.Number)]) + \".pkl\"\n",
    "        \n",
    "        # Getting Embedding\n",
    "        self.model.to(torch.device('cpu'))\n",
    "        dict_emb = w2v.get_embedding()\n",
    "        \n",
    "        # Adding loss history to embedding\n",
    "        dict_emb['loss_list'] = [x.to(torch.device('cpu')) for x in self.loss_list]\n",
    "        \n",
    "        # Adding score list to embedding \n",
    "        dict_emb['ws_list'] = self.ws_list\n",
    "        \n",
    "                \n",
    "        # Saving time spent to calculate 1 epoch\n",
    "        dict_emb['time'] = self.time\n",
    "        \n",
    "        # Logging\n",
    "        print(\"Saving embedding: {} to disk with ws_score: {} \".format(filename,dict_emb['ws_list']))\n",
    "    \n",
    "        # Writing embedding dictionnary to disk\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(dict_emb, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        self.model.to(device)\n",
    "        self.loss_list = [x.to(device) for x in self.loss_list]\n",
    "    \n",
    " \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n",
      "10692\n",
      "cpu\n",
      "starting training\n",
      "0%=========90%, Time:  00:00:07.87, cum_loss = 67698.5546875\n",
      "1 epoch of 2\n",
      " 1069 1070 batches, pairs 10692, cum loss: 74447.14844\n",
      "Current score on wordsim Task: 0.3085665560266278\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (38) : no CUDA-capable device is detected at /build/python-pytorch/src/pytorch-1.0.1-cuda/aten/src/THC/THCGeneral.cpp:51",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-83d2b9d7cec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m191\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2V\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-96af46018053>\u001b[0m in \u001b[0;36mtrain_with_loader\u001b[0;34m(self, save_embedding)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current score on wordsim Task: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    161\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (38) : no CUDA-capable device is detected at /build/python-pytorch/src/pytorch-1.0.1-cuda/aten/src/THC/THCGeneral.cpp:51"
     ]
    }
   ],
   "source": [
    "y = text8_dataset[0:100]\n",
    "y.append([\"this\", \"is\",\"a\",\"test\"])\n",
    "dataset = wDataSet(y,ctx_window=3)\n",
    "#print(dataset.__getitem__(1082*10))\n",
    "print(dataset.__len__())\n",
    "dataset.idx2word[191]\n",
    "w2v = W2V(dataset,alpha =0.1, momentum=0, nesterov=False,shuffle=False,batch_size=10, iterations=2)\n",
    "w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n"
     ]
    }
   ],
   "source": [
    "text8_wDataset = wDataSet((text8_dataset),ctx_window=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "starting training\n",
      "0%=====50%, Time:  00:21:01.69, cum_loss = 111136312.0\r"
     ]
    }
   ],
   "source": [
    "for x in [0.001]:\n",
    "    w2v = W2V(text8_wDataset,alpha =x, momentum=0.9, nesterov=True,shuffle=True)\n",
    "    w2v.train_with_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdagrad_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.001_dim100_workers1_ctxw5_neg_samples10_use_cudaTrue_iterations20.pkl to disk with ws_score: [0.013481162799224079, 0.01459762065124132, 0.015572023071707916, 0.016924285313042076] \n"
     ]
    }
   ],
   "source": [
    "w2v.save_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordsim Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3.8356, requires_grad=True),\n",
       " tensor(3.1526, requires_grad=True),\n",
       " tensor(3.0362, requires_grad=True),\n",
       " tensor(2.9739, requires_grad=True),\n",
       " tensor(2.9349, requires_grad=True),\n",
       " tensor(2.9065, requires_grad=True),\n",
       " tensor(2.8826, requires_grad=True),\n",
       " tensor(2.8661, requires_grad=True),\n",
       " tensor(2.8608, requires_grad=True),\n",
       " tensor(2.8345, requires_grad=True),\n",
       " tensor(2.8215, requires_grad=True),\n",
       " tensor(2.8103, requires_grad=True),\n",
       " tensor(2.7993, requires_grad=True),\n",
       " tensor(2.7897, requires_grad=True),\n",
       " tensor(2.7821, requires_grad=True),\n",
       " tensor(2.7747, requires_grad=True),\n",
       " tensor(2.7676, requires_grad=True),\n",
       " tensor(2.7610, requires_grad=True),\n",
       " tensor(2.7545, requires_grad=True),\n",
       " tensor(2.7476, requires_grad=True)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "with open(\"dict_embshuffleFalse_batch_size5000_alpha20_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations20.pkl\", 'rb') as output:\n",
    "        dict_emb = pickle.load(output)\n",
    "dict_emb['loss_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30364346504211426\n",
      "0.45958149433135986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5577877461910248"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x = spatial.distance.cosine(gensim_emb['love'], gensim_emb['music'])\n",
    "y = spatial.distance.cosine(gensim_emb['anarchism'],gensim_emb['music'])\n",
    "z = spatial.distance.cosine(gensim_emb['revolution'],gensim_emb['creatine'])\n",
    "\n",
    "l = ['music','anarchism','revolution','philosophy','creatine']\n",
    "print(x)\n",
    "print(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5.5450, requires_grad=True),\n",
       " tensor(5.5439, requires_grad=True),\n",
       " tensor(5.5429, requires_grad=True),\n",
       " tensor(5.5426, requires_grad=True),\n",
       " tensor(5.5412, requires_grad=True),\n",
       " tensor(5.5425, requires_grad=True),\n",
       " tensor(5.5414, requires_grad=True),\n",
       " tensor(5.5402, requires_grad=True),\n",
       " tensor(5.5409, requires_grad=True),\n",
       " tensor(5.5391, requires_grad=True),\n",
       " tensor(5.5376, requires_grad=True),\n",
       " tensor(5.5360, requires_grad=True),\n",
       " tensor(5.5368, requires_grad=True),\n",
       " tensor(5.5365, requires_grad=True),\n",
       " tensor(5.5357, requires_grad=True),\n",
       " tensor(5.5351, requires_grad=True),\n",
       " tensor(5.5354, requires_grad=True),\n",
       " tensor(5.5333, requires_grad=True),\n",
       " tensor(5.5336, requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_emb.pop('loss_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_emb = dict()\n",
    "for sentences in text8_dataset:\n",
    "    for word in sentences:\n",
    "        gensim_emb[word] = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 end: avg_loss=18135602.967545956\n",
      "Epoch #1 end: avg_loss=32065792.321420036\n",
      "Epoch #2 end: avg_loss=43534352.25820159\n",
      "Epoch #3 end: avg_loss=51144233.176004134\n",
      "Epoch #4 end: avg_loss=56890286.97092096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-05fbdda2b0a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#TODO: logging, save loss, batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mepoch_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpochLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext8_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.075\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.cum_loss = 0\n",
    "        self.processed_batches = 0\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        avg_loss = self.cum_loss/self.processed_batches\n",
    "        print(\"Epoch #{} end: avg_loss={}\".format(self.epoch,avg_loss))\n",
    "        self.epoch += 1\n",
    "    \n",
    "    def on_batch_end(self, model):\n",
    "        \"\"\"Method called at the end of each batch.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel`\n",
    "            Current model.\n",
    "        \"\"\"\n",
    "        self.cum_loss += model.get_latest_training_loss()\n",
    "        self.processed_batches +=1\n",
    "        \n",
    "#TODO: logging, save loss, batch_size\n",
    "epoch_logger = EpochLogger()\n",
    "model = Word2Vec(text8_dataset, size=100,window=10,negative=7, alpha=0.075, min_count=1, workers=4,sg=1, iter=20, batch_words=7000, callbacks=[epoch_logger],compute_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(word):\n",
    "    for x in dict_emb.keys():\n",
    "        yield(x, spatial.distance.cosine(dict_emb[word],dict_emb[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0927695588b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_dict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_dict_emb_gensim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-0927695588b9>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_dict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_dict_emb_gensim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2449\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2451\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2452\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "n_dict_emb = {(word): (x / np.linalg.norm(x)) for (word, x) in (dict_emb.items())}\n",
    "n_dict_emb_gensim = {(word): (x / np.linalg.norm(x)) for (word, x) in (gensim_emb.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_closest(score_dict, word):\n",
    "    closest = ()\n",
    "    distance = 3\n",
    "    for (x,y),score in score_dict.items():\n",
    "        #print(x,y,score)\n",
    "        if((x != y) and ((x==word)or(y==word))):\n",
    "            if (distance > score):\n",
    "                closest = (x,y)\n",
    "                distance = score\n",
    "    return closest\n",
    "\n",
    "def get_closest_with_score(dict_emb,y):\n",
    "    distance = 100\n",
    "    for x,emb in dict_emb.items():\n",
    "        if(spatial.distance.cosine(dict_emb[x], dict_emb[y])<distance):\n",
    "            closest = x\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALOGY TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/questions-words.txt\")\n",
    "questions = file.readlines()\n",
    "for i,x in enumerate(questions): \n",
    "    questions[i] = x.rstrip(\"\\n\").split()\n",
    "    if x[0]==':':\n",
    "        del questions[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(questions,dict_emb):\n",
    "    score = []\n",
    "    if all(word in dict_emb for word in questions):\n",
    "        y = dict_emb[questions[0]] -  dict_emb[questions[1]] +  dict_emb[questions[2]]\n",
    "        x = get_closest_with_score(dict_emb,y)\n",
    "        if x == questions[3]:\n",
    "            score.append(1)\n",
    "        else: \n",
    "            score.append(0)\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_sim(dict_emb): \n",
    "    # Create dictionnary with id for every word, this is needed because sometimes we only have access to the dict_emb\n",
    "    # and not the whole model \n",
    "    idx2word = {idx: w for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    \n",
    "    emb_size = len(next(iter(dict_emb.values())))\n",
    "    \n",
    "    # Create an embedding dictionnary with normalized vectors\n",
    "    normalized_dict_emb = {(word): (x / np.linalg.norm(x)) for (word, x) in (dict_emb.items())}\n",
    "    \n",
    "    # Create an vocab_size*emb_size Matrix that holds the normalized embeding of each word in it's row called matrix_row\n",
    "    # Create an emb_size*vocab_size Matrix that holds the normalized embeding of each word in it's colomn  matrix_colomn\n",
    "    for i in range(0,len(dict_emb.keys())):\n",
    "        y = normalized_dict_emb[idx2word[i]]\n",
    "        if i ==0:\n",
    "            matrix_colomn = torch.tensor(y).view(emb_size,1)\n",
    "            matrix_row = torch.tensor(y)\n",
    "        else:\n",
    "            matrix_colomn = torch.cat([matrix_colomn,torch.tensor(y).view(emb_size,1)],1)\n",
    "            #pdb.set_trace()\n",
    "            matrix_row = torch.cat([matrix_row,torch.tensor(y)])\n",
    "    \n",
    "    matrix_row = matrix_row.view(-1,emb_size)\n",
    "    \n",
    "    matrix_row = matrix_row.to(device)\n",
    "    matrix_colomn = matrix_colomn.to(device)\n",
    "    \n",
    "    return 1-(torch.matmul(matrix_row,matrix_colomn)),word2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
