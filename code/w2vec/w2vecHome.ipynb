{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-1,1)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        #pdb.set_trace()\n",
    "        pos_u = pos_u.view(-1)\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        #neg_v = neg_v.view(len(pos_u),-1)\n",
    "        #pdb.set_trace()\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        #pdb.set_trace()\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        #pdb.set_trace()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, power=0.75,ctx_window=2, neg_samples=2,sampling=False,sampling_treshhold=1):\n",
    "        self.ctx_window = ctx_window\n",
    "        self.dataset = dataset\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.vocab_size = int()\n",
    "        self.vocab = set()\n",
    "        self.create_vocab(sampling, sampling_treshhold)\n",
    "        self.pairs = self.generate_pairs()\n",
    "        self.key_pairs = self.generate_key_pairs(self.pairs)\n",
    "        self.power = power        \n",
    "\n",
    "        \n",
    "    def generate_pairs(self):\n",
    "        print(\"Generating pairs\")\n",
    "        pairs = []\n",
    "        for sentence in self.dataset:\n",
    "            for i,word in enumerate(sentence):\n",
    "                for j in range(1,self.ctx_window):\n",
    "                    if(i+j<len(sentence)):\n",
    "                        pairs.append((word,sentence[i+j]))\n",
    "                    if((i-j)>0):\n",
    "                        pairs.append((word,sentence[i-j]))\n",
    "\n",
    "        return pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.key_pairs)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.key_pairs\n",
    "    \n",
    "    def get_neg_samples(self, count, batch_size):\n",
    "        return torch.tensor(np.random.choice(list(self.idx2word.keys()),size=(batch_size)*count,replace=True,p=self.make_neg_table(self.power))).view(batch_size,-1)\n",
    "        #neg_v = []\n",
    "        #for x in range(1,batch_size+1):\n",
    "         #   neg_v.append(random.sample(range(0,self.vocab_size),count))\n",
    "       # return torch.tensor(neg_v).view(batch_size,-1)\n",
    "         \n",
    "   \n",
    "    \"\"\" Defines the probability of choosing a negative sampling, set empiraccaly by mikolov\"\"\"\n",
    "    def make_neg_table(self, power):\n",
    "        pow_frequency = np.array([self.word_count[self.idx2word[i]] for i in range(len(self.word_count))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def generate_key_pairs(self,pairs):\n",
    "        print(\"Generating key_pairs\")\n",
    "        key_pairs = []\n",
    "        #print(pairs)\n",
    "        for x,y in pairs:\n",
    "            key_pairs.append((self.word2idx.get(x),self.word2idx.get(y)))\n",
    "            if self.word2idx.get(x) is None: \n",
    "                pdb.set_trace()\n",
    "        print(\"finished creating key_pairs\")\n",
    "        return key_pairs\n",
    "    \n",
    "    \"\"\"\"Creating vocabulary: first counting all words then deleting all frequent\n",
    "    words, then creating dictionary with a one to one mapping int to word\"\"\"\n",
    "    def create_vocab(self,sampling, treshhold):\n",
    "        print(\"Creating vocab\")\n",
    "        if sampling:\n",
    "            self.create_vocab_with_sampling(treshhold)\n",
    "        else:\n",
    "            for i,sentence in enumerate(self.dataset):\n",
    "                for word in sentence:\n",
    "                    self.word_count[word] += 1\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def create_vocab_with_sampling(self,treshhold):\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "                for word in sentence:\n",
    "                    self.word_count[word] += 1\n",
    "        #Creeate Sampling table     \n",
    "        sampling_table = self.make_sampling_table(treshhold)  \n",
    "        assert len(sampling_table)== len(self.word_count)\n",
    "        #Select which words are going to be deleted\n",
    "        sampled_words = [word for i,word in enumerate(self.word_count.keys()) if random.random() < sampling_table[i]]\n",
    "        #Create new dataset by deleting sampled words\n",
    "        self.sampled_words = sampled_words\n",
    "        self.word_count = defaultdict(int)\n",
    "        new_dataset = []\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "            new_sentence = [word for word in sentence if word not in sampled_words]\n",
    "            new_dataset.append(new_sentence)\n",
    "            for word in new_sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.word_count[word] += 1 \n",
    "        self.dataset = new_dataset\n",
    "        for sentence in self.dataset:\n",
    "            assert all(words not in sampled_words for words in sentence)\n",
    "    def make_sampling_table(self,treshhold): \n",
    "        count = np.array([x for x in self.word_count.values()])\n",
    "        table = [1-x for x in list( np.sqrt(treshhold/(count)))]\n",
    "        return table\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class W2V():\n",
    "    def __init__(self, data,dim=5, neg_samples=2, alpha=0.01, iterations=20, batch_size=2, \n",
    "                 shuffle=False,use_cuda=True,workers=7):\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "\n",
    "        self.neg_samples = neg_samples\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab), self.dim)\n",
    "        #self.model.cuda()\n",
    "        print(device)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha)\n",
    "\n",
    "        self.iterations = iterations\n",
    "        #self.train()        \n",
    " \n",
    "    def train_with_loader(self):\n",
    "        loader = DataLoader(self.data.key_pairs, self.batch_size, self.shuffle, num_workers=self.workers)\n",
    "        print('starting training')\n",
    "        tenth = int(len(loader)/10)\n",
    "        for epoch in range(1,self.iterations):\n",
    "            percent = 0\n",
    "            start = time.time()\n",
    "            for i,(pos_u,pos_v) in enumerate(loader):\n",
    "                if(i%tenth == 0 ):\n",
    "                    end = time.time()\n",
    "                    hours, rem = divmod(end-start, 3600)\n",
    "                    minutes, seconds = divmod(rem, 60)\n",
    "                    print(\"Time since start: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "                    print(str(percent) + \"% of epoch is done\" )\n",
    "                    percent+=10\n",
    "                neg_v = self.data.get_neg_samples(self.neg_samples,pos_v.size()[0])\n",
    "                pos_v = pos_v.view(len(neg_v),-1)\n",
    "                #pos_v.cuda()\n",
    "                #pos_u.cuda()\n",
    "                #neg_v.cuda()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                prev_loss = loss\n",
    "            print(\"loss = \" + str(loss))\n",
    "            print(\"{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.idx2word)):\n",
    "            embedding_dict[self.data.idx2word[i]]= embedding[i]\n",
    "        return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "#sentences = LineSentence(datapath('lee_background.cor'))\n",
    "dataset = api.load('text8')\n",
    "text8_dataset = []\n",
    "for x in dataset: \n",
    "    y = [x[i:i + 20] for i in range(0, len(x), 20)]\n",
    "    text8_dataset.extend(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_first_sentence = text8_dataset[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n",
      "Generating pairs\n",
      "Generating key_pairs\n",
      "finished creating key_pairs\n"
     ]
    }
   ],
   "source": [
    "text8_dataset_first_sentence = wDataSet((text8_first_sentence))\n",
    "#text8_wDataset = wDataSet((text8_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "w2v = W2V(text8_dataset_first_sentence)\n",
    "#w2v = W2V(text8_wDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Time since start: 00:00:00.28\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.33\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.35\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.36\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.36\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.41\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.56\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.62\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.63\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.64\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.65\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.66\n",
      "120% of epoch is done\n",
      "loss = tensor(2.1187, grad_fn=<DivBackward0>)\n",
      "2 epoch of 20\n",
      "Time since start: 00:00:00.20\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.33\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.35\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.37\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.41\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.46\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.48\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.50\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.51\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0755, grad_fn=<DivBackward0>)\n",
      "3 epoch of 20\n",
      "Time since start: 00:00:00.17\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.30\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.33\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.34\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.35\n",
      "120% of epoch is done\n",
      "loss = tensor(2.1839, grad_fn=<DivBackward0>)\n",
      "4 epoch of 20\n",
      "Time since start: 00:00:00.15\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.16\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.18\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.18\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0644, grad_fn=<DivBackward0>)\n",
      "5 epoch of 20\n",
      "Time since start: 00:00:00.15\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.18\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.36\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.38\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.40\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.41\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.42\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.43\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.44\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.45\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0612, grad_fn=<DivBackward0>)\n",
      "6 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.17\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.32\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.32\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0794, grad_fn=<DivBackward0>)\n",
      "7 epoch of 20\n",
      "Time since start: 00:00:00.15\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.30\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.32\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.34\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.35\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0613, grad_fn=<DivBackward0>)\n",
      "8 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.17\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0865, grad_fn=<DivBackward0>)\n",
      "9 epoch of 20\n",
      "Time since start: 00:00:00.15\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.18\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0792, grad_fn=<DivBackward0>)\n",
      "10 epoch of 20\n",
      "Time since start: 00:00:00.19\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.30\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.32\n",
      "120% of epoch is done\n",
      "loss = tensor(2.1545, grad_fn=<DivBackward0>)\n",
      "11 epoch of 20\n",
      "Time since start: 00:00:00.15\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.17\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.30\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "120% of epoch is done\n",
      "loss = tensor(2.1313, grad_fn=<DivBackward0>)\n",
      "12 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.30\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.32\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.33\n",
      "120% of epoch is done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(2.0783, grad_fn=<DivBackward0>)\n",
      "13 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.18\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.33\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.35\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.37\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "120% of epoch is done\n",
      "loss = tensor(1.9995, grad_fn=<DivBackward0>)\n",
      "14 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.32\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.43\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.47\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.51\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.55\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.58\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.60\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.63\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.65\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.66\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.67\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0101, grad_fn=<DivBackward0>)\n",
      "15 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.30\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.32\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.33\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0264, grad_fn=<DivBackward0>)\n",
      "16 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.29\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.30\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.31\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.33\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.34\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.35\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0850, grad_fn=<DivBackward0>)\n",
      "17 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0302, grad_fn=<DivBackward0>)\n",
      "18 epoch of 20\n",
      "Time since start: 00:00:00.15\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.38\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.39\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.40\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.41\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.42\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.43\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.44\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.45\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.47\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.48\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.49\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0257, grad_fn=<DivBackward0>)\n",
      "19 epoch of 20\n",
      "Time since start: 00:00:00.16\n",
      "0% of epoch is done\n",
      "Time since start: 00:00:00.17\n",
      "10% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "20% of epoch is done\n",
      "Time since start: 00:00:00.19\n",
      "30% of epoch is done\n",
      "Time since start: 00:00:00.20\n",
      "40% of epoch is done\n",
      "Time since start: 00:00:00.21\n",
      "50% of epoch is done\n",
      "Time since start: 00:00:00.22\n",
      "60% of epoch is done\n",
      "Time since start: 00:00:00.23\n",
      "70% of epoch is done\n",
      "Time since start: 00:00:00.24\n",
      "80% of epoch is done\n",
      "Time since start: 00:00:00.25\n",
      "90% of epoch is done\n",
      "Time since start: 00:00:00.26\n",
      "100% of epoch is done\n",
      "Time since start: 00:00:00.27\n",
      "110% of epoch is done\n",
      "Time since start: 00:00:00.28\n",
      "120% of epoch is done\n",
      "loss = tensor(2.0285, grad_fn=<DivBackward0>)\n",
      "20 epoch of 20\n"
     ]
    }
   ],
   "source": [
    "w2v.train_with_loader()\n",
    "dict_emb = w2v.get_embedding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(w2v):\n",
    "    # creating filename\n",
    "    bs = 'bs'+str(w2v.batch_size)\n",
    "    neg = 'neg'+str(w2v.neg_samples)\n",
    "    dim = 'dim' + str(w2v.dim)\n",
    "    epochs = 'epochs'+ str(w2v.iterations)\n",
    "    ctxw = 'ctxw' + str(w2v.data.ctx_window)\n",
    "    l = [bs,neg,dim,epochs,ctxw]\n",
    "    filename= \"w2v\"\n",
    "\n",
    "    for x in l: \n",
    "        filename += (str(x) + '_')\n",
    "        filename = \"dict_emb\" + filename + '.pkl'\n",
    "\n",
    "    dict_emb = w2v.get_embedding()\n",
    "    \n",
    "    #Writing embedding dictionnary to disk\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(w2v_loaded, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'wDataSet' object has no attribute 'ctx_window'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0af152c6345c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-6ec6dcc936b3>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dim'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mctxw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ctxw'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctxw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"w2v\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'wDataSet' object has no attribute 'ctx_window'"
     ]
    }
   ],
   "source": [
    "save_model(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dict_embw2vbs10000_neg3_dim100_epochs10_ctxw3_.pkl\", 'rb') as output:\n",
    "        dict_emb = pickle.load( output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALOGY TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('triz', array([ 2.7908196e-03, -3.9299615e-03, -2.3204442e-03, -3.4591248e-03,\n",
      "        3.1974176e-03,  4.4118953e-03, -4.0873745e-03,  2.3508798e-03,\n",
      "       -3.8561493e-03,  1.1269100e-03,  8.4011431e-04, -1.4577062e-03,\n",
      "       -7.8327197e-04, -3.6435522e-04, -2.5357916e-03, -2.5398063e-03,\n",
      "        9.3340687e-04,  5.8832829e-04,  4.4516465e-03,  3.3982263e-03,\n",
      "       -3.5082817e-03,  2.2542973e-03, -1.8671324e-03, -1.4677208e-03,\n",
      "       -1.6102671e-03,  4.9482221e-03,  1.0405207e-03,  2.0895998e-03,\n",
      "       -5.1818328e-04, -3.4137368e-03, -4.0643257e-03,  3.1268112e-03,\n",
      "       -4.7573191e-03, -1.0440495e-03,  1.3065534e-03, -3.0183150e-03,\n",
      "       -4.3906020e-03, -3.3723561e-03,  1.3491756e-03,  9.6055761e-04,\n",
      "        5.3626538e-04, -2.2270638e-03, -4.0281776e-04,  9.6523174e-04,\n",
      "       -7.1435230e-04,  5.1316920e-06,  1.9256553e-03, -3.2166936e-03,\n",
      "        1.2036571e-03,  3.8689924e-03,  1.2253888e-03, -6.5210654e-04,\n",
      "       -1.5750157e-03,  3.6504273e-03,  2.3467934e-03, -4.6054446e-03,\n",
      "       -2.0249526e-03,  2.4581626e-03, -5.6356512e-04,  2.8489290e-03,\n",
      "        3.1225586e-03, -3.8220917e-03, -1.2941057e-03,  6.6663517e-04,\n",
      "        2.3197646e-03, -2.9292086e-03, -4.4421232e-03,  3.3205708e-03,\n",
      "        1.4725043e-03, -4.3244464e-03, -4.7754413e-03, -4.3552564e-03,\n",
      "        9.1309065e-04,  3.6646551e-03,  3.8491888e-04, -3.7358264e-03,\n",
      "        1.5673040e-03, -4.8955681e-04, -1.3232864e-04,  2.2048465e-05,\n",
      "        2.3130656e-03,  7.4513041e-04, -2.1683834e-03,  2.9147845e-03,\n",
      "       -1.0063079e-03, -1.0122831e-03, -2.8061823e-03,  2.6791724e-03,\n",
      "        2.6399321e-03, -4.4496404e-03, -3.7802579e-03, -2.0250403e-03,\n",
      "       -4.2842869e-03, -4.2603691e-03,  2.4898809e-03, -2.1139425e-03,\n",
      "        2.5337334e-03,  9.9224499e-06,  3.7592915e-03,  4.8472462e-05],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dict_emb = {(word,i): (x / np.linalg.norm(x)) for i,(word, x) in enumerate(dict_emb.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normalized_dict_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.1959678\n",
      "1.1959678\n",
      "1.370727\n",
      "1.3707269\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-da47ef3e4328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def calculate_sim(dict_emb): \n",
    "    # Create dictionnary with id for every word, this is needed because sometimes we only have access to the dict_emb\n",
    "    # and not the whole model \n",
    "    idx2word = {idx: w for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    \n",
    "    emb_size = len(next(iter(dict_emb.values())))\n",
    "    \n",
    "    # Create an embedding dictionnary with normalized vectors\n",
    "    normalized_dict_emb = {(word): (x / np.linalg.norm(x)) for (word, x) in (dict_emb.items())}\n",
    "    \n",
    "    # Create an vocab_size*emb_size Matrix that holds the normalized embeding of each word in it's row called matrix_row\n",
    "    # Create an emb_size*vocab_size Matrix that holds the normalized embeding of each word in it's colomn  matrix_colomn\n",
    "    for i in range(0,len(dict_emb.keys())):\n",
    "        y = normalized_dict_emb[idx2word[i]]\n",
    "        if i ==0:\n",
    "            matrix_colomn = torch.tensor(y).view(emb_size,1)\n",
    "            matrix_row = torch.tensor(y)\n",
    "        else:\n",
    "            matrix_colomn = torch.cat([matrix_colomn,torch.tensor(y).view(emb_size,1)],1)\n",
    "            #pdb.set_trace()\n",
    "            matrix_row = torch.cat([matrix_row,torch.tensor(y)])\n",
    "    \n",
    "    matrix_row = matrix_row.view(-1,emb_size)\n",
    "    \n",
    "    matrix_row = matrix_row.to(device)\n",
    "    matrix_colomn = matrix_colomn.to(device)\n",
    "    \n",
    "    return 1-(torch.matmul(matrix_row,matrix_colomn)),word2idx\n",
    "\n",
    "matrix, word2idx = calculate_sim(dict_emb) \n",
    "\n",
    "import torch \n",
    "from scipy import spatial\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math\n",
    "score_dict = dict()\n",
    "for i,(x,y) in enumerate(itertools.product(text8_dataset_first_sentence.vocab,text8_dataset_first_sentence.vocab)):\n",
    "    distance = spatial.distance.cosine(dict_emb[x], dict_emb[y])\n",
    "    score_dict[(x,y)] = distance\n",
    "    print(round(distance,7))\n",
    "    print(round(float(matrix[word2idx[x],word2idx[y]]),7))\n",
    "    assert( math.isclose(round(distance,7),round(float(matrix[word2idx[x],word2idx[y]]),7)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from scipy import spatial\n",
    "import itertools\n",
    "import numpy as np\n",
    "score = []\n",
    "score_dict = dict()\n",
    "for i,(x,y) in enumerate(itertools.product(text8_dataset_first_sentence.vocab,text8_dataset_first_sentence.vocab)):\n",
    "    distance = spatial.distance.cosine(dict_emb[x], dict_emb[y])\n",
    "    score_dict[(x,y)] = distance\n",
    "    score.append(distance)\n",
    "l = []\n",
    "l1 = []\n",
    "for i,x in enumerate(score): \n",
    "    if x == 0 and i != 0:\n",
    "        l.append(l1)\n",
    "        l1 = []\n",
    "        l1.append(x)\n",
    "    else:\n",
    "        l1.append(x)\n",
    "len(l)        \n",
    "len(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, tensor(-1.1921e-07)),\n",
       " (1.3198491632938385, tensor(1.3198)),\n",
       " (1.5785843133926392, tensor(1.5786))]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i,(x,y) in enumerate(normalized_dict_emb.items()):\n",
    "    l.append(torch.matmul(torch.tensor(y), matrix))\n",
    "# 0 = is 1 = working 2 = pejorative\n",
    "p = ['is', 'working', 'pejorative']\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "p_score = []\n",
    "for x,y in itertools.product(*[p,p]):\n",
    "    p_score.append(spatial.distance.cosine(dict_emb[x], dict_emb[y]))\n",
    "p_score_n = [(x,1-y) for x,y in zip(p_score,l[0][0:3])]\n",
    "p_score_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'artist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-23c1e1f43b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'artist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'music'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anarchism'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'music'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'revolution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anarchism'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'artist'"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x = spatial.distance.cosine(dict_emb['artist'], dict_emb['music'])\n",
    "y = spatial.distance.cosine(dict_emb['anarchism'],dict_emb['music'])\n",
    "z = spatial.distance.cosine(dict_emb['revolution'],dict_emb['anarchism'])\n",
    "\n",
    "l = ['music','anarchism','artist','revolution','philosophy','creatine']\n",
    "print(x)\n",
    "print(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-99d5ef8f0aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalized_dict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_emb\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_emb' is not defined"
     ]
    }
   ],
   "source": [
    "normalized_dict_emb = {word: x / np.linalg.norm(x) for word, x in dict_emb }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(word):\n",
    "    for x in dict_emb.keys():\n",
    "        yield(x, spatial.distance.cosine(dict_emb[word],dict_emb[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for x in l:\n",
    "    p.append((list(get_distances(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import heapq\n",
    "for x in p:\n",
    "    print((heapq.nsmallest(5,x, key=itemgetter(1)))[0])\n",
    "    print((heapq.nlargest(5,x, key=itemgetter(1)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-1626a600654d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mumu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0muv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m     \u001b[0muu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "score = []\n",
    "score_dict = dict()\n",
    "for i,(x,y) in enumerate(itertools.product(text8_dataset_first_sentence.vocab,text8_dataset_first_sentence.vocab)):\n",
    "    if(i%1000000==0):\n",
    "        print(i)\n",
    "    distance = spatial.distance.cosine(dict_emb[x], dict_emb[y])\n",
    "    score_dict[(x,y)] = distance\n",
    "    score.append(distance)\n",
    "print(np.mean(score))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11336667971198654"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(score)\n",
    "#print(score_dict[('anarchism','music')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_closest(score_dict, word):\n",
    "    closest = ()\n",
    "    distance = 3\n",
    "    for (x,y),score in score_dict.items():\n",
    "        #print(x,y,score)\n",
    "        if((x != y) and ((x==word)or(y==word))):\n",
    "            if (distance > score):\n",
    "                closest = (x,y)\n",
    "                distance = score\n",
    "    return closest\n",
    "\n",
    "def get_closest_with_score(dict_emb,y):\n",
    "    distance = 100\n",
    "    for x,emb in dict_emb.items():\n",
    "        if(spatial.distance.cosine(dict_emb[x], dict_emb[y])<distance):\n",
    "            closest = x\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/questions-words.txt\")\n",
    "questions = file.readlines()\n",
    "for i,x in enumerate(questions): \n",
    "    questions[i] = x.rstrip(\"\\n\").split()\n",
    "    if x[0]==':':\n",
    "        del questions[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(questions,dict_emb):\n",
    "    score = []\n",
    "    if all(word in dict_emb for word in questions):\n",
    "        y = dict_emb[questions[0]] -  dict_emb[questions[1]] +  dict_emb[questions[2]]\n",
    "        x = get_closest_with_score(dict_emb,y)\n",
    "        if x == questions[3]:\n",
    "            score.append(1)\n",
    "        else: \n",
    "            score.append(0)\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "score = analogy_task(questions,dict_emb)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = random.sample(dict_emb.keys(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('armed', 'with')\n",
      "('zerzan', 'developments')\n",
      "('writings', 'increase')\n",
      "('list', 'women')\n",
      "('science', 'archons')\n",
      "('mysogyny', 'coo')\n",
      "('dominance', 'existing')\n",
      "('chomsky', 'controlled')\n",
      "('interact', 'assist')\n",
      "('operative', 'cgt')\n"
     ]
    }
   ],
   "source": [
    "for x in words:\n",
    "    print(get_closest(score_dict,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordsim Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT DATA\n",
    "import csv\n",
    "wordsim_data = [] \n",
    "with open('./data/wordsim/set1.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "    for row in reader: \n",
    "        wordsim_data.append(row[0].split(',')[0:3])\n",
    "    del wordsim_data[0]\n",
    "with open('./data/wordsim/set2.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "    for i,row in enumerate(reader):\n",
    "        if i!=0:\n",
    "            wordsim_data.append(row[0].split(',')[0:3])\n",
    "\n",
    "for x in wordsim_data:\n",
    "    wordsim_vocab.add(x[0])\n",
    "    wordsim_vocab.add(x[1])\n",
    "\n",
    "len(wordsim_vocab.intersection(text8_wDataset.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09963367870712828"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "def wordsim_task(wordsim_data, dict_emb):\n",
    "    out = []\n",
    "    for task in wordsim_data: \n",
    "        if (task[0] in dict_emb.keys() ) and (task[1] in dict_emb.keys()):\n",
    "            target_distance = spatial.distance.cosine(dict_emb[task[0]], dict_emb[task[1]])\n",
    "            out.append((task,target_distance))\n",
    "    return stats.zscore(np.array([x[1] for x in out],dtype=float))\n",
    "    \n",
    "scores_standarized = wordsim_task(wordsim_data,dict_emb)\n",
    "target_score_standarized = stats.zscore(np.array([x[2] for x in wordsim_data],dtype=float))\n",
    "scores_compared = [x-y for x,y in zip(scores_standarized,target_score_standarized)]\n",
    "np.array(scores_compared).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.3285001937146879,\n",
       " 1.0461726604646144,\n",
       " -1.7345487029390991,\n",
       " 1.272174973055252,\n",
       " -0.7214558165319331,\n",
       " 0.5224930556146146,\n",
       " -0.11268205722151096,\n",
       " -0.14343305875357407,\n",
       " 0.552586020991095,\n",
       " -1.6467055406256832,\n",
       " 0.6664479314573448,\n",
       " -1.8386724265343677,\n",
       " 0.7302832181586759,\n",
       " 0.6833366707670256,\n",
       " 0.1995678796536017,\n",
       " -0.17415108533341858,\n",
       " 0.8772457453887108,\n",
       " -0.11892497110267058,\n",
       " 0.6175223593333428,\n",
       " -0.7543665934980727,\n",
       " -0.36353426982230436,\n",
       " -0.5471759755398041,\n",
       " -1.4584214892350396,\n",
       " -0.7467178434903627,\n",
       " 1.3243285411927292,\n",
       " 2.1971309682655304]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t \n",
    "scores = np.array([x[1] for x in t],dtype=float)\n",
    "scores_standarized = [(x - scores.mean())/scores.std() for x in scores]\n",
    "scores_standarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7492966550299953\n",
      "0.35838971834824074\n",
      "-3.6421836694051715\n",
      "0.5337566638904002\n",
      "-1.5335255686763896\n",
      "-0.27116383578494063\n",
      "-0.07315699991429062\n",
      "-0.35248162150251877\n",
      "-0.20424514891865797\n",
      "-2.0675020019409907\n",
      "-0.05355751696260591\n",
      "-2.2962946093394776\n",
      "0.5764732376444341\n",
      "0.6538135002808666\n",
      "-0.3271025309448874\n",
      "-0.5258993188553471\n",
      "0.4380364233285022\n",
      "0.4499698326204565\n",
      "0.638634555895662\n",
      "-1.317862725586364\n",
      "-1.3873519205331235\n",
      "1.4026833840509063\n",
      "0.6341375411286543\n",
      "1.5253665791361173\n",
      "3.1867268122451593\n",
      "1.8131602284400248\n"
     ]
    }
   ],
   "source": [
    "l = np.array([x[2] for x in wordsim_data], dtype=float)\n",
    "p = [(x - l.mean())/l.std() for x in l]\n",
    "p\n",
    "\n",
    "for x,y in zip(scores_standarized,p):\n",
    "    print(x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
