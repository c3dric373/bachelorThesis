{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-1,1)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        #pdb.set_trace()\n",
    "        pos_u = pos_u.view(-1)\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        #neg_v = neg_v.view(len(pos_u),-1)\n",
    "        #pdb.set_trace()\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        #pdb.set_trace()\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        #pdb.set_trace()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, power=0.75, neg_samples=2,sampling=False,sampling_treshhold=0.00000001):\n",
    "        self.dataset = dataset\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.vocab_size = int()\n",
    "        self.vocab = set()\n",
    "        self.create_vocab(sampling, sampling_treshhold)\n",
    "        self.pairs = self.generate_pairs(dataset,neg_samples)\n",
    "        self.key_pairs = self.generate_key_pairs(self.pairs)\n",
    "        self.neg_samples=neg_samples\n",
    "        #self.dataset_tensors=self.create_dataset_tensors()\n",
    "        \n",
    "\n",
    "        \n",
    "    def generate_pairs(self, dataset, ctx_window):\n",
    "            print(\"Generating pairs\")\n",
    "            pairs = []\n",
    "            for sentence in dataset:\n",
    "                for i,word in enumerate(sentence):\n",
    "                    for j in range(1,ctx_window):\n",
    "                        if(i+j<len(sentence)):\n",
    "                            pairs.append((word,sentence[i+j]))\n",
    "                        if((i-j)>0):\n",
    "                            pairs.append((word,sentence[i-j]))\n",
    "                            \n",
    "            return pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.key_pairs)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.key_pairs\n",
    "    \n",
    "    def get_neg_samples(self, count, batch_size):\n",
    "        neg_v = []\n",
    "        for x in range(1,batch_size+1):\n",
    "            neg_v.append(random.sample(range(0,self.vocab_size),count))\n",
    "        return torch.tensor(neg_v).view(batch_size,-1)\n",
    "   \n",
    "    \"\"\" Defines the probability of choosing a negative sampling, set empiraccaly by mikolov\"\"\"\n",
    "    def make_neg_table(self, power):\n",
    "        pow_frequency = np.array([self.vocab_ctx[self.index2ctx[i]] for i in range(len(self.vocab_ctx))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def generate_key_pairs(self,pairs):\n",
    "        print(\"Generating key_pairs\")\n",
    "        key_pairs = []\n",
    "        #print(pairs)\n",
    "        for x,y in pairs:\n",
    "            key_pairs.append((self.word2idx.get(x),self.word2idx.get(y)))\n",
    "        print(\"finished creating key_pairs\")\n",
    "        return key_pairs\n",
    "    \n",
    "    \"\"\"\"Creating vocabulary: first counting all words then deleting all frequent\n",
    "    words, then creating dictionary with a one to one mapping int to word\"\"\"\n",
    "    def create_vocab(self,sampling, treshhold):\n",
    "        print(\"Creating vocab\")\n",
    "        if sampling:\n",
    "            self.create_vocab_with_sampling(treshhold)\n",
    "        else:\n",
    "            for i,sentence in enumerate(self.dataset):\n",
    "                for word in sentence:\n",
    "                    self.word_count[word] += 1\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def create_vocab_with_sampling(self,treshhold):\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "                for word in sentence:\n",
    "                    self.word_count[word] += 1\n",
    "                    \n",
    "        sampling_table = self.make_sampling_table(treshhold)  \n",
    "        print(sampling_table)\n",
    "        #pdb.set_trace()\n",
    "        assert len(sampling_table)== len(self.word_count)\n",
    "        #pdb.set_trace()\n",
    "        sampled_words = [word for i,word in enumerate(self.word_count.keys()) if random.random() < sampling_table[i]]\n",
    "        self.sampled_words = sampled_words\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "            for word in sentence: \n",
    "                if word in sampled_words:\n",
    "                    sentence.remove(word)\n",
    "                else: \n",
    "                    self.vocab.add(word)\n",
    "                    \n",
    "        \n",
    "    def make_sampling_table(self,treshhold): \n",
    "        count = np.array([x for x in self.word_count.values()])\n",
    "        table = [1-x for x in list( np.sqrt(treshhold/(count)))]\n",
    "        return table\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35,  0],\n",
       "        [17, 16],\n",
       "        [34, 45],\n",
       "        [19, 32],\n",
       "        [ 4,  6],\n",
       "        [33, 32],\n",
       "        [ 8, 36],\n",
       "        [44, 36],\n",
       "        [18, 25],\n",
       "        [42,  5]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = []\n",
    "for x in range(1,11):\n",
    "    tmp.append(random.sample(range(0,50),2))\n",
    "t = torch.tensor(tmp)\n",
    "t.view(10,-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "class W2V():\n",
    "    def __init__(self, data,dim=100, neg_samples=3, alpha=0.01, iterations=10, batch_size=500, \n",
    "                 shuffle=False,use_cuda=True,workers=4):\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "\n",
    "        self.neg_samples = neg_samples\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab), self.dim)\n",
    "        #self.model.cuda()\n",
    "        print(device)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha)\n",
    "\n",
    "        self.iterations = iterations\n",
    "        #self.train()        \n",
    " \n",
    "    def train_with_loader(self):\n",
    "        loader = DataLoader(self.data.key_pairs, self.batch_size, self.shuffle, num_workers=self.workers)\n",
    "        print('starting training')\n",
    "        tenth = int(len(loader)/10)\n",
    "        for epoch in range(1,self.iterations):\n",
    "            percent = 0\n",
    "            for i,(pos_u,pos_v) in enumerate(loader):\n",
    "                neg_v = self.data.get_neg_samples(self.data.neg_samples,pos_v.size()[0])\n",
    "                pos_v = pos_v.view(len(neg_v),-1)\n",
    "                #pos_v.cuda()\n",
    "                #pos_u.cuda()\n",
    "                #neg_v.cuda()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                prev_loss = loss\n",
    "            print(\"loss = \" + str(loss))\n",
    "            print(\"{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.idx2word)):\n",
    "            embedding_dict[self.data.idx2word[i]]= embedding[i]\n",
    "        return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    print('starting training')\n",
    "    for epoch in range(1,epochs):\n",
    "        for i,(pos_u,pos_v,neg_v) in enumerate(dataset):\n",
    "            pos_v = pos_v.view(-1,1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward(pos_u,pos_v,neg_v)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"loss = \" + str(loss))\n",
    "        print(\"{0:d} epoch of {1:d}\".format(epoch+1, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "#sentences = LineSentence(datapath('lee_background.cor'))\n",
    "dataset = api.load('text8')\n",
    "text8_dataset = []\n",
    "for x in dataset: \n",
    "    text8_dataset.append(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_first_sentence = []\n",
    "sentence = []\n",
    "for i,x in enumerate(text8_dataset[0]):\n",
    "    sentence.append(x)\n",
    "    if (i%30 == 0 and i>0):\n",
    "        text8_first_sentence.append(sentence)\n",
    "        sentence=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n",
      "Generating pairs\n",
      "Generating key_pairs\n",
      "finished creating key_pairs\n"
     ]
    }
   ],
   "source": [
    "text8_dataset_first_sentence = wDataSet((text8_first_sentence))\n",
    "#text8_wDataset = wDataSet((text8_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "w2v = W2V(text8_dataset_first_sentence)\n",
    "#w2v = W2V(text8_wDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def create_vocab_with_sampling(self,treshhold):\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "                for word in sentence:\n",
    "                    self.word_count[word] += 1\n",
    "                    \n",
    "        sampling_table = self.make_sampling_table(treshhold)  \n",
    "        print(sampling_table)\n",
    "        #pdb.set_trace()\n",
    "        assert len(sampling_table)== len(self.word_count)\n",
    "        #pdb.set_trace()\n",
    "        sampled_words = [word for i,word in enumerate(self.word_count.keys()) if random.random() < sampling_table[i]]\n",
    "        self.sampled_words = sampled_words\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "            for word in sentence: \n",
    "                if word in sampled_words:\n",
    "                    sentence.remove(word)\n",
    "                else: \n",
    "                    self.vocab.add(word)\n",
    "                    \n",
    "        \n",
    "    def make_sampling_table(self,treshhold): \n",
    "        \n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7418011102528389,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42264973081037427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7113248654051871,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.683772233983162,\n",
       " 0.591751709536137,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.6220355269907728,\n",
       " 0.757464374963667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.42264973081037427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5527864045000421,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42264973081037427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6220355269907728,\n",
       " 0.0,\n",
       " 0.6220355269907728,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42264973081037427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42264973081037427,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.2928932188134524,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42264973081037427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = np.array([x for x in text8_dataset_first_sentence.word_count.values()])\n",
    "table = [1-x for x in list( np.sqrt(1/(count)))]\n",
    "list(zip(table,text8_dataset_first_sentence.word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.data.dataset_tensors[0][0].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "loss = tensor(2.0763, grad_fn=<DivBackward0>)\n",
      "2 epoch of 10\n",
      "loss = tensor(2.0773, grad_fn=<DivBackward0>)\n",
      "3 epoch of 10\n",
      "loss = tensor(2.0666, grad_fn=<DivBackward0>)\n",
      "4 epoch of 10\n",
      "loss = tensor(2.0611, grad_fn=<DivBackward0>)\n",
      "5 epoch of 10\n",
      "loss = tensor(2.0623, grad_fn=<DivBackward0>)\n",
      "6 epoch of 10\n",
      "loss = tensor(2.0610, grad_fn=<DivBackward0>)\n",
      "7 epoch of 10\n",
      "loss = tensor(2.0736, grad_fn=<DivBackward0>)\n",
      "8 epoch of 10\n",
      "loss = tensor(2.0371, grad_fn=<DivBackward0>)\n",
      "9 epoch of 10\n",
      "loss = tensor(2.0458, grad_fn=<DivBackward0>)\n",
      "10 epoch of 10\n"
     ]
    }
   ],
   "source": [
    "w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_emb = get_embedding(model, text8_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9448586255311966\n",
      "0.7955727875232697\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x = spatial.distance.cosine(dict_emb['artist'], dict_emb['music'])\n",
    "y = spatial.distance.cosine(dict_emb['anarchism'],dict_emb['music'])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-1626a600654d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mumu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0muv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m     \u001b[0muu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "score = []\n",
    "score_dict = dict()\n",
    "for i,(x,y) in enumerate(itertools.product(text8_dataset.vocab,text8_dataset.vocab)):\n",
    "    if(i%1000000==0):\n",
    "        print(i)\n",
    "    distance = spatial.distance.cosine(dict_emb[x], dict_emb[y])\n",
    "    score_dict[(x,y)] = distance\n",
    "    score.append(distance)\n",
    "print(np.mean(score))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11336667971198654"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(score)\n",
    "#print(score_dict[('anarchism','music')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_closest(score_dict, word):\n",
    "    closest = ()\n",
    "    distance = 3\n",
    "    for (x,y),score in score_dict.items():\n",
    "        #print(x,y,score)\n",
    "        if((x != y) and ((x==word)or(y==word))):\n",
    "            if (distance > score):\n",
    "                closest = (x,y)\n",
    "                distance = score\n",
    "    return closest\n",
    "\n",
    "def get_closest_with_score(dict_emb,y):\n",
    "    distance = 100\n",
    "    for x,emb in dict_emb.items():\n",
    "        if(spatial.distance.cosine(dict_emb[x], dict_emb[y])<distance):\n",
    "            closest = x\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/questions-words.txt\")\n",
    "questions = file.readlines()\n",
    "for i,x in enumerate(questions): \n",
    "    questions[i] = x.rstrip(\"\\n\").split()\n",
    "    if x[0]==':':\n",
    "        del questions[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(questions,dict_emb):\n",
    "    score = []\n",
    "    if all(word in dict_emb for word in questions):\n",
    "        y = dict_emb[questions[0]] -  dict_emb[questions[1]] +  dict_emb[questions[2]]\n",
    "        x = get_closest_with_score(dict_emb,y)\n",
    "        if x == questions[3]:\n",
    "            score.append(1)\n",
    "        else: \n",
    "            score.append(0)\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "score = analogy_task(questions,dict_emb)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = random.sample(dict_emb.keys(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('armed', 'with')\n",
      "('zerzan', 'developments')\n",
      "('writings', 'increase')\n",
      "('list', 'women')\n",
      "('science', 'archons')\n",
      "('mysogyny', 'coo')\n",
      "('dominance', 'existing')\n",
      "('chomsky', 'controlled')\n",
      "('interact', 'assist')\n",
      "('operative', 'cgt')\n"
     ]
    }
   ],
   "source": [
    "for x in words:\n",
    "    print(get_closest(score_dict,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3],\n",
       "        [1, 2],\n",
       "        [3, 1],\n",
       "        [1, 2],\n",
       "        [1, 3],\n",
       "        [2, 3],\n",
       "        [3, 0],\n",
       "        [2, 2],\n",
       "        [0, 2],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2664\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prod_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2665\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in range(1,100000):\n",
    "    tmp = []\n",
    "    for x in range(1,11):\n",
    "        tmp.append(np.random.choice(range(0,10000000),5))\n",
    "    t = torch.tensor(tmp)\n",
    "    t.view(10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 3)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3291\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-20-a8d30616b072>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', 'for x in range(1,100000):\\n      tmp = []\\n    for x in range(1,11):\\n    tmp.append(random.sample(range(0,10000000),5))\\n    t = torch.tensor(tmp)\\n    t.view(10,-1)\\n    \\n')\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2347\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(magic_arg_s, cell)\n",
      "  File \u001b[1;32m\"</usr/lib/python3.7/site-packages/decorator.py:decorator-gen-61>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1248\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr = self.shell.transform_cell(cell)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3076\u001b[0m, in \u001b[1;35mtransform_cell\u001b[0m\n    cell = self.input_transformer_manager.transform_cell(raw_cell)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/inputtransformer2.py\"\u001b[0m, line \u001b[1;32m576\u001b[0m, in \u001b[1;35mtransform_cell\u001b[0m\n    lines = self.do_token_transforms(lines)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/inputtransformer2.py\"\u001b[0m, line \u001b[1;32m561\u001b[0m, in \u001b[1;35mdo_token_transforms\u001b[0m\n    changed, lines = self.do_one_token_transform(lines)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/inputtransformer2.py\"\u001b[0m, line \u001b[1;32m541\u001b[0m, in \u001b[1;35mdo_one_token_transform\u001b[0m\n    tokens_by_line = make_tokens_by_line(lines)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.7/site-packages/IPython/core/inputtransformer2.py\"\u001b[0m, line \u001b[1;32m471\u001b[0m, in \u001b[1;35mmake_tokens_by_line\u001b[0m\n    for token in tokenize.generate_tokens(iter(lines).__next__):\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.7/tokenize.py\"\u001b[0;36m, line \u001b[0;32m572\u001b[0;36m, in \u001b[0;35m_tokenize\u001b[0;36m\u001b[0m\n\u001b[0;31m    (\"<tokenize>\", lnum, pos, line))\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    for x in range(1,11):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in range(1,100000):\n",
    "    tmp = []\n",
    "    for x in range(1,11):\n",
    "        tmp.append(random.sample(range(0,10000000),5))\n",
    "    t = torch.tensor(tmp)\n",
    "    t.view(10,-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TABLE FOR CHOOSING NEG SAMPLES\n",
    "def make_cum_table(power):\n",
    "    pow_frequency = np.array([text8_dataset_first_sentence.word_count[text8_dataset_first_sentence.idx2word[i]] for i in range(len(text8_dataset_first_sentence.vocab))])**power\n",
    "    return pow_frequency / pow_frequency.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00017816 0.00017816 0.00017816 ... 0.00029963 0.00107612 0.00040612]\n"
     ]
    }
   ],
   "source": [
    "print(make_cum_table(0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(102, 0.9010298123715242, 'anarchism'),\n",
       " (2, 0.2932114884917837, 'originated'),\n",
       " (133, 0.913328031779658, 'as'),\n",
       " (184, 0.9263122038302961, 'a'),\n",
       " (16, 0.750112525323897, 'term'),\n",
       " (355, 0.9469493996529437, 'of'),\n",
       " (2, 0.2932114884917837, 'abuse'),\n",
       " (25, 0.8000900202591177, 'first'),\n",
       " (13, 0.7227747375258775, 'used'),\n",
       " (16, 0.750112525323897, 'against'),\n",
       " (15, 0.7419173259076335, 'early'),\n",
       " (6, 0.5919354626205964, 'working'),\n",
       " (11, 0.6986243660690654, 'class'),\n",
       " (1, 0.0004501012955880901, 'radicals'),\n",
       " (9, 0.6668167004318627, 'including'),\n",
       " (521, 0.9562089238619531, 'the'),\n",
       " (2, 0.2932114884917837, 'diggers'),\n",
       " (6, 0.5919354626205964, 'english'),\n",
       " (12, 0.7114547984572723, 'revolution'),\n",
       " (302, 0.9424823666542567, 'and'),\n",
       " (1, 0.0004501012955880901, 'sans'),\n",
       " (1, 0.0004501012955880901, 'culottes'),\n",
       " (7, 0.6222056492897605, 'french'),\n",
       " (2, 0.2932114884917837, 'whilst'),\n",
       " (110, 0.9046966565501122, 'is'),\n",
       " (5, 0.5529876959187812, 'still'),\n",
       " (250, 0.9367829137020695, 'in'),\n",
       " (1, 0.0004501012955880901, 'pejorative'),\n",
       " (6, 0.5919354626205964, 'way'),\n",
       " (227, 0.9336575431122491, 'to'),\n",
       " (3, 0.42290959691454466, 'describe'),\n",
       " (5, 0.5529876959187812, 'any'),\n",
       " (2, 0.2932114884917837, 'act'),\n",
       " (102, 0.9010298123715242, 'that'),\n",
       " (6, 0.5919354626205964, 'violent'),\n",
       " (11, 0.6986243660690654, 'means'),\n",
       " (2, 0.2932114884917837, 'destroy'),\n",
       " (4, 0.5002250506477941, 'organization'),\n",
       " (21, 0.7818803299191234, 'society'),\n",
       " (43, 0.8475700694714774, 'it'),\n",
       " (21, 0.7818803299191234, 'has'),\n",
       " (32, 0.823302872122946, 'also'),\n",
       " (14, 0.7328590527188209, 'been'),\n",
       " (2, 0.2932114884917837, 'taken'),\n",
       " (7, 0.6222056492897605, 'up'),\n",
       " (2, 0.2932114884917837, 'positive'),\n",
       " (4, 0.5002250506477941, 'label'),\n",
       " (57, 0.867606381666492, 'by'),\n",
       " (14, 0.7328590527188209, 'self'),\n",
       " (3, 0.42290959691454466, 'defined')]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "p = one\n",
    "x = list(zip(count,one, text8_dataset_first_sentence.word_count))[0:50]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9010298123715242"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(0,3))\n",
    "y = [0.1,0.4,0.8,1]\n",
    "z = []\n",
    "for x in range(0,100000):\n",
    "    z.append([a for i,a in enumerate(y) if random.random() < y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1 for x in z if 0.1 in x]\n",
    "y = [1 for x in z if 0.4 in x]\n",
    "a = [1 for x in z if 0.8 in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09784\n",
      "0.39883\n",
      "0.80003\n"
     ]
    }
   ],
   "source": [
    "print(len(x)/len(z))\n",
    "print(len(y)/len(z))\n",
    "print(len(a)/len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
