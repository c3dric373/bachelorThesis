{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size_u, emb_size_v,emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size_u, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(emb_size_v, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        neg_v = neg_v.view(len(pos_u),-1)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, power=0.75, neg_samples=10):\n",
    "        self.pairs = self.generate_pairs(dataset,5)\n",
    "        self.neg_samples=neg_samples\n",
    "        \n",
    "    def generate_pairs(self, dataset, ctx_window):\n",
    "            pairs = []\n",
    "            for sentence in dataset:\n",
    "                for i,word in enumerate(sentence):\n",
    "                    for j in range(1,ctx_window):\n",
    "                        if(i+j<len(sentence[i])):\n",
    "                            pairs.append((word,sentence[i+j]))\n",
    "                        if((i-j)>0):\n",
    "                            pairs.append((word,sentence[i-j]))\n",
    "                            \n",
    "            return pairs\n",
    "                            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return pairs[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'text8.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(len(p))\\nsum = 0\\nfor x in p: \\n    sum += len(x)\\nprint(sum/len(p))\\nprint(len(p[0]))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "#sentences = LineSentence(datapath('lee_background.cor'))\n",
    "dataset = api.load('text8')\n",
    "print(type(dataset))\n",
    "p = []\n",
    "for x in dataset: \n",
    "    p.append(x)\n",
    "    \n",
    "\"\"\"\n",
    "print(len(p))\n",
    "sum = 0\n",
    "for x in p: \n",
    "    sum += len(x)\n",
    "print(sum/len(p))\n",
    "print(len(p[0]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = wDataSet(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "tensor([0, 0, 1,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tmp=[['he', 'is', 'a', 'king'],\n",
    " ['she', 'is', 'a', 'queen'],\n",
    " ['he', 'is', 'a', 'man'],\n",
    " ['she', 'is', 'a', 'woman'],\n",
    " ['warsaw', 'is', 'poland', 'capital'],\n",
    " ['berlin', 'is', 'germany', 'capital'],\n",
    " ['paris', 'is', 'france', 'capital']]\n",
    "print(type(tmp))\n",
    "print(type(p))\n",
    "small_dataset=tmp\n",
    "vocabulary = []\n",
    "for sentence in small_dataset:\n",
    "    for word in sentence:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)\n",
    "print(vocabulary)\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Embedding(5, 1)\n",
      "[0, 1]\n",
      "tensor([[ 0.0485],\n",
      "        [ 0.0485],\n",
      "        [-0.0991],\n",
      "        ...,\n",
      "        [ 0.0485],\n",
      "        [ 0.0485],\n",
      "        [ 0.0485]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "v_embeddings = nn.Embedding(5, 1, sparse=False)\n",
    "u_embeddings = nn.Embedding(5, 1, sparse=False)\n",
    "initrange = 0.5 / 5\n",
    "u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "v_embeddings.weight.data.uniform_(-0, 0)\n",
    "print(u_embeddings)\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(index):\n",
    "    x = torch.zeros((vocabulary_size)).long()\n",
    "    x[index] = 1\n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class Test(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(Test, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        neg_v = neg_v.view(len(pos_u),-1)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
