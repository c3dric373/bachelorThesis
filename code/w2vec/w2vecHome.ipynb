{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size_u, emb_size_v,emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size_u, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(emb_size_v, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        neg_v = neg_v.view(len(pos_u),-1)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, power=0.75, neg_samples=10):\n",
    "        self.pairs = self.generate_pairs(dataset,5)\n",
    "        self.neg_samples=neg_samples\n",
    "        \n",
    "    def generate_pairs(self, dataset, ctx_window):\n",
    "            pairs = []\n",
    "            for sentence in dataset:\n",
    "                for i,word in enumerate(sentence):\n",
    "                    for j in range(1,ctx_window):\n",
    "                        if(i+j<len(sentence[i])):\n",
    "                            pairs.append((word,sentence[i+j]))\n",
    "                        if((i-j)>0):\n",
    "                            pairs.append((word,sentence[i-j]))\n",
    "                            \n",
    "            return pairs\n",
    "                            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return pairs[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'text8.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(len(p))\\nsum = 0\\nfor x in p: \\n    sum += len(x)\\nprint(sum/len(p))\\nprint(len(p[0]))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "#sentences = LineSentence(datapath('lee_background.cor'))\n",
    "dataset = api.load('text8')\n",
    "print(type(dataset))\n",
    "p = []\n",
    "for x in dataset: \n",
    "    p.append(x)\n",
    "    \n",
    "\"\"\"\n",
    "print(len(p))\n",
    "sum = 0\n",
    "for x in p: \n",
    "    sum += len(x)\n",
    "print(sum/len(p))\n",
    "print(len(p[0]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = wDataSet(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "['he', 'is', 'a', 'king', 'she', 'queen', 'man', 'woman', 'warsaw', 'poland', 'capital', 'berlin', 'germany', 'paris', 'france']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tmp=[['he', 'is', 'a', 'king'],\n",
    " ['she', 'is', 'a', 'queen'],\n",
    " ['he', 'is', 'a', 'man'],\n",
    " ['she', 'is', 'a', 'woman'],\n",
    " ['warsaw', 'is', 'poland', 'capital'],\n",
    " ['berlin', 'is', 'germany', 'capital'],\n",
    " ['paris', 'is', 'france', 'capital']]\n",
    "print(type(tmp))\n",
    "print(type(p))\n",
    "small_dataset=tmp\n",
    "vocabulary = []\n",
    "for sentence in small_dataset:\n",
    "    for word in sentence:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)\n",
    "print(vocabulary)\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "def create_pairs(dataset, ctx_window):\n",
    "            pairs = []\n",
    "            for sentence in dataset:\n",
    "                for i,word in enumerate(sentence):\n",
    "                    for j in range(1,ctx_window+1):\n",
    "                        if((i+j)<len(sentence)):\n",
    "                            pairs.append((word,sentence[i+j]))\n",
    "                        if((i-j)>0):\n",
    "                            pairs.append((word,sentence[i-j]))\n",
    "            return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(5, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "v_embeddings = nn.Embedding(5, 1, sparse=False)\n",
    "u_embeddings = nn.Embedding(5, 1, sparse=False)\n",
    "initrange = 0.5 / 5\n",
    "u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "v_embeddings.weight.data.uniform_(-0, 0)\n",
    "print(u_embeddings)\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(index):\n",
    "    x = torch.zeros((vocabulary_size)).long()\n",
    "    x[index] = 1\n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class Test(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(Test, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        neg_v = neg_v.view(len(pos_u),-1)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Test(vocabulary_size, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d0a0707ca037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpos_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_v' is not defined"
     ]
    }
   ],
   "source": [
    "print(pos_v)\n",
    "pos_v = pos_v.view(-1,1)\n",
    "print(pos_v)\n",
    "print(model.forward((one_hot_vector(1)),pos_v,(one_hot_vector(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epoch in range(1,epochs):\n",
    "        for pos_u,pos_v,neg_v in dataset:\n",
    "            pos_v = pos_v.view(-1,1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward(pos_u,pos_v,neg_v)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"loss = \" + str(loss))\n",
    "        print(\"{0:d} epoch of {1:d}\".format(epoch+1, epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_pairs(pairs):\n",
    "    key_pairs = []\n",
    "    for x,y in pairs:\n",
    "        key_pairs.append((word2idx.get(x),word2idx.get(y)))\n",
    "    return key_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_dataset_with_samples(dataset, vocab_size, neg_samples=2):\n",
    "    dataset_with_samples = []\n",
    "    pairs = create_key_pairs(create_pairs(small_dataset,neg_samples))\n",
    "    for x,y in pairs: \n",
    "        neg_v = one_hot_vector(random.randint(0,vocab_size-1))\n",
    "        for z in random.sample(range(0,vocab_size),neg_samples - 1):\n",
    "            neg_v = torch.cat((neg_v,one_hot_vector(z)))\n",
    "        dataset_with_samples.append((one_hot_vector(x),one_hot_vector(y).view(-1,1),neg_v))\n",
    "    return dataset_with_samples\n",
    "        \n",
    "dataset_with_samples = create_dataset_with_samples(small_dataset,vocabulary_size)         \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_samples[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(2.0792, grad_fn=<DivBackward0>)\n",
      "2 epoch of 100\n",
      "loss = tensor(2.0789, grad_fn=<DivBackward0>)\n",
      "3 epoch of 100\n",
      "loss = tensor(2.0784, grad_fn=<DivBackward0>)\n",
      "4 epoch of 100\n",
      "loss = tensor(2.0776, grad_fn=<DivBackward0>)\n",
      "5 epoch of 100\n",
      "loss = tensor(2.0762, grad_fn=<DivBackward0>)\n",
      "6 epoch of 100\n",
      "loss = tensor(2.0738, grad_fn=<DivBackward0>)\n",
      "7 epoch of 100\n",
      "loss = tensor(2.0697, grad_fn=<DivBackward0>)\n",
      "8 epoch of 100\n",
      "loss = tensor(2.0630, grad_fn=<DivBackward0>)\n",
      "9 epoch of 100\n",
      "loss = tensor(2.0523, grad_fn=<DivBackward0>)\n",
      "10 epoch of 100\n",
      "loss = tensor(2.0361, grad_fn=<DivBackward0>)\n",
      "11 epoch of 100\n",
      "loss = tensor(2.0138, grad_fn=<DivBackward0>)\n",
      "12 epoch of 100\n",
      "loss = tensor(1.9870, grad_fn=<DivBackward0>)\n",
      "13 epoch of 100\n",
      "loss = tensor(1.9601, grad_fn=<DivBackward0>)\n",
      "14 epoch of 100\n",
      "loss = tensor(1.9382, grad_fn=<DivBackward0>)\n",
      "15 epoch of 100\n",
      "loss = tensor(1.9237, grad_fn=<DivBackward0>)\n",
      "16 epoch of 100\n",
      "loss = tensor(1.9159, grad_fn=<DivBackward0>)\n",
      "17 epoch of 100\n",
      "loss = tensor(1.9121, grad_fn=<DivBackward0>)\n",
      "18 epoch of 100\n",
      "loss = tensor(1.9106, grad_fn=<DivBackward0>)\n",
      "19 epoch of 100\n",
      "loss = tensor(1.9099, grad_fn=<DivBackward0>)\n",
      "20 epoch of 100\n",
      "loss = tensor(1.9097, grad_fn=<DivBackward0>)\n",
      "21 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "22 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "23 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "24 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "25 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "26 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "27 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "28 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "29 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "30 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "31 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "32 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "33 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "34 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "35 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "36 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "37 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "38 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "39 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "40 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "41 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "42 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "43 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "44 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "45 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "46 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "47 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "48 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "49 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "50 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "51 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "52 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "53 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "54 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "55 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "56 epoch of 100\n",
      "loss = tensor(1.9096, grad_fn=<DivBackward0>)\n",
      "57 epoch of 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-604405425f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_with_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-597eaa9037c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0:d} epoch of {1:d}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,dataset_with_samples,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
