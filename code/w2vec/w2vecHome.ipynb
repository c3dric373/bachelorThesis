{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(Test, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        neg_v = neg_v.view(len(pos_u),-1)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, power=0.75, neg_samples=10):\n",
    "        self.dataset = dataset\n",
    "        self.neg_samples=neg_samples\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.vocab = list()\n",
    "        self.create_vocab()\n",
    "        self.pairs = self.generate_pairs(dataset,5)\n",
    "\n",
    "        \n",
    "    def generate_pairs(self, dataset, ctx_window):\n",
    "            pairs = []\n",
    "            for sentence in dataset:\n",
    "                for i,word in enumerate(sentence):\n",
    "                    for j in range(1,ctx_window):\n",
    "                        if(i+j<len(sentence[i])):\n",
    "                            pairs.append((word,sentence[i+j]))\n",
    "                        if((i-j)>0):\n",
    "                            pairs.append((word,sentence[i-j]))\n",
    "                            \n",
    "            return pairs\n",
    "    \n",
    "    def one_hot_vector(index):\n",
    "        vector = torch.zeros((vocabulary_size)).long()\n",
    "        vector[index] = 1\n",
    "        return vector \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "    \n",
    "    def create_key_pairs(pairs):\n",
    "        key_pairs = []\n",
    "        for x,y in pairs:\n",
    "            key_pairs.append((word2idx.get(x),word2idx.get(y)))\n",
    "        return key_pairs\n",
    "    \n",
    "    def create_dataset_with_samples(dataset, vocab_size, neg_samples=2):\n",
    "        dataset_with_samples = []\n",
    "        pairs = create_key_pairs(create_pairs(dataset,neg_samples))\n",
    "        for x,y in pairs: \n",
    "            neg_v = one_hot_vector(random.randint(0,vocab_size-1))\n",
    "            for z in random.sample(range(0,vocab_size),neg_samples - 1):\n",
    "                neg_v = torch.cat((neg_v,one_hot_vector(z)))\n",
    "            dataset_with_samples.append((one_hot_vector(x),one_hot_vector(y).view(-1,1),neg_v))\n",
    "        return dataset_with_samples\n",
    "    \n",
    "    def create_vocab(self):\n",
    "        for sentence in self.dataset:\n",
    "                for word in sentence:\n",
    "                    if word not in vocabulary:\n",
    "                        vocab.append(word)\n",
    "            \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epoch in range(1,epochs):\n",
    "        for pos_u,pos_v,neg_v in dataset:\n",
    "            pos_v = pos_v.view(-1,1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward(pos_u,pos_v,neg_v)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"loss = \" + str(loss))\n",
    "        print(\"{0:d} epoch of {1:d}\".format(epoch+1, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(5, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset_with_samples = create_dataset_with_samples(small_dataset,vocabulary_size)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Test(vocabulary_size, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,dataset_with_samples,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'text8.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(len(p))\\nsum = 0\\nfor x in p: \\n    sum += len(x)\\nprint(sum/len(p))\\nprint(len(p[0]))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "#sentences = LineSentence(datapath('lee_background.cor'))\n",
    "dataset = api.load('text8')\n",
    "print(type(dataset))\n",
    "p = []\n",
    "for x in dataset: \n",
    "    p.append(x)\n",
    "    \n",
    "\"\"\"\n",
    "print(len(p))\n",
    "sum = 0\n",
    "for x in p: \n",
    "    sum += len(x)\n",
    "print(sum/len(p))\n",
    "print(len(p[0]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = wDataSet(p)\n",
    "#print(dataset.pairs[0:6])\n",
    "#print(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "['he', 'is', 'a', 'king', 'she', 'queen', 'man', 'woman', 'warsaw', 'poland', 'capital', 'berlin', 'germany', 'paris', 'france']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tmp=[['he', 'is', 'a', 'king'],\n",
    " ['she', 'is', 'a', 'queen'],\n",
    " ['he', 'is', 'a', 'man'],\n",
    " ['she', 'is', 'a', 'woman'],\n",
    " ['warsaw', 'is', 'poland', 'capital'],\n",
    " ['berlin', 'is', 'germany', 'capital'],\n",
    " ['paris', 'is', 'france', 'capital']]\n",
    "print(type(tmp))\n",
    "print(type(p))\n",
    "small_dataset=tmp\n",
    "vocabulary = []\n",
    "for sentence in small_dataset:\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
