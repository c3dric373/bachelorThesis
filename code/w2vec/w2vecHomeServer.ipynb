{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-1,1)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        #pdb.set_trace()\n",
    "        pos_u = pos_u.view(-1)\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        #neg_v = neg_v.view(len(pos_u),-1)\n",
    "        #pdb.set_trace()\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        #pdb.set_trace()\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        #pdb.set_trace()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, power=0.75,ctx_window=2):\n",
    "        self.ctx_window = ctx_window\n",
    "        self.dataset = dataset\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.vocab_size = int()\n",
    "        self.vocab = set()\n",
    "        self.create_vocab()\n",
    "        self.pairs = self.generate_pairs()\n",
    "        self.key_pairs = self.generate_key_pairs(self.pairs)\n",
    "        #self.dataset_tensors=self.create_dataset_tensors()\n",
    "        \n",
    "\n",
    "        \n",
    "    def generate_pairs(self):\n",
    "            print(\"Generating pairs\")\n",
    "            pairs = []\n",
    "            for sentence in self.dataset:\n",
    "                for i,word in enumerate(sentence):\n",
    "                    for j in range(1,self.ctx_window):\n",
    "                        if(i+j<len(sentence)):\n",
    "                            pairs.append((word,sentence[i+j]))\n",
    "                        if((i-j)>0):\n",
    "                            pairs.append((word,sentence[i-j]))\n",
    "                            \n",
    "            return pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.key_pairs)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.key_pairs\n",
    "    \n",
    "    def get_neg_samples(self, count, batch_size):\n",
    "        neg_v = []\n",
    "        for x in range(1,batch_size+1):\n",
    "            neg_v.append(random.sample(range(0,self.vocab_size),count))\n",
    "        return torch.tensor(neg_v).view(batch_size,-1)\n",
    "        \n",
    "    \n",
    "    def generate_key_pairs(self,pairs):\n",
    "        print(\"Generating key_pairs\")\n",
    "        key_pairs = []\n",
    "        #print(pairs)\n",
    "        for x,y in pairs:\n",
    "            key_pairs.append((self.word2idx.get(x),self.word2idx.get(y)))\n",
    "        print(\"finished creating key_pairs\")\n",
    "        return key_pairs\n",
    "    \n",
    "    \n",
    "    def create_vocab(self):\n",
    "        print(\"Creating vocab\")\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour is: 18\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "currentDT = datetime.datetime.now()\n",
    "print (\"Current Hour is: %d\" % currentDT.hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class W2V():\n",
    "    def __init__(self, data,dim=100, neg_samples=2, alpha=0.01, iterations=10, batch_size=15000, \n",
    "                 shuffle=False,use_cuda=True,workers=7):\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "\n",
    "        self.neg_samples = neg_samples\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab), self.dim)\n",
    "        #self.model.cuda()\n",
    "        print(device)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha)\n",
    "\n",
    "        self.iterations = iterations\n",
    "        #self.train()        \n",
    " \n",
    "    def train_with_loader(self):\n",
    "        loader = DataLoader(self.data.key_pairs, self.batch_size, self.shuffle, num_workers=self.workers)\n",
    "        print('starting training')\n",
    "        tenth = int(len(loader)/10)\n",
    "        for epoch in range(1,self.iterations):\n",
    "            percent = 0\n",
    "            start = time.time()\n",
    "            for i,(pos_u,pos_v) in enumerate(loader):\n",
    "                if(i%tenth == 0 ):\n",
    "                    end = time.time()\n",
    "                    hours, rem = divmod(end-start, 3600)\n",
    "                    minutes, seconds = divmod(rem, 60)\n",
    "                    print(\"Time since start: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "                    print(str(percent) + \"% of epoch is done\" )\n",
    "                    percent+=10\n",
    "                neg_v = self.data.get_neg_samples(self.neg_samples,pos_v.size()[0])\n",
    "                pos_v = pos_v.view(len(neg_v),-1)\n",
    "                #pos_v.cuda()\n",
    "                #pos_u.cuda()\n",
    "                #neg_v.cuda()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                prev_loss = loss\n",
    "            print(\"loss = \" + str(loss))\n",
    "            print(\"{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.idx2word)):\n",
    "            embedding_dict[self.data.idx2word[i]]= embedding[i]\n",
    "        return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    print('starting training')\n",
    "    for epoch in range(1,epochs):\n",
    "        for i,(pos_u,pos_v,neg_v) in enumerate(dataset):\n",
    "            pos_v = pos_v.view(-1,1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward(pos_u,pos_v,neg_v)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"loss = \" + str(loss))\n",
    "        print(\"{0:d} epoch of {1:d}\".format(epoch+1, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "#sentences = LineSentence(datapath('lee_background.cor'))\n",
    "dataset = api.load('text8')\n",
    "text8_dataset = []\n",
    "for x in dataset: \n",
    "    text8_dataset.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_first_sentence = []\n",
    "sentence = []\n",
    "for i,x in enumerate(text8_dataset[0]):\n",
    "    sentence.append(x)\n",
    "    if (i%30 == 0 and i>0):\n",
    "        text8_first_sentence.append(sentence)\n",
    "        sentence=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n",
      "Generating pairs\n",
      "Generating key_pairs\n",
      "finished creating key_pairs\n"
     ]
    }
   ],
   "source": [
    "#text8_dataset_first_sentence = wDataSet((text8_first_sentence))\n",
    "text8_wDataset = wDataSet((text8_dataset),ctx_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34005311"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text8_wDataset.key_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#w2v = W2V(text8_dataset_first_sentence)\n",
    "w2v = W2V(text8_wDataset,neg_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Time since start: 00:00:02.08\n",
      "0% of epoch is done\n",
      "Time since start: 00:08:35.96\n",
      "10% of epoch is done\n",
      "Time since start: 00:17:05.11\n",
      "20% of epoch is done\n",
      "Time since start: 00:25:27.44\n",
      "30% of epoch is done\n",
      "Time since start: 00:33:51.47\n",
      "40% of epoch is done\n",
      "Time since start: 00:42:12.10\n",
      "50% of epoch is done\n",
      "Time since start: 00:50:41.31\n",
      "60% of epoch is done\n",
      "Time since start: 00:59:03.37\n",
      "70% of epoch is done\n"
     ]
    }
   ],
   "source": [
    "w2v.train_with_loader()\n",
    "bs = 'bs'+str(w2v.batch_size)\n",
    "neg = 'neg'+str(w2v.neg_samples)\n",
    "dim = 'dim' + str(w2v.dim)\n",
    "epochs = 'epochs'+ str(w2v.iterations)\n",
    "ctxw = 'ctxw' + str(w2v.data.ctx_window)\n",
    "l = [bs,neg,dim,epochs,ctxw]\n",
    "filename= \"w2v\"\n",
    "for x in l: \n",
    "    filename += (str(x) + '_')\n",
    "filename += '.pkl'\n",
    "import pickle\n",
    "dict_emb = w2v.get_embedding()\n",
    "pickle_out = open( \"dict_emb_\" + filename ,\"wb\")\n",
    "pickle.dump(dict_emb, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w2vbs20000_neg4_dim100_epochs10_ctxw3_.pkl'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "bs = 'bs'+str(w2v.batch_size)\n",
    "neg = 'neg'+str(w2v.neg_samples)\n",
    "dim = 'dim' + str(w2v.dim)\n",
    "epochs = 'epochs'+ str(w2v.iterations)\n",
    "ctxw = 'ctxw' + str(w2v.data.ctx_window)\n",
    "l = [bs,neg,dim,epochs,ctxw]\n",
    "filename= \"w2v\"\n",
    "for x in l: \n",
    "    filename += (str(x) + '_')\n",
    "filename += '.pkl'\n",
    "filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.W2V'>: it's not the same object as __main__.W2V",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-090a9e2c78ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpickle_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpickle_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.W2V'>: it's not the same object as __main__.W2V"
     ]
    }
   ],
   "source": [
    "pickle_out = open(filename,\"wb\")\n",
    "pickle.dump(w2v, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('w2v_bs10k_neg3_dim100.pkl', 'wb') as output:\n",
    "        pickle.dump(w2v_loaded, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253854"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "len(text8_wDataset.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_emb = w2v.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8569162636995316\n",
      "0.9923342950642109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9120782390236855"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x = spatial.distance.cosine(dict_emb['artist'], dict_emb['music'])\n",
    "y = spatial.distance.cosine(dict_emb['anarchism'],dict_emb['music'])\n",
    "z = spatial.distance.cosine(dict_emb['revolution'],dict_emb['anarchism'])\n",
    "\n",
    "l = ['music','anarchism','artist','revolution','philosophy','creatine']\n",
    "print(x)\n",
    "print(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(word):\n",
    "    for x in dict_emb.keys():\n",
    "        yield(x, spatial.distance.cosine(dict_emb[word],dict_emb[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for x in l:\n",
    "    p.append((list(get_distances(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('music', 0.0)\n",
      "('komnenos', 1.4891326129436493)\n",
      "('anarchism', 0.0)\n",
      "('gondwanda', 1.4429270923137665)\n",
      "('artist', 0.0)\n",
      "('frittura', 1.4317588806152344)\n",
      "('revolution', 0.0)\n",
      "('diacriticals', 1.4835915863513947)\n",
      "('philosophy', 0.0)\n",
      "('bomblet', 1.4533373713493347)\n",
      "('creatine', 0.0)\n",
      "('aupperle', 1.4633850157260895)\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "import heapq\n",
    "for x in p:\n",
    "    print((heapq.nsmallest(5,x, key=itemgetter(1)))[0])\n",
    "    print((heapq.nlargest(5,x, key=itemgetter(1)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spatial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f02140f2dc8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spatial' is not defined"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "score = []\n",
    "score_dict = dict()\n",
    "for i,(x,y) in enumerate(itertools.product(text8_wDataset.vocab,text8_wDataset.vocab)):\n",
    "    if(i%10000000==0):\n",
    "        print(i)\n",
    "    distance = spatial.distance.cosine(dict_emb[x], dict_emb[y])\n",
    "    score_dict[(x,y)] = distance\n",
    "    score.append(distance)\n",
    "print(np.mean(score))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11336667971198654"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(score)\n",
    "#print(score_dict[('anarchism','music')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_closest(score_dict, word):\n",
    "    closest = ()\n",
    "    distance = 3\n",
    "    for (x,y),score in score_dict.items():\n",
    "        #print(x,y,score)\n",
    "        if((x != y) and ((x==word)or(y==word))):\n",
    "            if (distance > score):\n",
    "                closest = (x,y)\n",
    "                distance = score\n",
    "    return closest\n",
    "\n",
    "def get_closest_with_score(dict_emb,y):\n",
    "    distance = 100\n",
    "    for x,emb in dict_emb.items():\n",
    "        if(spatial.distance.cosine(dict_emb[x], dict_emb[y])<distance):\n",
    "            closest = x\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/questions-words.txt\")\n",
    "questions = file.readlines()\n",
    "for i,x in enumerate(questions): \n",
    "    questions[i] = x.rstrip(\"\\n\").split()\n",
    "    if x[0]==':':\n",
    "        del questions[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(questions,dict_emb):\n",
    "    score = []\n",
    "    if all(word in dict_emb for word in questions):\n",
    "        y = dict_emb[questions[0]] -  dict_emb[questions[1]] +  dict_emb[questions[2]]\n",
    "        x = get_closest_with_score(dict_emb,y)\n",
    "        if x == questions[3]:\n",
    "            score.append(1)\n",
    "        else: \n",
    "            score.append(0)\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "score = analogy_task(questions,dict_emb)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = random.sample(dict_emb.keys(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b298f991b73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_closest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score_dict' is not defined"
     ]
    }
   ],
   "source": [
    "for x in words:\n",
    "    print(get_closest(score_dict,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
