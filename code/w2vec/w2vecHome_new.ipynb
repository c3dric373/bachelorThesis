{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting and preprocessing the data\n",
    "First we get the dataset online, then apply subsampling, then divide the dataset in equally long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntm\n",
      "CPU times: user 19.8 s, sys: 751 ms, total: 20.5 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "import random\n",
    "from itertools import dropwhile\n",
    "\n",
    "def sampling(dataset,threshold=1e-4, min_count=5):\n",
    "    \n",
    "    # Count occurences of each word in the dataset \n",
    "    word_counts = Counter(dataset)  \n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in dataset if random.random() < (1 - p_drop[word]) and word_counts[word]>min_count]\n",
    "    return train_words\n",
    "\n",
    "\"Transforms a list of words to a list of sentences with length=len_sen\"\n",
    "def words_to_sentences(words):\n",
    "    new_ds = []\n",
    "    len_sen = int(len(words)/1700)\n",
    "    len_sen = 20\n",
    "    for i in range(0, len(words), len_sen):\n",
    "        y = [words[i:i + len_sen]]\n",
    "        new_ds.extend(y)\n",
    "    return new_ds\n",
    "    \n",
    "\n",
    "# Get dataset online\n",
    "dataset = api.load('text8')\n",
    "print('ntm')\n",
    "# Convert to list of words\n",
    "text8_ds = []\n",
    "for x in dataset: \n",
    "    for y in x:\n",
    "        text8_ds.append(y)\n",
    "        \n",
    "# Subsampling\n",
    "text8_ds = sampling(text8_ds)\n",
    "\n",
    "# New dataset with sentences of length=20\n",
    "text8_dataset = words_to_sentences(text8_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, spatial \n",
    "import csv, numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import spatial \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#IMPORT DATA\n",
    "def get_wordsim_data():\n",
    "    wordsim_data = [] \n",
    "    with open('./data/wordsim/set1.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for row in reader: \n",
    "            wordsim_data.append(row[0].split(',')[0:3])\n",
    "    del wordsim_data[0]\n",
    "    with open('./data/wordsim/set2.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for i,row in enumerate(reader):\n",
    "            if i!=0:\n",
    "                wordsim_data.append(row[0].split(',')[0:3])\n",
    "\n",
    "    wordsim_vocab = set()\n",
    "    for x in wordsim_data:\n",
    "        wordsim_vocab.add(x[0])\n",
    "        wordsim_vocab.add(x[1])\n",
    "    return wordsim_data\n",
    "\n",
    "#len(wordsim_vocab.intersection(text8_dataset_first_sentence.vocab))\n",
    "def wordsim_task(dict_emb):\n",
    "    wordsim_data = get_wordsim_data()\n",
    "    scores = []\n",
    "    distances = []\n",
    "    distances1 = []\n",
    "    for task in wordsim_data: \n",
    "        if (task[0] in dict_emb.keys() ) and (task[1] in dict_emb.keys()):\n",
    "            scores.append(float(task[2]))\n",
    "            distances.append(10-(spatial.distance.cosine(dict_emb[task[0]], dict_emb[task[1]]))*10)\n",
    "            \n",
    "            \n",
    "    #return stats.zscore(np.array([x[1] for x in out],dtype=float))\n",
    "    return np.corrcoef(scores,distances),zip(scores,distances)\n",
    "\n",
    "#print(wordsim_task(gensim_emb))\n",
    "#wordsim_task(dict_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "with open(\"/home/c3dric/model/Adagrad/dict_emb_OptimAdagrad_momentum0_nesterovFalse_step_size1_gamma1_shuffleFalse_batch_size2000_alpha0.25_dim100_workers1_ctxw5_neg_samples10_use_cudaTrue_iterations3.pkl\", 'rb') as output:\n",
    "        dict_emb = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 0.66890975]), array([0.66890975, 1.        ])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = wordsim_task(dict_emb)\n",
    "list(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-1,1)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        pos_u = pos_u.view(-1).to(device)\n",
    "        pos_v = pos_v.to(device)\n",
    "        neg_v = neg_v.to(device)\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]), tensor(2.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.ones(5,2)\n",
    "y = torch.sum(x)/x.size(0)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    LEN_SEN =20\n",
    "    def __init__(self, dataset, power=0.75,ctx_window=2,):\n",
    "        assert( all(len(sentence)== LEN_SEN) for sentence in dataset)\n",
    "        self.ctx_window = ctx_window\n",
    "        self.dataset = dataset\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.vocab_size = int()\n",
    "        self.vocab = set()\n",
    "        self.create_vocab()\n",
    "        self.pairs = self.generate_pairs()\n",
    "        self.key_pairs = self.generate_key_pairs(self.pairs)\n",
    "        self.power = power        \n",
    "        self.neg_table = self.make_neg_table(self.power)\n",
    "        self.len = self.__len__()\n",
    "\n",
    "        \n",
    "    def generate_pairs(self):\n",
    "        print(\"Generating pairs\")\n",
    "        pairs = []\n",
    "        for sentence in self.dataset:\n",
    "            for i,word in enumerate(sentence):\n",
    "                for j in range(1,random.randint(2,self.ctx_window+1)):\n",
    "                    if(i+j<len(sentence)):\n",
    "                        pairs.append((word,sentence[i+j]))\n",
    "                    if((i-j)>=0):\n",
    "                        pairs.append((word,sentence[i-j]))\n",
    "\n",
    "        return pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        center_pairs = ((LEN_SEN - ctx_window*2)*ctx_window*2) \n",
    "        border_pairs = sum([ctx_window + i for i in range(ctx_window)])*2\n",
    "        return center_pairs + border_pairs        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        id_sen = int(self.len/idx)\n",
    "        return self.key_pairs\n",
    "    \n",
    "    def get_neg_samples(self, count, batch_size):\n",
    "        return torch.tensor(np.random.choice(list(self.idx2word.keys()),size=(batch_size)*count,replace=True,p=self.neg_table)).view(batch_size,-1)\n",
    "   \n",
    "    \"\"\" Defines the probability of choosing a negative sampling, set empiraccaly by mikolov\"\"\"\n",
    "    def make_neg_table(self, power):\n",
    "        pow_frequency = np.array([self.word_count[self.idx2word[i]] for i in range(len(self.word_count))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "        \n",
    "\n",
    "    def generate_key_pairs(self,pairs):\n",
    "        print(\"Generating key_pairs\")\n",
    "        key_pairs = []\n",
    "        for x,y in pairs:\n",
    "            key_pairs.append((self.word2idx.get(x),self.word2idx.get(y)))\n",
    "        print(\"finished creating key_pairs\")\n",
    "        return key_pairs\n",
    "    \n",
    "    \"\"\"\"Creating vocabulary and creating dictionary with a one to one mapping int to word\"\"\"\n",
    "    def create_vocab(self):\n",
    "        print(\"Creating vocab\")\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "            for word in sentence:\n",
    "                self.word_count[word] += 1\n",
    "                self.vocab.add(word)\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "import numbers \n",
    "class W2V():\n",
    "    def __init__(self, data,dim=100, neg_samples=10, alpha=0.4, iterations=20, batch_size=5000, \n",
    "                 shuffle=False,workers=1,momentum=0,nesterov=False,step_size=1,gamma=1,rho=0.9):\n",
    "        #Param for sgd with momentum\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        \n",
    "        # Params for decayin the learning rate \n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # General training params\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "        self.ctxw = self.data.ctx_window\n",
    "        self.neg_samples = neg_samples\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.loss_list = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab), self.dim)\n",
    "        self.model.to(device)\n",
    "        # Param needed for adadelta \n",
    "        self.rho = rho \n",
    "        print(device)\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha, momentum=momentum,nesterov=nesterov)\n",
    "       # self.optimizer = torch.optim.Adadelta(self.model.parameters(),rho=self.rho)\n",
    "        #self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
    "        self.optimizer = torch.optim.Adagrad(self.model.parameters(), lr=alpha)\n",
    "\n",
    "        self.iterations = iterations\n",
    " \n",
    "    def train_with_loader(self,save_embedding=True):\n",
    "        loader = DataLoader(self.data.key_pairs, self.batch_size, self.shuffle, num_workers=self.workers)\n",
    "        print('starting training')\n",
    "        tenth = int(len(loader)/10)\n",
    "        no_improvement = 0\n",
    "        for epoch in range(self.iterations):\n",
    "\n",
    "            percent = 0\n",
    "            start = time.time()\n",
    "            processed_batches = 0 \n",
    "            pairs = 0\n",
    "            cum_loss = 0 \n",
    "            avg_loss =0\n",
    "            best_loss = 10 \n",
    "            \n",
    "            for i,(pos_u,pos_v) in enumerate(loader):\n",
    "                if(i%tenth == 0 ):\n",
    "                    end = time.time()\n",
    "                    hours, rem = divmod(end-start, 3600)\n",
    "                    minutes, seconds = divmod(rem, 60)\n",
    "                    time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "                    if(processed_batches!=0):\n",
    "                        avg_loss = cum_loss / processed_batches\n",
    "                    print(\"0%\" + \"=\" *(int(percent/10))+ str(percent) +\"%, \" + time_since_start + \", avg_loss = {}\".format(avg_loss),end=\"\\r\" )\n",
    "                    percent+=10\n",
    "                    \n",
    "                neg_v = self.data.get_neg_samples(self.neg_samples,pos_v.size()[0])\n",
    "                pos_v = pos_v.view(len(neg_v),-1)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                cum_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                pairs += len(pos_u)\n",
    "                processed_batches += 1\n",
    "                \n",
    "            print(\"\\n{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "            avg_loss = cum_loss / processed_batches\n",
    "            print(\" {0:d} {1:d} batches, pairs {2:d}, avg loss: {3:.5f}\".format(i,processed_batches, pairs,avg_loss))\n",
    "            self.loss_list.append(avg_loss)\n",
    "            self.time = time_since_start\n",
    "            self.model = self.model.to(cpu)\n",
    "            print(\"Current score on wordsim Task: {}\".format(wordsim_task(self.get_embedding())[0][1]))\n",
    "            self.model = self.model.to(device)\n",
    "        \n",
    "            if(avg_loss<best_loss):\n",
    "                best_loss = avg_loss\n",
    "            else:\n",
    "                no_improvement +=1\n",
    "        \n",
    "            if(no_improvement > 3):\n",
    "                self.iterations = epoch\n",
    "                break\n",
    "            \n",
    "        if(save_embedding):\n",
    "            self.save_embedding()\n",
    "            \n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.idx2word)):\n",
    "            embedding_dict[self.data.idx2word[i]]= embedding[i]\n",
    "        return embedding_dict\n",
    "    \n",
    "    def save_embedding(self, with_loss=True):\n",
    "        print('ntm')\n",
    "        # Creating filename\n",
    "        optim = \"Optim\" + str(self.optimizer).split(\" \")[0] + \"_\"\n",
    "        filename = \"dict_emb_\" +  optim + \"_\".join([x + str(y) for x,y in vars(self).items() if isinstance(y, numbers.Number)]) + \".pkl\"\n",
    "        \n",
    "        # Getting Embedding\n",
    "        self.model.to(torch.device('cpu'))\n",
    "        dict_emb = w2v.get_embedding()\n",
    "        \n",
    "        # Adding loss history to embedding\n",
    "        dict_emb['loss_list'] = [x.to(torch.device('cpu')) for x in self.loss_list]\n",
    "        \n",
    "        # Evaluating the model on the word similarity task, saving the score to the embedding \n",
    "        dict_emb['ws_score'] = wordsim_task(dict_emb)[0][1]\n",
    "        \n",
    "        # Saving time spent to calculate 1 epoch\n",
    "        dict_emb['time'] = self.time\n",
    "        \n",
    "        # Logging\n",
    "        print(\"Saving embedding: {} to disk with ws_score: {} \".format(filename,dict_emb['ws_score']))\n",
    "    \n",
    "        # Writing embedding dictionnary to disk\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(dict_emb, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        self.model.to(device)\n",
    "        self.loss_list = [x.to(device) for x in self.loss_list]\n",
    "    \n",
    " \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n",
      "Generating pairs\n",
      "Generating key_pairs\n",
      "finished creating key_pairs\n",
      "cpu\n",
      "starting training\n",
      "0%==========100%, Time:  00:00:05.13, avg_loss = 1949.69531258125\n",
      "1 epoch of 3\n",
      " 544 545 batches, pairs 5449, avg loss: 1953.19678\n",
      "Current score on wordsim Task: [0.78478594 1.        ]\n",
      "0%==20%, Time:  00:00:01.11, avg_loss = 2787.626953125\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-036a10b3a27a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext8_dataset_first_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext8_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctx_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2V\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext8_dataset_first_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-b79a2359338b>\u001b[0m in \u001b[0;36mtrain_with_loader\u001b[0;34m(self, save_embedding)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mpairs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text8_dataset_first_sentence = wDataSet((text8_dataset[0:100]),ctx_window=2)\n",
    "w2v = W2V(text8_dataset_first_sentence, batch_size=10,iterations=3,alpha=10)\n",
    "w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "35\n",
      "sentence number =1\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169], [170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339], [340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509], [510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679], [680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849], [850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019], [1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189], [1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359], [1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529], [1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699]]\n"
     ]
    }
   ],
   "source": [
    "LEN_SEN =20\n",
    "def get_len(ctx_window):\n",
    "    number_pairs = ((LEN_SEN - ctx_window*2)*ctx_window*2) \n",
    "    #print(number_pairs)\n",
    "    #print([ctx_window + i for i in range(ctx_window)])\n",
    "    start_pairs = sum([ctx_window + i for i in range(ctx_window)])*2\n",
    "    #print(len([ctx_window + i for i in range(ctx_window)]))\n",
    "    #print(start_pairs)\n",
    "    return start_pairs + number_pairs\n",
    "print(get_len(5))\n",
    "n_pairs_in_sen = get_len(5)\n",
    "n_pairs = 10* get_len(5)\n",
    "list_of_sen = [i for i in range(1700)]\n",
    "list_of_sen = [list_of_sen[i:i + 170] for i in range(0, len(list_of_sen), 170)]\n",
    "dataset = [[x for x in range(20)] for i in range(10)]\n",
    "ctx_window = 5\n",
    "def get_item(idx):\n",
    "        start_pairs = sum([ctx_window + i for i in range(ctx_window)])\n",
    "        number_pairs = ((LEN_SEN - ctx_window*2)*ctx_window*2)\n",
    "        print(start_pairs)\n",
    "        id_sen = int(idx/n_pairs_in_sen)\n",
    "        sen  = list_of_sen[id_sen]\n",
    "        print(\"sentence number =\" + str(id_sen))\n",
    "        pair_id_in_sen = idx - id_sen*(start_pairs*2 + number_pairs)\n",
    "        if(pair_id_in_sen <= start_pairs):\n",
    "            return -1\n",
    "        elif(pair_id_in_sen >= start_pairs + number_pairs):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "\n",
    "#print(list_of_sen)\n",
    "get_item(206)\n",
    "print(list_of_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "> <ipython-input-28-cb5b77309c61>(39)forward()\n",
      "-> return -1 * (torch.sum(score))/ pos_u.size(0)\n",
      "(Pdb) emb_u.unsqueeze(2)\n",
      "tensor([[[ 2.7795e-03],\n",
      "         [ 3.8988e-03],\n",
      "         [-4.8614e-03],\n",
      "         [-4.2876e-03],\n",
      "         [-3.9053e-03],\n",
      "         [-3.7474e-03],\n",
      "         [-2.5727e-03],\n",
      "         [ 1.4524e-03],\n",
      "         [ 1.8151e-03],\n",
      "         [-1.6951e-03],\n",
      "         [ 4.2642e-04],\n",
      "         [ 4.9185e-03],\n",
      "         [ 4.6678e-04],\n",
      "         [-2.6394e-03],\n",
      "         [ 1.3716e-03],\n",
      "         [ 2.9871e-03],\n",
      "         [ 4.3334e-03],\n",
      "         [ 2.5287e-03],\n",
      "         [-2.1342e-03],\n",
      "         [ 3.7585e-03],\n",
      "         [ 1.4491e-03],\n",
      "         [-1.4925e-03],\n",
      "         [ 3.1388e-03],\n",
      "         [-3.0012e-03],\n",
      "         [-1.8019e-03],\n",
      "         [ 1.4885e-04],\n",
      "         [ 3.5942e-03],\n",
      "         [-2.3165e-03],\n",
      "         [-1.0843e-03],\n",
      "         [-1.7156e-03],\n",
      "         [-2.3725e-03],\n",
      "         [-2.7886e-04],\n",
      "         [ 4.8063e-03],\n",
      "         [-8.1922e-04],\n",
      "         [-2.7201e-04],\n",
      "         [ 3.0028e-03],\n",
      "         [ 4.9118e-03],\n",
      "         [-3.7040e-03],\n",
      "         [ 3.6839e-03],\n",
      "         [-1.0022e-03],\n",
      "         [-3.2024e-03],\n",
      "         [-4.2423e-03],\n",
      "         [ 3.3233e-03],\n",
      "         [-3.8680e-03],\n",
      "         [-1.6963e-03],\n",
      "         [-3.9622e-03],\n",
      "         [ 1.3882e-03],\n",
      "         [ 4.6754e-03],\n",
      "         [ 1.6600e-04],\n",
      "         [-1.0608e-03],\n",
      "         [ 3.4877e-03],\n",
      "         [ 1.6969e-03],\n",
      "         [-3.8134e-03],\n",
      "         [ 4.1009e-03],\n",
      "         [-1.1232e-03],\n",
      "         [-4.1192e-03],\n",
      "         [-8.8679e-04],\n",
      "         [-4.6002e-03],\n",
      "         [-2.1161e-03],\n",
      "         [-3.3816e-03],\n",
      "         [ 1.5778e-04],\n",
      "         [-2.7541e-03],\n",
      "         [ 2.0555e-03],\n",
      "         [ 4.7565e-03],\n",
      "         [-1.7823e-03],\n",
      "         [ 2.7214e-03],\n",
      "         [ 1.8833e-03],\n",
      "         [ 5.8083e-04],\n",
      "         [-1.4601e-03],\n",
      "         [ 1.4126e-03],\n",
      "         [-2.1943e-03],\n",
      "         [-2.7539e-03],\n",
      "         [ 1.3251e-03],\n",
      "         [-4.3452e-03],\n",
      "         [-1.8390e-04],\n",
      "         [ 9.2421e-05],\n",
      "         [ 1.0850e-03],\n",
      "         [-2.4707e-03],\n",
      "         [-2.0778e-03],\n",
      "         [ 4.5479e-03],\n",
      "         [ 5.7475e-04],\n",
      "         [ 4.0962e-03],\n",
      "         [ 1.2754e-03],\n",
      "         [ 3.5186e-03],\n",
      "         [-2.4351e-03],\n",
      "         [-2.9159e-03],\n",
      "         [ 4.3817e-03],\n",
      "         [ 1.1089e-03],\n",
      "         [-4.7051e-03],\n",
      "         [ 1.1064e-03],\n",
      "         [-3.6203e-03],\n",
      "         [ 4.0361e-03],\n",
      "         [-2.0514e-03],\n",
      "         [-4.3118e-03],\n",
      "         [ 4.1043e-03],\n",
      "         [ 3.1151e-03],\n",
      "         [-1.5914e-03],\n",
      "         [-3.0122e-03],\n",
      "         [-2.5739e-03],\n",
      "         [-4.4764e-03]],\n",
      "\n",
      "        [[ 2.7795e-03],\n",
      "         [ 3.8988e-03],\n",
      "         [-4.8614e-03],\n",
      "         [-4.2876e-03],\n",
      "         [-3.9053e-03],\n",
      "         [-3.7474e-03],\n",
      "         [-2.5727e-03],\n",
      "         [ 1.4524e-03],\n",
      "         [ 1.8151e-03],\n",
      "         [-1.6951e-03],\n",
      "         [ 4.2642e-04],\n",
      "         [ 4.9185e-03],\n",
      "         [ 4.6678e-04],\n",
      "         [-2.6394e-03],\n",
      "         [ 1.3716e-03],\n",
      "         [ 2.9871e-03],\n",
      "         [ 4.3334e-03],\n",
      "         [ 2.5287e-03],\n",
      "         [-2.1342e-03],\n",
      "         [ 3.7585e-03],\n",
      "         [ 1.4491e-03],\n",
      "         [-1.4925e-03],\n",
      "         [ 3.1388e-03],\n",
      "         [-3.0012e-03],\n",
      "         [-1.8019e-03],\n",
      "         [ 1.4885e-04],\n",
      "         [ 3.5942e-03],\n",
      "         [-2.3165e-03],\n",
      "         [-1.0843e-03],\n",
      "         [-1.7156e-03],\n",
      "         [-2.3725e-03],\n",
      "         [-2.7886e-04],\n",
      "         [ 4.8063e-03],\n",
      "         [-8.1922e-04],\n",
      "         [-2.7201e-04],\n",
      "         [ 3.0028e-03],\n",
      "         [ 4.9118e-03],\n",
      "         [-3.7040e-03],\n",
      "         [ 3.6839e-03],\n",
      "         [-1.0022e-03],\n",
      "         [-3.2024e-03],\n",
      "         [-4.2423e-03],\n",
      "         [ 3.3233e-03],\n",
      "         [-3.8680e-03],\n",
      "         [-1.6963e-03],\n",
      "         [-3.9622e-03],\n",
      "         [ 1.3882e-03],\n",
      "         [ 4.6754e-03],\n",
      "         [ 1.6600e-04],\n",
      "         [-1.0608e-03],\n",
      "         [ 3.4877e-03],\n",
      "         [ 1.6969e-03],\n",
      "         [-3.8134e-03],\n",
      "         [ 4.1009e-03],\n",
      "         [-1.1232e-03],\n",
      "         [-4.1192e-03],\n",
      "         [-8.8679e-04],\n",
      "         [-4.6002e-03],\n",
      "         [-2.1161e-03],\n",
      "         [-3.3816e-03],\n",
      "         [ 1.5778e-04],\n",
      "         [-2.7541e-03],\n",
      "         [ 2.0555e-03],\n",
      "         [ 4.7565e-03],\n",
      "         [-1.7823e-03],\n",
      "         [ 2.7214e-03],\n",
      "         [ 1.8833e-03],\n",
      "         [ 5.8083e-04],\n",
      "         [-1.4601e-03],\n",
      "         [ 1.4126e-03],\n",
      "         [-2.1943e-03],\n",
      "         [-2.7539e-03],\n",
      "         [ 1.3251e-03],\n",
      "         [-4.3452e-03],\n",
      "         [-1.8390e-04],\n",
      "         [ 9.2421e-05],\n",
      "         [ 1.0850e-03],\n",
      "         [-2.4707e-03],\n",
      "         [-2.0778e-03],\n",
      "         [ 4.5479e-03],\n",
      "         [ 5.7475e-04],\n",
      "         [ 4.0962e-03],\n",
      "         [ 1.2754e-03],\n",
      "         [ 3.5186e-03],\n",
      "         [-2.4351e-03],\n",
      "         [-2.9159e-03],\n",
      "         [ 4.3817e-03],\n",
      "         [ 1.1089e-03],\n",
      "         [-4.7051e-03],\n",
      "         [ 1.1064e-03],\n",
      "         [-3.6203e-03],\n",
      "         [ 4.0361e-03],\n",
      "         [-2.0514e-03],\n",
      "         [-4.3118e-03],\n",
      "         [ 4.1043e-03],\n",
      "         [ 3.1151e-03],\n",
      "         [-1.5914e-03],\n",
      "         [-3.0122e-03],\n",
      "         [-2.5739e-03],\n",
      "         [-4.4764e-03]],\n",
      "\n",
      "        [[-7.9719e-04],\n",
      "         [-2.4089e-03],\n",
      "         [ 2.2443e-03],\n",
      "         [-3.1911e-03],\n",
      "         [-4.8712e-03],\n",
      "         [-2.0596e-03],\n",
      "         [-3.7070e-03],\n",
      "         [ 4.5876e-03],\n",
      "         [-4.1963e-03],\n",
      "         [-4.7180e-03],\n",
      "         [-3.4685e-03],\n",
      "         [-3.9383e-03],\n",
      "         [ 3.2483e-03],\n",
      "         [-4.0756e-03],\n",
      "         [ 1.2821e-03],\n",
      "         [-4.6606e-03],\n",
      "         [ 2.6172e-03],\n",
      "         [ 3.3586e-03],\n",
      "         [ 2.0175e-03],\n",
      "         [-4.9630e-03],\n",
      "         [ 1.3485e-03],\n",
      "         [-3.7889e-03],\n",
      "         [ 2.8067e-03],\n",
      "         [-3.7917e-03],\n",
      "         [ 1.4279e-03],\n",
      "         [-3.7752e-03],\n",
      "         [-1.2464e-04],\n",
      "         [-1.9420e-03],\n",
      "         [-4.8060e-03],\n",
      "         [-3.5626e-05],\n",
      "         [-2.7096e-03],\n",
      "         [ 3.0479e-03],\n",
      "         [ 1.0715e-03],\n",
      "         [ 1.6979e-03],\n",
      "         [ 2.1754e-03],\n",
      "         [ 3.6647e-03],\n",
      "         [ 9.5884e-05],\n",
      "         [-3.3096e-03],\n",
      "         [-4.2876e-03],\n",
      "         [-8.6354e-04],\n",
      "         [-3.1818e-03],\n",
      "         [ 1.8873e-03],\n",
      "         [-3.4017e-03],\n",
      "         [ 2.5559e-03],\n",
      "         [-4.7900e-03],\n",
      "         [ 8.9378e-04],\n",
      "         [-2.3550e-04],\n",
      "         [-5.0475e-05],\n",
      "         [-3.2188e-03],\n",
      "         [-4.0165e-03],\n",
      "         [-2.4203e-03],\n",
      "         [-2.3103e-03],\n",
      "         [ 2.4454e-03],\n",
      "         [-3.0819e-03],\n",
      "         [ 1.5025e-03],\n",
      "         [-4.0828e-03],\n",
      "         [ 4.5757e-03],\n",
      "         [ 2.2397e-03],\n",
      "         [ 3.9862e-03],\n",
      "         [-3.6553e-04],\n",
      "         [ 2.6765e-03],\n",
      "         [ 1.9871e-04],\n",
      "         [ 7.3808e-04],\n",
      "         [ 2.5398e-03],\n",
      "         [ 1.5700e-03],\n",
      "         [-2.8223e-03],\n",
      "         [ 1.1047e-04],\n",
      "         [ 1.1096e-03],\n",
      "         [ 3.7745e-03],\n",
      "         [ 4.2175e-03],\n",
      "         [-1.9744e-03],\n",
      "         [ 2.6022e-03],\n",
      "         [-4.7127e-03],\n",
      "         [-1.7089e-03],\n",
      "         [-2.4017e-03],\n",
      "         [-1.7487e-04],\n",
      "         [ 3.8834e-03],\n",
      "         [-1.1248e-03],\n",
      "         [-3.4785e-03],\n",
      "         [ 2.3310e-04],\n",
      "         [ 4.4181e-03],\n",
      "         [-3.6404e-03],\n",
      "         [-4.6224e-03],\n",
      "         [-3.6461e-04],\n",
      "         [ 1.9876e-03],\n",
      "         [ 1.6210e-03],\n",
      "         [ 2.7672e-03],\n",
      "         [ 4.7495e-03],\n",
      "         [ 5.0255e-04],\n",
      "         [ 1.9280e-04],\n",
      "         [-2.4375e-03],\n",
      "         [ 1.6462e-03],\n",
      "         [ 4.9256e-04],\n",
      "         [-2.9363e-03],\n",
      "         [-2.9158e-03],\n",
      "         [-3.7871e-04],\n",
      "         [ 1.1886e-03],\n",
      "         [ 3.8402e-03],\n",
      "         [-2.8130e-03],\n",
      "         [ 3.2600e-03]]], grad_fn=<UnsqueezeBackward0>)\n",
      "(Pdb) emb_u.unsqueeze(2).size()\n",
      "torch.Size([3, 100, 1])\n",
      "(Pdb) emb_v.size()\n",
      "torch.Size([3, 11, 100])\n",
      "(Pdb) score.size()\n",
      "torch.Size([3, 11])\n",
      "(Pdb) score\n",
      "tensor([[-0.6896, -0.7141, -0.6786, -0.6920, -0.6931, -0.6937, -0.7004, -0.6853,\n",
      "         -0.7053, -0.6883, -0.6902],\n",
      "        [-0.6957, -0.7012, -0.6876, -0.6836, -0.6913, -0.6919, -0.6861, -0.7007,\n",
      "         -0.6981, -0.6925, -0.7034],\n",
      "        [-0.7050, -0.6954, -0.6957, -0.6831, -0.6977, -0.7024, -0.6890, -0.6985,\n",
      "         -0.6944, -0.6878, -0.6991]], grad_fn=<LogSigmoidBackward>)\n",
      "(Pdb) torch.sum(score)\n",
      "tensor(-22.9109, grad_fn=<SumBackward0>)\n",
      "(Pdb) -1*torch.sum(score)/pos_u.size(0)\n",
      "tensor(7.6370, grad_fn=<DivBackward0>)\n",
      "(Pdb) qq\n",
      "*** NameError: name 'qq' is not defined\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-31067b1ba487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-c7ea1df353ae>\u001b[0m in \u001b[0;36mtrain_with_loader\u001b[0;34m(self, save_embedding)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mpos_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-cb5b77309c61>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pos_u, pos_v, neg_v)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mpos_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-cb5b77309c61>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pos_u, pos_v, neg_v)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mpos_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "text8_wDataset = wDataSet((text8_dataset),ctx_window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "0%==========100% Time:  00:00:03.29\n",
      "1 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.08\n",
      "0%==========100% Time:  00:00:02.86\n",
      "2 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.07\n",
      "0%==========100% Time:  00:00:03.11\n",
      "3 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.07\n",
      "0%==========100% Time:  00:00:02.92\n",
      "4 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.06\n",
      "0%==========100% Time:  00:00:03.81\n",
      "5 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.06\n",
      "0%==========100% Time:  00:00:03.47\n",
      "6 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.06\n",
      "0%==========100% Time:  00:00:02.98\n",
      "7 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.05\n",
      "0%==========100% Time:  00:00:03.42\n",
      "8 epoch of 10\n",
      " 369 370 batches, pairs 7400, avg loss: 2.05\n",
      "0%========80% Time:  00:00:02.34\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-31067b1ba487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-14f02b4fd312>\u001b[0m in \u001b[0;36mtrain_with_loader\u001b[0;34m(self, save_embedding)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mpos_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-caf058025281>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pos_u, pos_v, neg_v)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0memb_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_rho = [0.9,0.85,0.8,0.95]\n",
    "for rho in l_rho:\n",
    "    w2v = W2V(text8_wDataset, rho=rho)\n",
    "    w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e901cb14a9bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dict_emb_w2vbs7000_neg7_dim100_epochs20_ctxw10_alpha0-075decayhalf.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "with open(\"dict_emb_w2vbs7000_neg7_dim100_epochs20_ctxw10_alpha0-075decayhalf.pkl\", 'rb') as output:\n",
    "        dict_emb = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30364346504211426\n",
      "0.45958149433135986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5577877461910248"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x = spatial.distance.cosine(gensim_emb['love'], gensim_emb['music'])\n",
    "y = spatial.distance.cosine(gensim_emb['anarchism'],gensim_emb['music'])\n",
    "z = spatial.distance.cosine(gensim_emb['revolution'],gensim_emb['creatine'])\n",
    "\n",
    "l = ['music','anarchism','revolution','philosophy','creatine']\n",
    "print(x)\n",
    "print(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5.5450, requires_grad=True),\n",
       " tensor(5.5439, requires_grad=True),\n",
       " tensor(5.5429, requires_grad=True),\n",
       " tensor(5.5426, requires_grad=True),\n",
       " tensor(5.5412, requires_grad=True),\n",
       " tensor(5.5425, requires_grad=True),\n",
       " tensor(5.5414, requires_grad=True),\n",
       " tensor(5.5402, requires_grad=True),\n",
       " tensor(5.5409, requires_grad=True),\n",
       " tensor(5.5391, requires_grad=True),\n",
       " tensor(5.5376, requires_grad=True),\n",
       " tensor(5.5360, requires_grad=True),\n",
       " tensor(5.5368, requires_grad=True),\n",
       " tensor(5.5365, requires_grad=True),\n",
       " tensor(5.5357, requires_grad=True),\n",
       " tensor(5.5351, requires_grad=True),\n",
       " tensor(5.5354, requires_grad=True),\n",
       " tensor(5.5333, requires_grad=True),\n",
       " tensor(5.5336, requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_emb.pop('loss_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_emb = dict()\n",
    "for sentences in text8_dataset:\n",
    "    for word in sentences:\n",
    "        gensim_emb[word] = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.cum_loss = 0\n",
    "        self.processed_batches = 0\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        avg_loss = self.cum_loss/self.processed_batches\n",
    "        print(\"Epoch #{} end: avg_loss={}\".format(self.epoch,avg_loss))\n",
    "        self.epoch += 1\n",
    "    \n",
    "    def on_batch_end(self, model):\n",
    "        \"\"\"Method called at the end of each batch.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel`\n",
    "            Current model.\n",
    "        \"\"\"\n",
    "        self.cum_loss += model.get_latest_training_loss()\n",
    "        self.processed_batches +=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(word):\n",
    "    for x in dict_emb.keys():\n",
    "        yield(x, spatial.distance.cosine(dict_emb[word],dict_emb[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0927695588b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_dict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_dict_emb_gensim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-0927695588b9>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_dict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_dict_emb_gensim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2449\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2451\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2452\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "n_dict_emb = {(word): (x / np.linalg.norm(x)) for (word, x) in (dict_emb.items())}\n",
    "n_dict_emb_gensim = {(word): (x / np.linalg.norm(x)) for (word, x) in (gensim_emb.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALOGY TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/questions-words.txt\")\n",
    "questions = file.readlines()\n",
    "qeustions_vocab = set()\n",
    "for i,x in enumerate(questions): \n",
    "    questions[i] = x.rstrip(\"\\n\").split()\n",
    "    if x[0]==':':\n",
    "        del questions[i]\n",
    "    else: \n",
    "        for word in x:\n",
    "            questions_vocab.add(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file_object, chunk_size=1024):\n",
    "    \"\"\"Lazy function (generator) to read a file piece by piece.\n",
    "    Default chunk size: 1k.\"\"\"\n",
    "    while True:\n",
    "        data = file_object.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/enwik9\")\n",
    "enwik9 = read_in_chunks(file)\n",
    "l = []\n",
    "for x in enwik9:\n",
    "    l.append(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: logging, save loss, batch_size\n",
    "epoch_logger = EpochLogger()\n",
    "model = Word2Vec(l, size=100,window=5,negative=10, alpha=0.01, min_count=5, workers=4,sg=1, callbacks=[epoch_logger],compute_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(questions,dict_emb):\n",
    "    score = []\n",
    "    if all(word in dict_emb for word in questions):\n",
    "        y = dict_emb[questions[0]] -  dict_emb[questions[1]] +  dict_emb[questions[2]]\n",
    "        x = get_closest_with_score(dict_emb,y)\n",
    "        if x == questions[3]:\n",
    "            score.append(1)\n",
    "        else: \n",
    "            score.append(0)\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# TODO: calculate closest only for a given set of words to get faster computation for analogy task\n",
    "def calculate_sim(dict_emb): \n",
    "    # Create dictionnary with id for every word, this is needed because sometimes we only have access to the dict_emb\n",
    "    # and not the whole model \n",
    "    idx2word = {idx: w for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    \n",
    "    emb_size = len(next(iter(dict_emb.values())))\n",
    "    \n",
    "    # Create an embedding dictionnary with normalized vectors\n",
    "    normalized_dict_emb = {(word): (x / np.linalg.norm(x)) for (word, x) in (dict_emb.items())}\n",
    "    \n",
    "    # Create an vocab_size*emb_size Matrix that holds the normalized embeding of each word in it's row called matrix_row\n",
    "    # Create an emb_size*vocab_size Matrix that holds the normalized embeding of each word in it's colomn  matrix_colomn\n",
    "    for i in range(0,len(dict_emb.keys())):\n",
    "        y = normalized_dict_emb[idx2word[i]]\n",
    "        if i ==0:\n",
    "            matrix_colomn = torch.tensor(y).view(emb_size,1)\n",
    "            matrix_row = torch.tensor(y)\n",
    "        else:\n",
    "            matrix_colomn = torch.cat([matrix_colomn,torch.tensor(y).view(emb_size,1)],1)\n",
    "            matrix_row = torch.cat([matrix_row,torch.tensor(y)])\n",
    "    \n",
    "    matrix_row = matrix_row.view(-1,emb_size)\n",
    "    \n",
    "    matrix_row = matrix_row.to(device)\n",
    "    matrix_colomn = matrix_colomn.to(device)\n",
    "    \n",
    "    return 1-(torch.matmul(matrix_row,matrix_colomn)),word2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_closest(score_dict, word):\n",
    "    closest = ()\n",
    "    distance = 3\n",
    "    for (x,y),score in score_dict.items():\n",
    "        #print(x,y,score)\n",
    "        if((x != y) and ((x==word)or(y==word))):\n",
    "            if (distance > score):\n",
    "                closest = (x,y)\n",
    "                distance = score\n",
    "    return closest\n",
    "\n",
    "def get_closest_with_score(dict_emb,y):\n",
    "    distance = 100\n",
    "    for x,emb in dict_emb.items():\n",
    "        if(spatial.distance.cosine(dict_emb[x], dict_emb[y])<distance):\n",
    "            closest = x\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "list_of_files = []\n",
    "for file in os.listdir(\"/home/c3dric/model/todo\"):\n",
    "        list_of_files.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_files.remove('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_emb = []\n",
    "for file in list_of_files:\n",
    "    with open(\"/home/c3dric/model/todo/\" + file, 'rb') as output:\n",
    "        dict_emb = pickle.load(output)\n",
    "    file_emb.append((file, [float(x) for x in dict_emb['ws_list']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-1966ddd67d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "file_emb[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "with_0 = r'(\\w*)alpha(\\d.\\d*)_\\w*'\n",
    "without_0 = r'(\\w*)alpha(\\d)_\\w*'\n",
    "sgd_reg = r'(\\w*)momentum0_.*'\n",
    "models_sgd = []\n",
    "models_adam = []\n",
    "models_adagrad = []\n",
    "models_mom = []\n",
    "models_nag = []\n",
    "for model in file_emb:\n",
    "    filename = model[0] \n",
    "    if \"OptimAdagrad\" in filename: \n",
    "        models_adagrad.append(model)\n",
    "    if \"OptimAdam\" in filename: \n",
    "        models_adam.append(model)\n",
    "    if \"OptimSGD\" in filename:\n",
    "        if(re.search(sgd_reg,filename)):\n",
    "            models_sgd.append(model)\n",
    "        elif(\"nesterovFalse\" in filename):\n",
    "            models_mom.append(model)\n",
    "        else:\n",
    "            models_nag.append(model)\n",
    "        \n",
    "assert(len(file_emb) == len(models_sgd + models_adam + models_adagrad + models_mom + models_nag))\n",
    "len(models_sgd + models_adam + models_adagrad + models_mom + models_nag)\n",
    "\n",
    "def create_csv(models,csv_file_name):\n",
    "    lr = []\n",
    "    lr_scores = []\n",
    "    epochs = [[] for x in range(20)]\n",
    "    for model in models:\n",
    "        filename = model[0]\n",
    "        if(re.search (without_0,filename)):\n",
    "            alpha =  int(re.search(without_0,filename).group(2))\n",
    "            lr_scores.append((alpha,model[1]))\n",
    "        if(re.search (with_0,filename)):\n",
    "            alpha =  float(re.search(with_0,filename).group(2))\n",
    "            lr_scores.append((alpha,model[1]))\n",
    "    lr_scores = sorted(lr_scores)\n",
    "    lr = [x[0] for x in lr_scores]\n",
    "    scores = [x[1] for  x in lr_scores]\n",
    "    \n",
    "    for x in lr_scores: \n",
    "        ws_scores = x[1]\n",
    "        for i,score in enumerate(ws_scores):\n",
    "            epochs[i].append(score)\n",
    "        for j in range(i+1,20):\n",
    "            epochs[j].append(\"\")\n",
    "            \n",
    "    \n",
    "    output = [lr] + epochs \n",
    "    \n",
    "    with open(csv_file_name, 'w') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerows(output)\n",
    "        csvFile.close()\n",
    "\n",
    "\n",
    "create_csv(models_adam,\"adam.csv\")\n",
    "create_csv(models_adagrad,\"adagrad.csv\")\n",
    "create_csv(models_sgd,\"sgd.csv\")\n",
    "create_csv(models_mom,\"mom.csv\")\n",
    "create_csv(models_nag,\"nag.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_example_conv = file_emb[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/c3dric/model/Adagrad/dict_emb_OptimAdagrad_momentum0_nesterovFalse_step_size1_gamma1_shuffleFalse_batch_size5000_alpha0.25_dim100_workers1_ctxw5_neg_samples10_use_cudaTrue_iterations20.pkl\", 'rb') as output:\n",
    "    dict_emb = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb##\n",
    "import logging\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def reduce_dimensions(model,vocab, vocab_plot, plot_in_notebook = True):\n",
    "\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = []        # positions in vector space\n",
    "    labels = []         # keep track of words to label our data again later\n",
    "    for word in vocab:\n",
    "        vectors.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "    \n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    logging.info('starting tSNE dimensionality reduction. This may take some time.')\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    \n",
    "    x_vals = [v[0] for i,v in enumerate(vectors) if labels[i] in vocab_plot]\n",
    "    y_vals = [v[1] for i,v in enumerate(vectors) if labels[i] in vocab_plot]\n",
    "        \n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=x_vals,\n",
    "        y=y_vals,\n",
    "        mode='text',\n",
    "        text=labels\n",
    "        )\n",
    "    \n",
    "    data = [trace]\n",
    "    \n",
    "    logging.info('All done. Plotting.')\n",
    "    \n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['popcorn', 'fruit', 'eat', 'grocery', 'potato', 'seafood', 'butter', 'cabbage', 'drink', 'wine', 'brandy', 'vodka', 'oil', 'grocery', 'cucumber', 'wine']\n",
      "['card', 'luxury', 'gem', 'investor', 'rading', 'earning', 'insuracnce,', 'dividend', 'profit', 'wealth', 'market', 'stock', 'payment', 'money']\n"
     ]
    }
   ],
   "source": [
    "money_vocab = 'card luxury gem investor rading earning insuracnce, dividend profit wealth market stock payment money'.split(\" \")\n",
    "foods_vocab = \"popcorn fruit eat grocery potato seafood butter cabbage drink wine brandy vodka oil grocery cucumber wine \".split()\n",
    "print(foods_vocab)\n",
    "print(money_vocab)\n",
    "dict_emb.pop(\"time\")\n",
    "dict_emb.pop(\"loss_list\")\n",
    "    \n",
    "reduce_dimensions(dict_emb, dict_emb.keys(),money_vocab + foods_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "l = []\n",
    "for x in list_of_files:\n",
    "    with open(\"models/SGD/test/\" + x, 'rb') as output:\n",
    "            dict_emb = pickle.load(output)\n",
    "    if (x!= \"dict_emb_OptimSGD_momentum0_nesterovFalse_step_size1_gamma1_shuffleFalse_batch_size5000_alpha30_dim100_workers1_ctxw5_neg_samples10_use_cudaTrue_iterations20.pkl\"):\n",
    "        l.append([x/ 1000 for x in dict_emb['loss_list']])\n",
    "    \n",
    "for x in l:\n",
    "    plt.plot(x)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'xticks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-dccadbf41d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#fig.xscale('log', basex=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'xticks'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VPW9//HXJxshBAiQgCyBsMu+BUQWBcS17rVWRC/ettIqKK6ty3W7Xq3XBa97q8XW9oJeFxC0tkr5IZogYNiXCIR9E5JA2MISku/vjwwtpUCGMDNn5uT9fDx4ZHJmhnmfnAdvTr7znfM15xwiIhL74rwOICIioaFCFxHxCRW6iIhPqNBFRHxChS4i4hMqdBERn1Chi4j4hApdRMQnVOgiIj6REMkXS09Pd1lZWZF8SRGRmDd//vwi51xGVY+LaKFnZWWRl5cXyZcUEYl5ZrYhmMdpyEVExCdU6CIiPqFCFxHxCRW6iIhPqNBFRHyiykI3s2Qzm2dmi81suZk9Edje0Mymm9nqwNcG4Y8rIiInE8wZ+iFgmHOuB9ATuMTM+gMPADOcc+2BGYHvRUTEI1XOQ3eVa9TtC3ybGPjjgKuAIYHt7wBfAr8KeUJgRv52Fm8qCcdfLVWolRjPiH4taVgnyesoIjFp78EyXvhiFfdc1IF6yYlhfa2gPlhkZvHAfKAd8Jpzbq6ZNXHObQNwzm0zs8Ynee5oYDRAy5YtqxVy1qpC/jQnqHn1EmLOQW5BEX/66TnEx5nXcURiytrCfdz6xzzWF5cyuH06F3RqEtbXs9NZJNrM0oApwB1AjnMu7Zj7djnnTjmOnp2d7fRJ0djyQd4m7v9wCXcP78C44e29jiMSM2Z+t4M731tIYnwcr93Ym3PbNqr232Vm851z2VU97rRmuTjnSqgcWrkE2G5mTQMv1hTYUY2cEuWu69OCa3s156UZq/hmTbHXcUSinnOO178s4CfvfEtmgxSmjR14RmV+OoKZ5ZIRODPHzGoDw4HvgGnAqMDDRgFTwxVSvGNmPHl1V7LS6zDuvYUU7zvkdSSRqFV6+AhjJy3k2b+u5PLuzfjotgG0aJASsdcP5gy9KTDTzJYA3wLTnXOfAs8AF5rZauDCwPfiQ3VqJfDajb0pOVDG3e8vpqIi+GE6kZpi085Srn19Np8t28YDl57Nyzf0pHZSfEQzBDPLZQnQ6wTbi4ELwhFKok+npvV47IrOPDxlGb/9ai23DWnrdSSRqDG7oIgxkxZQXuH4/S19GdLxhHNEwk6fFJWg3divJT/o3pTnv1jJ/A07vY4j4jnnHBNy1nHz2/NIT63FtLGDPCtzUKHLaTAznrm2Gy0a1OaOSQvZtf+w15FEPHOwrJz7PljCk5+u4IKzGzNlzECy0ut4mkmFLqelbnIir47oTeG+Q9z/4WJOZ9qriF9s232AH//2Gz5asJm7hrfnNzf1IbVWRNcLOiEVupy2bi3q89Blnfhb/g7ezl3vdRyRiMpbv5MrXsmlYMc+3ry5D3cN70BclHzoToUu1XLLgCwu6tyEZ/6Sr8sySI0xae5GRrw1h9Ra8Xw8ZiAXdTnL60j/RIUu1WJmPHddDxrXTWbsuwvYfaDM60giYXP4SAUPTVnKQ1OWMqBtOlPHDKJ9k7pex/oXKnSptvopibxyYy+2lRzkwclLNJ4uvlS49xAjfzeHSXM38ovz2/L2LX2pnxLei2xVlwpdzkjvlg24/+KOfLb0e/537kav44iE1JLNJVzxSg5Lt+zmlRG9eODSs6P6InUqdDljtw5uw9COGTz56QqWb93tdRyRkJi8YDPX/eYb4uOMj24bwBU9mnkdqUoqdDljcXHGC9f3pEFKImMnLWTfoSNeRxKptiPlFTz56QrueX8xvVumMW3sQLo0q+91rKCo0CUkGtZJ4uUberGheD8PT1mq8XSJSbv2H2bU7+cxIWcdtwzI4k8/PYdGqbW8jhU0FbqEzDltGnH38A5MXbSV9/M2eR1H5LTkb9vDla/l8O26XTx7XXcev7ILifGxVZGxlVai3u1D2zGoXTqPTVvOyu/3eh1HJCifLd3Gta/P5vCRCv7v5/25PjvT60jVokKXkIqPM8b/uAeptRIZO2kBpYc1ni7Rq7zC8dzn33H7xAV0alqXT8YOolfLUy68FtVU6BJyjesm89INPSko3MdjU5d7HUfkhPYcLOPWP+bx2sw13NA3k3dH96dxvWSvY50RFbqExcB26dwxtB0fzN/M5AWbvY4j8k8Kduzj6ldz+WpVIU9e3ZVfX9uNWgmRXYwiHFToEjZ3XtCefq0b8h8fL2NN4T6v44gAMCN/O9e8lsvuA2VM/Nk53Ny/FWbR+2Gh06FCl7BJiI/j5Rt6USshjjETF3CwrNzrSFKDOed4ZcZqfvbHPFqlp/DJHYM4p01kFm+OFBW6hNVZ9ZMZ/+OefPf9Xp78dIXXcaSG2n/oCLdPXMAL01dxVY9mfPiLATRLq+11rJCrstDNLNPMZppZvpktN7Nxge09zWyOmS0yszwz6xf+uBKLhnZszM/Pb8PEuRv5dMlWr+NIDbOxuJQfvjGbz5d/z8OXdeLFH/ckOTH2x8tPJJglNo4A9zrnFphZXWC+mU0HngWecM79xcwuC3w/JHxRJZbdd1FH5q3byYMfLaVb8/q0auTtUl1SM+Ssrly8GeCdn/RjcPsMjxOFV5Vn6M65bc65BYHbe4F8oDnggHqBh9UHdOolJ5UYH8crI3oRF2eMnbSQQ0c0ni7h45zjd1+v5d/enstZ9ZKZNnag78scTnMM3cyygF7AXOAu4Dkz2wQ8Dzx4kueMDgzJ5BUWFp5ZWolpLRqk8Nx13Vm6ZTfP/OU7r+OITx0sK+ee9xfzX3/O5+IuZzH59gE15jfCoAvdzFKBj4C7nHN7gNuAu51zmcDdwIQTPc8596ZzLts5l52R4f//IeXULupyFv8+MIvf567n8+Xfex1HfGZLyQGu+81sPl60hfsu6sDrI3tTJwoWb46UoArdzBKpLPOJzrnJgc2jgKO3PwD0pqgE5YFLz6Zb8/rc/8FiNu8q9TqO+MS8dTu58pUc1heV8tbN2Ywd1t4388uDFcwsF6Py7DvfOTf+mLu2AucHbg8DVoc+nvhRrYR4Xr2xF87BHe8upKy8wutIEsOcc/xpzgZufGsO9Wsn8vGYgQzv3MTrWJ4I5gx9IHAzMCwwRXFRYFbLrcALZrYYeBoYHcac4jOtGtXh1z/sxsKNJTz/xUqv40iMOnSknIemLOWRj5cxuH06U8YMpF3jVK9jeabKwSXnXA5wst9b+oQ2jtQkl3dvxjdrivntrLX0b9OIoR0bex1JYsiOPQe5beIC5m/YxZihbbnnwo5Rvd5nJOiTouKpRy7vzNln1eXe9xfz/e6DXseRGLFoUwlXvJrDiq17eO3G3tx/cXQv3hwpKnTxVHJiPK+N7M3BsnLufHchRzSeLlX4IG8T1//2G5IS4ph8+wB+0L2p15GihgpdPNc2I5WnrunKvPU7eXmG3luXEysrr+Dxacu5/8Ml9M1qwLQxg+jUtF7VT6xBas4ETYlq1/RqweyCYl6ZWUC/1o0Y1D7d60gSRXbuP8yYiQv4Zm0xPx3UmgcvPZuEGFvvMxL0E5Go8cRVXWiXkcpd/7eIHXs1ni6Vlm/dzRWv5DB/4y5e+FEPHrm8s8r8JPRTkaiRkpTAayN7s+9QGXf/3yLKK5zXkcRjnyzeyg/fmE2Fc3z4i3P5YZ8WXkeKaip0iSodmtTliSu7kFtQzOszC7yOIx4pr3A885fvuOPdhXRtVp9pYwfRvUWa17GinsbQJepcn53J7DXFvPi3VfRr3dB3q8rIqe0+UMa49xby5cpCbjynJY9f0YWkBJ17BkM/JYk6ZsZT13SjVaM63PneQor3HfI6kkTQQ5OXkltQxFPXdOXpa7qpzE+DflISlVJrJfDqjb3YVVrGvR8spkLj6TVCWXkFs1YVcl2fTEae08rrODFHhS5Rq0uz+jzyg058ubKQt75e63UciYAlm0vYd+gIgzVttVpU6BLVburfisu6ncVzn69k/oZdXseRMMtZXYwZnKv3TapFhS5Rzcz49bXdaZqWzJ3vLmR3aZnXkSSMctcU0bVZfRrUSfI6SkxSoUvUq187kVdH9GbH3oPc9+FinNN4uh/tP3SEhRt3MbCdhluqS4UuMaFHZhq/uuRspq/Yzh9mr/c6joTBvPU7KSt3DFKhV5sKXWLGTwe1Zninxjz9WT5LNpd4HUdCLHd1EUkJcWRnNfA6SsxSoUvMMDOe/1EPMlJrMXbSQvYc1Hi6n+QUFNE3qwHJifFeR4lZKnSJKWkpSbw8ohdbSg7w4OSlGk/3icK9h/ju+70aPz9DKnSJOdlZDbn3og78eck2Js3b6HUcCYHZa4oANH5+hqosdDPLNLOZZpZvZsvNbNwx991hZisD258Nb1SRf/jFeW05r0MGT3yyghVb93gdR85QbkER9Wsn0qVZfa+jxLRgztCPAPc65zoB/YExZtbZzIYCVwHdnXNdgOfDmFPkn8TFGeOv70Fa7UTGTlrA/kNHvI4k1eScI2d1EQPaNtK6oGeoykJ3zm1zzi0I3N4L5APNgduAZ5xzhwL37QhnUJHjpafW4qUberG+eD//8fEyjafHqPXFpWzdfVDj5yFwWmPoZpYF9ALmAh2AwWY218xmmVnf0McTObVz2zbizgvaM2XhFj6cv9nrOFINOQUaPw+VoAvdzFKBj4C7nHN7qLyWegMqh2HuB943s3/5fcnMRptZnpnlFRYWhii2yD/cMaw957ZpxKNTl7N6+16v48hpyl1dRPO02rRqlOJ1lJgXVKGbWSKVZT7ROTc5sHkzMNlVmgdUAP/yX6xz7k3nXLZzLjsjIyNUuUX+Lj7OeOmGnqQkxTNm0gIOHC73OpIEqbzCMXtNEYPapXOC80E5TcHMcjFgApDvnBt/zF0fA8MCj+kAJAFF4QgpUpXG9ZJ58cc9WbV9H098stzrOBKkZVt2s+fgEQbqcrkhEcwZ+kDgZmCYmS0K/LkMeBtoY2bLgPeAUU7vSomHzuuQwe1D2vLet5uYumiL13EkCEfHzwe01eVyQ6HKNUWdcznAyX4Xuim0cUTOzD0XdmDeup08NHkp3ZrXp01GqteR5BRyC4ro1LQe6am1vI7iC/qkqPhKQnwcL4/oRWJCHGMnLeRgmcbTo9WBw+Xkrd/FoHY6Ow8VFbr4TrO02rzwox6s2LaHp/6c73UcOYm8DTs5XF6h+echpEIXX7qgUxNuHdyaP83ZwGdLt3kdR04gp6CIxHijX+uGXkfxDRW6+Nb9F59duTDGh0vYWFzqdRw5Tm5BEb1aNiAlqcq38iRIKnTxraSEOF4d0QsM7nh3AYePVHgdSQJ27j/M8q179OnQEFOhi69lNkzhueu6s3jzbv77r995HUcCvllTjHNo/DzEVOjie5d0bcqoc1sxIWcd01ds9zqOUDl+nlorgR4tdLncUFKhS43w4GWd6NKsHvd9sJgtJQe8jlPj5RYU0b9NIxLiVUGhpJ+m1AjJifG8emNvjpRXcMvb88gt0FUqvLKxuJSNO0s1/zwMVOhSY7ROr8MbN/Wh9HA5I383l5snzGXZlt1ex6pxco8uN6frt4ScCl1qlPM6ZDDj3vP5jx90YumW3Vz+Sg53vrtQ0xojKKegiCb1atFWl2UIORW61DjJifH8bHAbvvrlUMYMbcsXK77ngvFf8vi05RTtO+R1PF+rqHDMLihioC6XGxYqdKmx6iUncv/FZzPr/qFc1yeTP83ZwPnPzuR//raKfVqjNCxWbNvDrtIyzT8PExW61HhN6iXz62u78cXd53Fehwz+52+rGfLcTP74zXp9GCnEZgfGzzX/PDxU6CIBbTNSeeOmPky+fQBtM1J5dOpyLnxxFp8s3kpFhS71Hwo5BcW0b5xKk3rJXkfxJRW6yHF6t2zAe6P78/t/70vtxHjueHchV76WQ85qTXU8E4eOlDNvXbHOzsNIhS5yAmbG0I6N+fOdgxl/fQ927S/jpgma6ngmFmwo4WBZhcbPw0iFLnIK8XHGtb1bnHCq44bi/V7Hiym5BUXExxnntNHlcsNFhS4ShBNOdXxhFo9NXaapjkHKKSiiZ2YadZMTvY7iW1UWupllmtlMM8s3s+VmNu64++8zM2dm+j1KfO/YqY7X983kf+du1FTHIOw+UMaSzSUaPw+zYM7QjwD3Ouc6Af2BMWbWGSrLHrgQ2Bi+iCLRp0m9ZJ6+pnKq4/kd/zHV8Z3Zmup4InPWFlPh0Ph5mFVZ6M65bc65BYHbe4F8oHng7heBXwKa0yU1UtuMVF4f2YcpgamOj01bzvDxs5imqY7/JLegiJSkeHpmpnkdxddOawzdzLKAXsBcM7sS2OKcWxyGXCIxpdcxUx1TkuK5892FXPvGbA3DBOQUFHFO64YkJehtu3AK+qdrZqnAR8BdVA7DPAw8GsTzRptZnpnlFRYWVjuoSLQ7OtXxszsH8+wPu7N4cwkvTl/ldSzPbS05wNrC/Ro/j4CgCt3MEqks84nOuclAW6A1sNjM1gMtgAVmdtbxz3XOvemcy3bOZWdkZIQuuUiUioszru+byYh+Lfl97roaP2/96LXndbnc8AtmlosBE4B859x4AOfcUudcY+dclnMuC9gM9HbOfR/WtCIx5FcXn03DOkk8/PEyymvweHpuQRHpqUl0bFLX6yi+F8wZ+kDgZmCYmS0K/LkszLlEYl79lEQeubwzizeVMGnuBq/jeMI5R05BsS6XGyEJVT3AOZcDnPJIBM7SReQ4V/Zoxgd5m3n2ryu5uOtZNK5bsy5KtWr7Por2HdL4eYToLWeRMDIznry6K4fKK3jy03yv40RcToEulxtJKnSRMGudXofbh7Tlk8Vb+WpVzZrplVtQROv0OjRPq+11lBpBhS4SAbcNaUub9Do8MnUZB8vKvY4TEWXlFcxZW8zAdo28jlJjqNBFIqBWQjz/dXVXNhSX8vrMAq/jRMSiTSWUHi7Xx/0jSIUuEiED2qVzTa/mvDFrDQU79nkdJ+xyVhdhBue2UaFHigpdJIIe/kEnaifG8/CUpTjn77npuQVFdG9en/opulxupKjQRSIoPbUWD1zaibnrdjJ5wRav44TN3oNlLNyky+VGmgpdJMJu6JtJ75ZpPPVZPrv2H/Y6TljMW7eT8gqn8fMIU6GLRFhcnPHUNd3YfaCM//7rd17HCYucgiJqJcTRu1UDr6PUKCp0EQ90alqPnw1qzXvfbuLb9Tu9jhNyuQVF9GvdkOTEeK+j1CgqdBGPjBvenuZptXl4ylJfrXK0Y89BVm3fp/FzD6jQRTySkpTAE1d2YdX2fUzIWed1nJDJXRO4XK4KPeJU6CIeGt65CRd3acJLM1axaWep13FCImd1MWkpiXRuWs/rKDWOCl3EY49d0YV4Mx6duizm56Y755i9poiBbdOJi9PlciNNhS7isWZptbn7wg7MXFnIX5fF9hoxa4v2s233QY2fe0SFLhIFbhmQReem9Xj8k+UxvbD035ebU6F7QoUuEgUS4uN4+tpu7Nh7iBe+WOl1nGrLWV1EZsPatGyU4nWUGkmFLhIlemamcdM5rXhn9vqYXFj6SHkF36wt1tm5h1ToIlHkvos70ii1Fg9NWRpzC0sv3bKbvQePaPzcQyp0kShSv3blwtJLNu/mf+fE1sLSR8fPB7RVoXulykI3s0wzm2lm+Wa23MzGBbY/Z2bfmdkSM5tiZmnhjyvif1d0b8rg9uk89/lKtu856HWcoOUUFNGlWT0a1knyOkqNFcwZ+hHgXudcJ6A/MMbMOgPTga7Oue7AKuDB8MUUqTnMjCev6srh8gr+89MVXscJSunhIyzYUKLxc49VWejOuW3OuQWB23uBfKC5c+4L59zR+VVzgBbhiylSs2Sl12Hs0Hb8eck2vly5w+s4Vfp2/S4Ol1do/NxjpzWGbmZZQC9g7nF3/QT4y0meM9rM8swsr7CwZq14LnImfn5+G9pk1OHRqcujfmHp3IIikuLj6JvV0OsoNVrQhW5mqcBHwF3OuT3HbH+YymGZiSd6nnPuTedctnMuOyMj40zzitQYtRLieerqbmzcWcor/2+113FOKWd1EX1aNaB2ki6X66WgCt3MEqks84nOucnHbB8FXA6MdLF+EQqRKHRu20Zc27s5b361ltXb93od54SK9x1ixbY9DGzXyOsoNV4ws1wMmADkO+fGH7P9EuBXwJXOOX9cJk4kCj18WSdSkhJ4+OPovHjX7DXFABo/jwLBnKEPBG4GhpnZosCfy4BXgbrA9MC234QzqEhN1Si1Fg9eejbz1u3kw/mbvY7zL3ILiqibnEC35vW9jlLjJVT1AOdcDnCi62B+Fvo4InIi12dn8uH8zTz9WT4XdGoSNXO9nXN8vbqIc9s0IiFen1P0mo6ASAw4urD03oNHeOYv+V7H+buNO0vZUnKAQe013BINVOgiMaLjWXX52eA2vJ+3mXnromNh6ZzAx/01fh4dVOgiMWTcBe1p0aA2D0XJwtK5BUU0rZ9Mm/Q6XkcRVOgiMaV2Ujz/eVUXCnbs462v13qapbzCMXtNMQPbpVM5GU68pkIXiTHDzm7CpV3P4uUZq9lY7N2M4RVb91BSWqbrt0QRFbpIDHr0is4kxBmPeLiw9NHx8wH6QFHUUKGLxKCm9Wtz70UdmbWqkM+WerOwdG5BER2b1KVx3WRPXl/+lQpdJEb927mt6Nq8Hk98spwdeyN73fSDZeXMW79Ts1uijApdJEYlxMfx9DXdKCktY8hzX/Li9FXsO3Sk6ieGwPwNuzh8pIJB7TXcEk1U6CIxrHuLNP5612CGdMzgpRmrOf/Zmbwze33YpzTmFBSREGf0a61CjyYqdJEY1yYjlddH9uHjMQNp3ySVx6YtZ/j4WUxdtIWKMC00nVtQRK+WaaTWqvLqIRJBKnQRn+iZmca7t/bnD//el5SkeMa9t4grXs3h69WhXVimpPQwS7fs1vh5FFKhi/iImTGkY2M+u3MwL/64B7sPlHHzhHmM/N0clm7eHZLX+GZNMc6h+edRSIUu4kNxccY1vVow497zefTyzqzYuocrXs1h7KQFrC/af0Z/d+6aIuokxdMjMy1EaSVUVOgiPlYrIZ6fDGrNV78cyp3D2jEjfwfDx8/ikY+XUbj3ULX+ztyCYvq3aUSiLpcbdXRERGqAusmJ3HNRR2b9cgg39Mtk0ryNnP/cTMZPX8Xeg2VB/z2bd5Wyrmi/xs+jlApdpAZpXDeZ/7q6G9PvPo+hHRvz8ozVDHnuS/6Quy6oqY6zCyqXm9P1z6OTCl2kBmqTkcprI3szdcxAOjSpy+OfrOCC8V9WOdUxp6CIjLq1aN84NYJpJVgqdJEarEdmGpNuPYd3ftKPurUSGffeIi5/JYdZqwr/5aJfFRWO3IIiBulyuVGrykI3s0wzm2lm+Wa23MzGBbY3NLPpZrY68LVB+OOKSKiZGed3yODTOwbx0g092XuojFFvz2Pk7+ayeFPJ3x+3cvteivcf1vh5FAvmDP0IcK9zrhPQHxhjZp2BB4AZzrn2wIzA9yISo+LijKt6NmfGPUN4/IrOfPf9Xq56LZcxkxawrmg/uX9fbk4f949WVX5u1zm3DdgWuL3XzPKB5sBVwJDAw94BvgR+FZaUIhIxSQlx3DKwNT/s04K3vl7H775ey+fLvictJZE2GXVoWr+21xHlJE5rDN3MsoBewFygSaDsj5Z+45M8Z7SZ5ZlZXmFhaD+CLCLhUzc5kXsu7MCX9w9hRL+WlJSWcWGnJl7HklOwYFc7MbNUYBbwlHNuspmVOOfSjrl/l3PulOPo2dnZLi8v74wCi4g3SkoPk5KUQFKC5lJEmpnNd85lV/W4oI6MmSUCHwETnXOTA5u3m1nTwP1NgR3VDSsi0S8tJUllHuWCmeViwAQg3zk3/pi7pgGjArdHAVNDH09ERIIVzMWMBwI3A0vNbFFg20PAM8D7ZvZTYCPwo/BEFBGRYAQzyyUHONmnCC4IbRwREakuDYiJiPiECl1ExCdU6CIiPqFCFxHxCRW6iIhPqNBFRHxChS4i4hMqdBERn1Chi4j4hApdRMQnVOgiIj6hQhcR8QkVuoiIT6jQRUR8QoUuIuITKnQREZ9QoYuI+IQKXUTEJ1ToIiI+UWWhm9nbZrbDzJYds62nmc0xs0Vmlmdm/cIbU0REqhLMGfofgEuO2/Ys8IRzrifwaOB7ERHxUJWF7pz7Cth5/GagXuB2fWBriHOJiMhpSqjm8+4CPjez56n8T2FA6CKJiEh1VPdN0duAu51zmcDdwISTPdDMRgfG2fMKCwur+XIiIlKV6hb6KGBy4PYHwEnfFHXOvemcy3bOZWdkZFTz5UREpCrVLfStwPmB28OA1aGJIyIi1VXlGLqZvQsMAdLNbDPwGHAr8JKZJQAHgdHhDCkiIlWrstCdcyNOclefEGcREZEzoE+Kioj4hApdRMQnVOgiIj6hQhcR8QkVuoiIT6jQRUR8QoUuEssmToSsLIiLq/w6caLXicRD1b04l4h4beJEGD0aSksrv9+wofJ7gJEjvcslntEZukisevjhf5T5UaWlldulRlKhi8SqjRtPb7v4ngpdJFa1bHl628X3VOgiseqppyAl5Z+3paRUbpcaSYUuEqtGjoQ334RWrcCs8uubb+oN0RpMs1xEYtnIkSpw+TudoYuI+IQKXUTEJ1ToIiI+oUIXEfEJFbqIiE9UWehm9raZ7TCzZcdtv8PMVprZcjN7NnwRRUQkGMGcof8BuOTYDWY2FLgK6O6c6wI8H/poIuIJXcExZlU5D90595WZZR23+TbgGefcocBjdoQ+mohEnK7gGNOqO4beARhsZnPNbJaZ9Q1lKBHxiK7gGNOq+0nRBKAB0B/oC7xvZm2cc+74B5rZaGA0QEtdNEgkuukKjjGtumfom4HJrtI8oAJIP9EDnXNvOueynXPZGRkZ1c0pIpGgKzjGtOoW+sfAMAAz6wAkAUWhCiUiHtEVHGNaMNMW3wW+ATqa2WYz+ynwNtAmMJXxPWDUiYZbRCTG6AqOMc0i2cPZ2dmT2KunAAADx0lEQVQuLy8vYq8nIuIHZjbfOZdd1eP0SVEREZ9QoYuI+IQKXUTEJ1ToIiI+oUIXEfGJiM5yMbNCYMNxm9Px1xx2v+0P+G+f/LY/4L998tv+wJntUyvnXJWfzIxooZ8wgFleMNNxYoXf9gf8t09+2x/w3z75bX8gMvukIRcREZ9QoYuI+EQ0FPqbXgcIMb/tD/hvn/y2P+C/ffLb/kAE9snzMXQREQmNaDhDFxGREPCs0M3sksAi0wVm9oBXOULJzNab2VIzW2RmMXcVshMtCG5mDc1supmtDnxt4GXG03WSfXrczLYEjtMiM7vMy4ynw8wyzWymmeUHFmgfF9gek8fpFPsTy8co2czmmdniwD49Edge9mPkyZCLmcUDq4ALqVws41tghHNuRcTDhJCZrQeynXMxOX/WzM4D9gF/dM51DWx7FtjpnHsm8B9vA+fcr7zMeTpOsk+PA/ucczG3uLmZNQWaOucWmFldYD5wNXALMXicTrE/1xO7x8iAOs65fWaWCOQA44BrCfMx8uoMvR9Q4Jxb65w7TOU11a/yKIsEOOe+AnYet/kq4J3A7Xeo/McWM06yTzHLObfNObcgcHsvkA80J0aP0yn2J2YFVnLbF/g2MfDHEYFj5FWhNwc2HfP9ZmL8IAY44Aszmx9YS9UPmjjntkHlPz6gscd5QmWsmS0JDMnExPDE8cwsC+gFzMUHx+m4/YEYPkZmFm9mi4AdwHTnXESOkVeFbifY5ofpNgOdc72BS4ExgV/3Jfq8AbQFegLbgBe8jXP6zCwV+Ai4yzm3x+s8Z+oE+xPTx8g5V+6c6wm0APqZWddIvK5Xhb4ZyDzm+xbAVo+yhIxzbmvg6w5gCpVDS7Fue2Cc8+h45w6P85wx59z2wD+4CuAtYuw4BcZlPwImOucmBzbH7HE60f7E+jE6yjlXAnwJXEIEjpFXhf4t0N7MWptZEnADMM2jLCFhZnUCb+pgZnWAi4Blp35WTJgGjArcHgVM9TBLSBz9RxVwDTF0nAJvuE0A8p1z44+5KyaP08n2J8aPUYaZpQVu1waGA98RgWPk2QeLAtOQ/geIB952zsX0suJm1obKs3KABGBSrO1TYEHwIVReFW478BjwMfA+0BLYCPzIORczbzKeZJ+GUPmrvAPWAz8/OrYZ7cxsEPA1sBSoCGx+iMpx55g7TqfYnxHE7jHqTuWbnvFUnjS/75z7TzNrRJiPkT4pKiLiE/qkqIiIT6jQRUR8QoUuIuITKnQREZ9QoYuI+IQKXUTEJ1ToIiI+oUIXEfGJ/w9Cwd9/7PUuSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tr_time_sgd = [30,30,30,25,24,21,20,27,30]\n",
    "lr_sgd      = [1,5,10,15,17.5,20,22.5,25,30]\n",
    "tr_time_adam = [30,25,27,30,30,30,30]\n",
    "lr_adam = [0.00025,0.0005,.00075,.001,.0025,0.005,0.1]\n",
    "assert( len(lr_sgd) == len(tr_time_sgd))\n",
    "fig = plt.figure()\n",
    "sgd = fig.add_subplot(111)\n",
    "#adam = fig.add_subplot(252)\n",
    "sgd.plot(lr_sgd,tr_time_sgd)\n",
    "#adam.plot(lr_adam,tr_time_adam)\n",
    "sgd.plot(20,17,'ro')\n",
    "sgd.plot(22,16, 'ro')\n",
    "#fig.xscale('log', basex=2)\n",
    "fig.xticks([0.0001,0.001,0.01,0.1,1,10,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(20,125,190)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "sgd",
         "type": "scatter",
         "uid": "949b1738-92ae-4f42-9f1c-fbfaa8096cc5",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8,
          9,
          10,
          11,
          12
         ],
         "y": [
          20,
          20,
          20,
          20,
          20,
          16,
          6,
          20,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(245,145,30)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adam",
         "type": "scatter",
         "uid": "52f2cedb-1865-47a3-887a-5d6286f4bc75",
         "x": [
          0,
          1,
          3,
          5,
          8,
          12,
          14,
          16,
          17,
          18,
          19,
          21,
          22
         ],
         "y": [
          10,
          9,
          10,
          5,
          5,
          5,
          5,
          12,
          11,
          10,
          9,
          10,
          12
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(25,160,75)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adagrad",
         "type": "scatter",
         "uid": "e384c363-f31c-4eef-95b2-93d82cee8a6b",
         "x": [
          4,
          5,
          12,
          13,
          14,
          15,
          16,
          17,
          23
         ],
         "y": [
          20,
          20,
          16,
          13,
          4,
          6,
          11,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(200,30,135)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "momentum",
         "type": "scatter",
         "uid": "bd8b5dc2-ed3b-4df2-8cd4-1fc2af9a3ddc",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "y": [
          20,
          20,
          13,
          11,
          9,
          20,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(0,0,0)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "nag",
         "type": "scatter",
         "uid": "69ac01a3-75ff-4360-8feb-d8e17ffa09d9",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8
         ],
         "y": [
          20,
          15,
          12,
          12,
          20,
          20
         ]
        }
       ],
       "layout": {
        "height": 500,
        "title": {
         "text": "Time to train vs. learning rate, by optimizer"
        },
        "width": 800,
        "xaxis": {
         "ticktext": [
          5e-05,
          0.0001,
          0.00025,
          0.0005,
          0.00075,
          0.001,
          0.002,
          0.0025,
          0.005,
          0.0075,
          0.01,
          0.025,
          0.05,
          0.075,
          0.1,
          0.25,
          0.5,
          1,
          1.5,
          2,
          2.5,
          3,
          4,
          5,
          7.5,
          10,
          15,
          17.5,
          20,
          22.5,
          25,
          30,
          32.5,
          35,
          40,
          45,
          50
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "title": {
          "text": "Learning rate"
         }
        },
        "yaxis": {
         "autorange": true,
         "tickvals": [
          1,
          2,
          3,
          4,
          5,
          10,
          20
         ],
         "title": {
          "text": "Training time in number of Epochs"
         },
         "type": "linear"
        }
       }
      },
      "text/html": [
       "<div id=\"41d2c1e2-1474-490c-a8fc-561af73f6cbf\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"41d2c1e2-1474-490c-a8fc-561af73f6cbf\")) {\n",
       "    Plotly.newPlot(\"41d2c1e2-1474-490c-a8fc-561af73f6cbf\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12], \"y\": [20, 20, 20, 20, 20, 16, 6, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"b972de5f-d4dd-4036-8398-4eac18ac567e\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam\", \"x\": [0, 1, 3, 5, 8, 12, 14, 16, 17, 18, 19, 21, 22], \"y\": [10, 9, 10, 5, 5, 5, 5, 12, 11, 10, 9, 10, 12], \"type\": \"scatter\", \"uid\": \"b5b5490e-1c9c-46ec-9939-9f7d6d89e130\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 23], \"y\": [20, 20, 16, 13, 4, 6, 11, 20, 20], \"type\": \"scatter\", \"uid\": \"fa2d633b-a5ef-49d9-bbdf-ad44ebe0113d\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8], \"y\": [20, 20, 13, 11, 9, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"3eeab82d-76f9-49ec-9e18-dbd579788b8f\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag\", \"x\": [2, 3, 4, 5, 7, 8], \"y\": [20, 15, 12, 12, 20, 20], \"type\": \"scatter\", \"uid\": \"bca46b41-f10b-4b43-9204-a57d4b3b32a7\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"tickvals\": [1, 2, 3, 4, 5, 10, 20], \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"41d2c1e2-1474-490c-a8fc-561af73f6cbf\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"41d2c1e2-1474-490c-a8fc-561af73f6cbf\")) {\n",
       "    Plotly.newPlot(\"41d2c1e2-1474-490c-a8fc-561af73f6cbf\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12], \"y\": [20, 20, 20, 20, 20, 16, 6, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"b972de5f-d4dd-4036-8398-4eac18ac567e\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam\", \"x\": [0, 1, 3, 5, 8, 12, 14, 16, 17, 18, 19, 21, 22], \"y\": [10, 9, 10, 5, 5, 5, 5, 12, 11, 10, 9, 10, 12], \"type\": \"scatter\", \"uid\": \"b5b5490e-1c9c-46ec-9939-9f7d6d89e130\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 23], \"y\": [20, 20, 16, 13, 4, 6, 11, 20, 20], \"type\": \"scatter\", \"uid\": \"fa2d633b-a5ef-49d9-bbdf-ad44ebe0113d\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8], \"y\": [20, 20, 13, 11, 9, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"3eeab82d-76f9-49ec-9e18-dbd579788b8f\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag\", \"x\": [2, 3, 4, 5, 7, 8], \"y\": [20, 15, 12, 12, 20, 20], \"type\": \"scatter\", \"uid\": \"bca46b41-f10b-4b43-9204-a57d4b3b32a7\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"tickvals\": [1, 2, 3, 4, 5, 10, 20], \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "learning_rates = [5e-05, 0.0001,0.00025,0.0005,.00075,.001,0.002,.0025,0.005,0.0075,.01,.025,.05,0.075,0.1,0.25,0.5] + [1,1.5,2,2.5,3,4,5,7.5,10,15,17.5,20,22.5,25,30,32.5,35,40,45,50]\n",
    "lr_dict = {x: i for i,x in enumerate(learning_rates)}\n",
    "# SGD\n",
    "lr_sgd      = [0.00025,0.0005,0.00075,0.001,0.0025,0.005,0.0075,0.01,0.025,0.05]\n",
    "tr_time_sgd = [20,20,20,20,20,16,6,20,20,20]\n",
    "lr_sgd_dict = [lr_dict[x] for x in lr_sgd]\n",
    "assert(len(lr_sgd)== len(tr_time_sgd))\n",
    "# ADAM\n",
    "lr_adam = [0.00005,0.0001,0.0005,0.001,0.005,0.05,0.1,0.5,1,1.5,2,3,4]\n",
    "lr_adam_dict = [lr_dict[x] for x in lr_adam]\n",
    "tr_time_adam = [10,9,10,5,5,5,5,12,11,10,9,10,12]\n",
    "# adagrad\n",
    "lr_adagrad = [0.00075,0.001,0.05,0.075,0.1,0.25,0.5,1,5]\n",
    "lr_adagrad_dict = [lr_dict[x] for x in lr_adagrad]\n",
    "tr_time_adagrad = [20,20,16,13,4,6,11,20,20]\n",
    "# momentum\n",
    "lr_mom = [0.0001,0.00025,0.0005,0.00075,0.001,0.002,0.0025,0.005] \n",
    "lr_mom_dict = [lr_dict[x] for x in lr_mom]\n",
    "tr_time_mom = [20,20,13,11,9,20,20,20]\n",
    "# NAG\n",
    "lr_nag = [0.00025,0.0005,0.00075,0.001,0.0025,0.005]\n",
    "lr_nag_dict = [lr_dict[x] for x in lr_nag]\n",
    "tr_time_nag = [20,15,12,12,20,20]\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=lr_sgd_dict,\n",
    "    y=tr_time_sgd,\n",
    "    mode='lines+markers',\n",
    "    name=\"sgd\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(20,125,190)'\n",
    "    )\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x= lr_adam_dict,\n",
    "    y=tr_time_adam,\n",
    "    mode='lines+markers',\n",
    "    name=\"adam\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(245,145,30)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x= lr_adagrad_dict,\n",
    "    y=tr_time_adagrad,\n",
    "    mode='lines+markers',\n",
    "    name=\"adagrad\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(25,160,75)'\n",
    "    )\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x= lr_mom_dict,\n",
    "    y=tr_time_mom,\n",
    "    mode='lines+markers',\n",
    "    name=\"momentum\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(200,30,135)'\n",
    "    )\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x= lr_nag_dict,\n",
    "    y=tr_time_nag,\n",
    "    mode='lines+markers',\n",
    "    name=\"nag\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(0,0,0)'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4,trace5]\n",
    "layout = dict(title = 'Time to train vs. learning rate, by optimizer',\n",
    "                 width = 800,\n",
    "    height = 500,\n",
    "    xaxis = dict(\n",
    "        tickvals = list(lr_dict.values()),\n",
    "        ticktext = list(lr_dict.keys()),\n",
    "      title = \"Learning rate\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        title = 'Training time in number of Epochs',\n",
    "        tickvals= [1,2,3,4,5,10,20],\n",
    "        autorange=True\n",
    "    )\n",
    "              )\n",
    "\n",
    "print(layout[\"xaxis\"][\"tickvals\"])\n",
    "fig = dict(data=data, layout=layout)\n",
    "init_notebook_mode(connected=True)\n",
    "iplot(fig, filename='word-embedding-plot.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(20,125,190)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "sgd_shuffle",
         "type": "scatter",
         "uid": "b1c940e9-7854-4acc-a688-8b3df38e484d",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ],
         "y": [
          20,
          20,
          20,
          20,
          20,
          19,
          11,
          11,
          11,
          7,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(245,145,30)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adam_shuffle",
         "type": "scatter",
         "uid": "28d0b408-e362-4164-b8e4-bbb5c6490351",
         "x": [
          0,
          1,
          3,
          5,
          8,
          12,
          14,
          16,
          17,
          18,
          19,
          21,
          22,
          23,
          24,
          25,
          26
         ],
         "y": [
          5,
          4,
          6,
          6,
          4,
          5,
          6,
          5,
          6,
          6,
          5,
          7,
          7,
          5,
          7,
          5,
          6
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(25,160,75)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adagrad_shuffle",
         "type": "scatter",
         "uid": "57583e45-bf3a-4e35-b31b-239385a8190d",
         "x": [
          4,
          5,
          12,
          13,
          14,
          15,
          16,
          17,
          19,
          23
         ],
         "y": [
          20,
          20,
          13,
          7,
          2,
          5,
          6,
          10,
          14,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(200,30,135)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "momentum_suffle",
         "type": "scatter",
         "uid": "45f337b7-c4d3-4d0b-b619-4dedb22b507c",
         "x": [
          3,
          4,
          6,
          7,
          12
         ],
         "y": [
          17,
          12,
          8,
          9,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(0,0,0)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "nag_shuffle",
         "type": "scatter",
         "uid": "d2374171-add3-41f3-83c6-bccbb273a579",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8,
          12
         ],
         "y": [
          20,
          19,
          9,
          9,
          7,
          6,
          20
         ]
        }
       ],
       "layout": {
        "height": 500,
        "title": {
         "text": "Time to train vs. learning rate, by optimizer"
        },
        "width": 800,
        "xaxis": {
         "ticktext": [
          5e-05,
          0.0001,
          0.00025,
          0.0005,
          0.00075,
          0.001,
          0.002,
          0.0025,
          0.005,
          0.0075,
          0.01,
          0.025,
          0.05,
          0.075,
          0.1,
          0.25,
          0.5,
          1,
          1.5,
          2,
          2.5,
          3,
          4,
          5,
          7.5,
          10,
          15,
          17.5,
          20,
          22.5,
          25,
          30,
          32.5,
          35,
          40,
          45,
          50
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "title": {
          "text": "Learning rate"
         }
        },
        "yaxis": {
         "autorange": true,
         "title": {
          "text": "Training time in number of Epochs"
         },
         "type": "linear"
        }
       }
      },
      "text/html": [
       "<div id=\"9919b2e3-ec1e-42e2-b737-2fb80c902794\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9919b2e3-ec1e-42e2-b737-2fb80c902794\")) {\n",
       "    Plotly.newPlot(\"9919b2e3-ec1e-42e2-b737-2fb80c902794\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [20, 20, 20, 20, 20, 19, 11, 11, 11, 7, 20, 20], \"type\": \"scatter\", \"uid\": \"8c2c8a95-340b-4923-83bd-5ab56fb16035\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [0, 1, 3, 5, 8, 12, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26], \"y\": [5, 4, 6, 6, 4, 5, 6, 5, 6, 6, 5, 7, 7, 5, 7, 5, 6], \"type\": \"scatter\", \"uid\": \"e108653f-afb2-4771-a39f-f067dc0f5cc2\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 19, 23], \"y\": [20, 20, 13, 7, 2, 5, 6, 10, 14, 20], \"type\": \"scatter\", \"uid\": \"8fe4c8f9-4599-4c58-ace5-4ab7e61c866b\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum_suffle\", \"x\": [3, 4, 6, 7, 12], \"y\": [17, 12, 8, 9, 20], \"type\": \"scatter\", \"uid\": \"42305170-d6a4-459f-ac8e-2c62d1254302\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 12], \"y\": [20, 19, 9, 9, 7, 6, 20], \"type\": \"scatter\", \"uid\": \"75391c9c-f65a-498d-bb2b-e30b9d683f73\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9919b2e3-ec1e-42e2-b737-2fb80c902794\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9919b2e3-ec1e-42e2-b737-2fb80c902794\")) {\n",
       "    Plotly.newPlot(\"9919b2e3-ec1e-42e2-b737-2fb80c902794\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [20, 20, 20, 20, 20, 19, 11, 11, 11, 7, 20, 20], \"type\": \"scatter\", \"uid\": \"8c2c8a95-340b-4923-83bd-5ab56fb16035\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [0, 1, 3, 5, 8, 12, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26], \"y\": [5, 4, 6, 6, 4, 5, 6, 5, 6, 6, 5, 7, 7, 5, 7, 5, 6], \"type\": \"scatter\", \"uid\": \"e108653f-afb2-4771-a39f-f067dc0f5cc2\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 19, 23], \"y\": [20, 20, 13, 7, 2, 5, 6, 10, 14, 20], \"type\": \"scatter\", \"uid\": \"8fe4c8f9-4599-4c58-ace5-4ab7e61c866b\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum_suffle\", \"x\": [3, 4, 6, 7, 12], \"y\": [17, 12, 8, 9, 20], \"type\": \"scatter\", \"uid\": \"42305170-d6a4-459f-ac8e-2c62d1254302\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 12], \"y\": [20, 19, 9, 9, 7, 6, 20], \"type\": \"scatter\", \"uid\": \"75391c9c-f65a-498d-bb2b-e30b9d683f73\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#learning_rates = [0.00025,0.0005,.00075,.001,0.1,0.25,0.5] + [2.5,5,7.5,10,15,20,22.5]\n",
    "#lr_dict = {x: i for i,x in enumerate(learning_rates)}\n",
    "# sgd_shuffle\n",
    "lr_sgd_shuffle    = [0.00025,0.0005,0.00075,0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1]\n",
    "tr_time_sgd_shuffle = [20,20,20,20,20,19,11,11,11,7,20,20]\n",
    "lr_sgd_shuffle_dict = [lr_dict[x] for x in lr_sgd_shuffle]\n",
    "assert len(lr_sgd_shuffle) == len(tr_time_sgd_shuffle)\n",
    "\n",
    "# adam_shuffle\n",
    "lr_adam_shuffle = [0.00005,0.0001,0.0005,0.001,0.005,0.05,0.1,0.5,1,1.5,2,3,4,5,7.5,10,15]\n",
    "lr_adam_shuffle_dict = [lr_dict[x] for x in lr_adam_shuffle]\n",
    "tr_time_adam_shuffle = [5,4,6,6,4,5,6,5,6,6,5,7,7,5,7,5,6]\n",
    "assert len(lr_adam_shuffle) == len(tr_time_adam_shuffle), (str(len(lr_adam_shuffle))+ \" \"+ str(len(tr_time_adam_shuffle)))\n",
    "\n",
    "# adagrad_shuffle\n",
    "lr_adagrad_shuffle = [0.00075,0.001,0.05,0.075,0.1,0.25,0.5,1,2,5]\n",
    "lr_adagrad_shuffle_dict = [lr_dict[x] for x in lr_adagrad_shuffle]\n",
    "tr_time_adagrad_shuffle = [20,20,13,7,2,5,6,10,14,20]\n",
    "assert len(lr_adagrad_shuffle) == len(tr_time_adagrad_shuffle), (str(len(lr_adagrad_shuffle))+ \" \"+ str(len(tr_time_adagrad_shuffle)))\n",
    "\n",
    "\n",
    "# mom_suffleentum\n",
    "lr_mom_suffle = [0.0005,0.00075,0.002,0.0025,0.05] \n",
    "lr_mom_suffle_dict = [lr_dict[x] for x in lr_mom_suffle]\n",
    "tr_time_mom_suffle = [17,12,8,9,20]\n",
    "assert len(lr_mom_suffle) == len(tr_time_mom_suffle), (str(len(lr_mom_suffle))+ \" \"+ str(len(tr_time_mom_suffle)))\n",
    "\n",
    "\n",
    "# nag_shuffle\n",
    "lr_nag_shuffle = [0.00025,0.0005,0.00075,0.001,0.0025,0.005,0.05]\n",
    "lr_nag_shuffle_dict = [lr_dict[x] for x in lr_nag_shuffle]\n",
    "tr_time_nag_shuffle = [20,19,9,9,7,6,20]\n",
    "assert len(lr_nag_shuffle) == len(tr_time_nag_shuffle),(str(len(lr_nag_shuffle))+ \" \"+ str(len(tr_time_nag_shuffle)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trace1a = go.Scatter(\n",
    "    x=lr_sgd_shuffle_dict,\n",
    "    y=tr_time_sgd_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"sgd_shuffle\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(20,125,190)'\n",
    "    )\n",
    ")\n",
    "trace2a = go.Scatter(\n",
    "    x= lr_adam_shuffle_dict,\n",
    "    y=tr_time_adam_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"adam_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(245,145,30)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace3a = go.Scatter(\n",
    "    x= lr_adagrad_shuffle_dict,\n",
    "    y=tr_time_adagrad_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"adagrad_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(25,160,75)'\n",
    "    )\n",
    ")\n",
    "trace4a = go.Scatter(\n",
    "    x= lr_mom_suffle_dict,\n",
    "    y=tr_time_mom_suffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"momentum_suffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(200,30,135)'\n",
    "    )\n",
    ")\n",
    "trace5a = go.Scatter(\n",
    "    x= lr_nag_shuffle_dict,\n",
    "    y=tr_time_nag_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"nag_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(0,0,0)'\n",
    "    )\n",
    ")\n",
    "data_a = [trace1a,trace2a,trace3a,trace4a,trace5a] \n",
    "data = data + data_a\n",
    "layout = dict(title = 'Time to train vs. learning rate, by optimizer',\n",
    "                 width = 800,\n",
    "    height = 500,\n",
    "    xaxis = dict(\n",
    "        tickvals = list(lr_dict.values()),\n",
    "        ticktext = list(lr_dict.keys()),\n",
    "      title = \"Learning rate\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        title = 'Training time in number of Epochs',\n",
    "        autorange=True\n",
    "    )\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "fig1 = dict(data=data_a, layout=layout)\n",
    "init_notebook_mode(connected=True)\n",
    "#iplot(fig, filename='word-embedding-plot')\n",
    "iplot(fig1, filename='word-embedding-plot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(139,0,0)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "gensim",
         "type": "scatter",
         "uid": "118c9a87-a184-4922-b097-8e38ebabc418",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.47,
          0.59,
          0.63,
          0.637,
          0.649,
          0.64991,
          0.653,
          0.664,
          0.6655,
          0.6615,
          0.6629,
          0.663
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(20,125,190)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "SGD_shuffle",
         "type": "scatter",
         "uid": "f21255b7-bba0-4522-9bda-9c0595746f27",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.3979027932,
          0.5345590809,
          0.5957749206,
          0.6150409906,
          0.6343477449,
          0.6408864911,
          0.6569273384,
          0.6638613219,
          0.664644281,
          0.6684268463,
          0.6676506492
         ]
        },
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(20,125,190)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "gensim",
         "type": "scatter",
         "uid": "dc39fd52-8c1d-4c73-973e-08e0c955a63a",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.2656567075,
          0.409453666,
          0.4961671944,
          0.5470711378,
          0.58201276,
          0.608320445,
          0.6224159501,
          0.6369093127,
          0.6457120334,
          0.653658488,
          0.6585042924,
          0.6608316399
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(25,160,75)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adagrad_shuffle",
         "type": "scatter",
         "uid": "cf2f325a-9b2b-4eb7-8869-145deca72e5f",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.6646210026,
          0.6818005927,
          0.6784043309,
          0.6762544168
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(25,160,75)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adagrad_shuffle",
         "type": "scatter",
         "uid": "533032f7-838a-4dce-8890-a687fdd4cc11",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.6589294722,
          0.6741820076,
          0.6718779453,
          0.6689097502
         ]
        }
       ],
       "layout": {
        "height": 500,
        "title": {
         "text": "Convergence time comparison"
        },
        "width": 800,
        "xaxis": {
         "title": {
          "text": "Training time in number of Epochs"
         }
        },
        "yaxis": {
         "autorange": true,
         "title": {
          "text": "Word similarity"
         },
         "type": "linear"
        }
       }
      },
      "text/html": [
       "<div id=\"dd57ce11-9514-4542-ae37-0e3faff99d34\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"dd57ce11-9514-4542-ae37-0e3faff99d34\")) {\n",
       "    Plotly.newPlot(\"dd57ce11-9514-4542-ae37-0e3faff99d34\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(139,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.47, 0.59, 0.63, 0.637, 0.649, 0.64991, 0.653, 0.664, 0.6655, 0.6615, 0.6629, 0.663], \"type\": \"scatter\", \"uid\": \"0d2f0a27-04a8-4280-a3e6-2bc9f9c3b1b1\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"SGD_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.3979027932, 0.5345590809, 0.5957749206, 0.6150409906, 0.6343477449, 0.6408864911, 0.6569273384, 0.6638613219, 0.664644281, 0.6684268463, 0.6676506492], \"type\": \"scatter\", \"uid\": \"7195d7e9-8a44-4424-b986-9a5bed82616e\"}, {\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.2656567075, 0.409453666, 0.4961671944, 0.5470711378, 0.58201276, 0.608320445, 0.6224159501, 0.6369093127, 0.6457120334, 0.653658488, 0.6585042924, 0.6608316399], \"type\": \"scatter\", \"uid\": \"893e13fe-dda1-47a3-a20f-4703af55cf9a\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.6646210026, 0.6818005927, 0.6784043309, 0.6762544168], \"type\": \"scatter\", \"uid\": \"e4757a9b-70fa-4a6f-b7a0-56779fd0f949\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.6589294722, 0.6741820076, 0.6718779453, 0.6689097502], \"type\": \"scatter\", \"uid\": \"29d3e799-e931-47cf-a916-30e97e37be91\"}], {\"height\": 500, \"title\": {\"text\": \"Convergence time comparison\"}, \"width\": 800, \"xaxis\": {\"title\": {\"text\": \"Training time in number of Epochs\"}}, \"yaxis\": {\"autorange\": true, \"title\": {\"text\": \"Word similarity\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"dd57ce11-9514-4542-ae37-0e3faff99d34\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"dd57ce11-9514-4542-ae37-0e3faff99d34\")) {\n",
       "    Plotly.newPlot(\"dd57ce11-9514-4542-ae37-0e3faff99d34\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(139,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.47, 0.59, 0.63, 0.637, 0.649, 0.64991, 0.653, 0.664, 0.6655, 0.6615, 0.6629, 0.663], \"type\": \"scatter\", \"uid\": \"0d2f0a27-04a8-4280-a3e6-2bc9f9c3b1b1\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"SGD_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.3979027932, 0.5345590809, 0.5957749206, 0.6150409906, 0.6343477449, 0.6408864911, 0.6569273384, 0.6638613219, 0.664644281, 0.6684268463, 0.6676506492], \"type\": \"scatter\", \"uid\": \"7195d7e9-8a44-4424-b986-9a5bed82616e\"}, {\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.2656567075, 0.409453666, 0.4961671944, 0.5470711378, 0.58201276, 0.608320445, 0.6224159501, 0.6369093127, 0.6457120334, 0.653658488, 0.6585042924, 0.6608316399], \"type\": \"scatter\", \"uid\": \"893e13fe-dda1-47a3-a20f-4703af55cf9a\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.6646210026, 0.6818005927, 0.6784043309, 0.6762544168], \"type\": \"scatter\", \"uid\": \"e4757a9b-70fa-4a6f-b7a0-56779fd0f949\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.6589294722, 0.6741820076, 0.6718779453, 0.6689097502], \"type\": \"scatter\", \"uid\": \"29d3e799-e931-47cf-a916-30e97e37be91\"}], {\"height\": 500, \"title\": {\"text\": \"Convergence time comparison\"}, \"width\": 800, \"xaxis\": {\"title\": {\"text\": \"Training time in number of Epochs\"}}, \"yaxis\": {\"autorange\": true, \"title\": {\"text\": \"Word similarity\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "gensim_conv = [0,0.47 ,0.59,0.63,0.637,0.649, 0.64991,0.653,0.664,0.6655,0.6615,0.6629,0.663] \n",
    "adam_conv = [0,0.4796831063 ,0.5862162636,0.6298946727,0.648326564,0.6663352711,0.6692290085, 0.6709985856, 0.6687059277, 0.6676404941]\n",
    "adagrad_conv = [0,0.6589294722,0.6741820076,0.6718779453,0.6689097502]\n",
    "adagrad_shuffle_conv = [0,0.6646210026,0.6818005927,0.6784043309,0.6762544168]\n",
    "sgd_conv = [0,0.2656567075,0.409453666,0.4961671944,0.5470711378,0.58201276,0.608320445,0.6224159501,0.6369093127,0.6457120334,0.653658488,0.6585042924,0.6608316399]\n",
    "sgd_shuffle_conv = [0,0.3979027932,0.5345590809,0.5957749206,0.6150409906,0.6343477449,0.6408864911,0.6569273384,0.6638613219,0.664644281,0.6684268463,0.6676506492]\n",
    "\n",
    "epoches = list(range(20))\n",
    "trace1 = go.Scatter(\n",
    "    x=epoches,\n",
    "    y=gensim_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"gensim\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(139,0,0)'\n",
    "    )\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x= epoches,\n",
    "    y=sgd_shuffle_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"SGD_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(20,125,190)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=epoches,\n",
    "    y=sgd_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"gensim\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(20,125,190)'\n",
    "    )\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x= epoches,\n",
    "    y=adagrad_shuffle_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"adagrad_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(25,160,75)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x= epoches,\n",
    "    y=adagrad_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"adagrad_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(25,160,75)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4,trace5]\n",
    "layout = dict(title = 'Convergence time comparison',\n",
    "                 width = 800,\n",
    "    height = 500,\n",
    "    xaxis = dict(\n",
    "      title = 'Training time in number of Epochs'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        title = \"Word similarity\",\n",
    "             autorange=True\n",
    "    )\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "init_notebook_mode(connected=True)\n",
    "iplot(fig, filename='word-embedding-plot.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
