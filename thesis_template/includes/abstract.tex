% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\chapter*{Abstract}
The Skip-gram Model with negative sampling (SGNS) is an effective algorithm to create word embeddings. SGNS  uses Stochastic Gradient Descent (SGD) as it's learning algorithm. While a lot of effort has gone into increasing the throughput of words of SGNS, not much work has gone into optimizing the convergence time. Therefore our work focuses on the latter. We used two techniques to achieve a better convergence time, namely advanced optimizers and input shuffling. We compared our work to the state of the art implementation Gensim. We used the Text8 dataset to train our model, and measured the quality of our word embeddings with the wordsim353 dataset, which measures the quality of word embeddings by judging the similarity of different words. We trained our model with multiple advanced optimizers: momentum, nesterov accelerated gradient, Adagrad, Adadelta and Adam. We also applied input shuffling to all of the above optimizers. Adam combined with input shuffling outperformed every optimizer. Adam with shuffling also outperformed the current state of the art implementation Gensim. Adam converged to a similarity value of 0.66 (state of the art) in 2 epochs, while Gensim took 4 epochs. We confirmed these results with the enwik9 dataset, as Gensim took 4 epochs to converge and our model (Adam with input shuffling) only 3 epochs. Hence this work shows that advanced optimizers combined with input shuffling do decrease the convergence time of the SGNS. Optimizing our model in terms of throughput, to have the same throughput as Gensim for example, has the potential of further reducing the overall runtime.

