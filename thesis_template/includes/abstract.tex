% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\chapter*{Abstract}
The Skip-gram Model with negative sampling (SGNS) is an effective algorithm to create word embeddings. SGNS uses Stochastic Gradient Descent (SGD) as it's learning algorithm. While a lot of effort has gone into increasing the throughput of words of the SGNS, not much work has gone into optimizing the convergence time. Therefore, our work focuses on the latter. In this work we used two techniques to achieve a better convergence time, namely advanced optimizers and input shuffling. We used the Text8 dataset to train our model and measured the quality of our word embeddings with the wordsim353 dataset, which measures the quality of word embeddings by judging the similarity of different words. We trained our model with multiple advanced optimizers: momentum, Nesterov accelerated gradient, Adagrad, Adadelta, and Adam. We also applied input shuffling to the above optimizers. Adam combined with input shuffling outperformed every optimizer. Adam with shuffling also outperformed the current state of the art implementation Gensim. Adam converged to a similarity value of 0.66 (state of the art) in 2 epochs, while Gensim took 4 epochs. Hence, this work shows that advanced optimizers combined with input shuffling do decrease the convergence time of the SGNS. These results must be confirmed with other datasets, but optimizing our model in terms of throughput has the potential of further reducing the overall run time.

