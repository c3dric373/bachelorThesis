\chapter{Project Plan}\label{chap:introduction}

This section will cover the main project plan, we will discuss exactly what we wish to implement and how we are going to test our implementation. 
The whole process will be done the following way: 
\begin{itemize}
\item \texttt{Phase 1: Research} \\ In this phase we will become a broad overlook on the subject, research possible libraries that we can use and start planing the following phases.
\item \texttt{Phase 2: Implementation} \\ This phase will be our main work, as we will implement our own word2vec version.
\item \texttt{Phase 3: Testing} \\ Here we will first test if our optimization ideas were succesfull, and if they were we will test the accuracy of our Model. 
\item \texttt{Phase 4: Writing} \\ In this phase we will summarize Phase 1-3 in our thesis. 
\end{itemize}
More details on phase 2 and 3:\\
\texttt{Phase2}
First we will implement our own version of the skip gram model. We will implement the optimization techniques stated in \ref{chap:questions}, this means we will use input shuffling and  advanced optimizers.  There exists a python implementation of the original word2vec mode, that is called Gensim \cite{gensim}maybe it's possible to tweak it to fit our needs, if not we are going to implement our own version. \\
\texttt{Phase3}
First we compare our model against the original gensim word2vec implementation. For this we wil use the dataset \textit{text8}\footnote{ http://mattmahoney.net/dc/text8.zip }, that was created by Matt Mahoney \footnote{mattmahoney.net}. We will first compare the convergence time. If we see promising results we will then test the accuracy of our model. This is quite difficult as the quality of word embeddings are often task dependent, but Mikolov et al. \cite{mikolov2} presented a word analogy task\footnote{http://download.tensorflow.org/data/questions-words.txt} and  \cite{wSimilarity} presented a  word similarity evaluation\footnote{ http://www.leviants.com/ira.leviant/WS353\_ALL\_Langs\_SIM\_TXT\_Format.zip}. The analogy tasks evaluates semantic and syntactic analogies. The idea is to guess where a specific vector $x$ should be located, and if the closest (using the cosine distance) vector to the guess is $x$ then the model passed the test. For example let $x = vec("Berlin") - vec("Germany") + vec("France")$ then the closest vector to $x$ should be $vec("France")$. The above described test would be a semantic test, for a syntactic test one could use the following words: quick, quickly, slow, slowly. The similarity task dataset consists of tuples of words assigned with a similarity value. This value is assigned by human annotator. For example (king, cabbage) have a low similarity but (king, queen) have a high similarity. Therefore the cosine distance of (king,queen) should be small and the one of (king, cabbage) should be high. We will test our model on both of these tasks if we achieve a significant optimization. 

