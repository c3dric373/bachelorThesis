\chapter{Conclusion}\label{chap:conclusion}

This work provides an overview of the Skip Gram Model with negative Sampling (SGNS) and the numerous successful attemps of optimizing the throughput of the model. As this is the case, no effort went into optimizng the convergence time of the SGNS, therefore this work focused on this point. We decided to use advanced optimizers and input shuffling as optimizing techniques. After giving a short overview over Gradient Descent algorithms this work proposes a slighlty altered version of the SGNS. Where the idea is to compute the loss over the sum of a lot of training samples instead of computing it for each invidiually. This allowed us a faster runtime. We did this as it allowed us to compute more models and analyze the convergence time faster. We used the text8 dataset for most of our models. And used the word similarity  as a quality measure for the word embeddings (WE). We used the State of the art implementation Gensim to compare ourself. We did achieve a better convergence time then gensim with Adam as an optimizer and the use of input shuffling. Gensim convereged in 4 epochs to a word similairty of 0.66 and our model only took 2 epochs to achieve the same quality. We did confirm these findings with one test run on the enwik9 dataset. As gensim took 5 epochs to converge and our model only 3.  Those results still need to be confirmed with more datasets. Finally if this work would combined with an optimized throughput could improve the state of the art runtime of the SGNS. 