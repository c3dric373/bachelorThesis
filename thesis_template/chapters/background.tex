\chapter{Background}\label{chap:background}

\section{The Skip-Gram Model} 
The Skip Gram Model is a model used to embed words into vectors, by analyzing the context in which the words happens. It's achieving this by maximizing the following equation:\begin{equation} \label{basicSkip} \prod_{t=1}^T \prod_{-m<j<m}  p(w_{t+j}|w_t) \end{equation} Where T is the number of words in the corpus data, $w_t$ the $t-th$ word in the corpus data and $m$ is the context window. This means that the $m$ nearest words to $w$ are considered as context words.
This equation can be transformed quite easily into sums by using log probabilities: 
\begin{equation} \sum _{t=1}^T \sum_{-m<j<m} log( p(w_{t+j}|w_t) )\end{equation} 
    where the parameters are the same as in \ref{basicSkip}. The basic Skip-Gram Model uses a classical Softmax to calculate the conditional probability $p(w_{t+j}|w_t)$: 
   \begin{equation}
   p(w_{t+j}|w_t)=  \frac{exp( \tilde{v}_{w_{t+j}}^Tv_{w_t})}{\sum_{w=1}^v exp(\tilde{v}_w^Tv_{ w_t})}
   \end{equation}
  
  Here $\tilde{v}_{w_t}$ and $ v_{w_t}$ are the vector representations.  There lies a problem in this approach. As a matter of fact it is unsuitable to compute the softmax. For the computation of $\sum_{w=1}^v exp(v_w^T w_t)$ one has to go over the whole corpus data. As very big data sets are needed to train the model, this is not a solution. But different solutions were proposed by Mikolov et al. \cite{mikolov2}. The first one is to use a Hierarchical soft max introduced by Morin and Bengio \cite{hsoftmax}. In this model the probability distribution of the output nodes is saved in a binary tree which gives one a logarithmic computation time for each of these probabilities, and this makes it feasable to compute the softmax. Another possibility is the use of negative sampling which is used in the original word2vec implementation \cite{mikolov2}, which I shall discuss in the next section. 

\section{Negative Sampling}
An alternative to the Hierarchical Softmax is Noise Contrastive Estimation (NCE) which was introduced by Gutmann and Hyv{\"a}rinen \cite{nce-original},  and first applied to NLP by Mnih and Teh \cite{mnih}. The idea behind NCE is to distinguish targets words from noise. It does so by reducing the problem to a logistic regression task, and does it by maximizing the log probability. The skip-gram Model is only interested in good word representation, hence the probability of the word is not meaningful as long as the quality of the word representations remains high. Mikolov et al. \cite{mikolov2} simplified NCE and called it Negative Sampling. Let's dive into it.\\
The idea behind negative sampling is to only update the output nodes of certain words. This will obviously save an enormous amount of computation time. The idea is that given a pair $(c,w) \in D$, where $c$ is a word in the context window of $w$ we will set $p(c|w) =1$, here $p$ is the score for our logistic regression. Then select $K$ random words $k_i$ from the corpus data and set $p(k_i|w) = 0$, more one the random distribution later. We will denote the score that the $(c,w)$ wasn't drawn at random the following way: $p(y=1|c,w)$, and if $(k,w) $ is chosen at random this way: $p(y=0|k,w)$.  Now we will use logistic regression to update the weights of the $k$ selected context words and $c$. By doing so we will only have to update $k+1$ output nodes.

Let's look at how we construct our objective function for a given word $w$ and one of its context words $c$: 

\begin{align*}
p(c|w) &= p(y=1|c,w) + \prod_{k\in K} p(y=0|k,c) 
\\&= p(y=1|c,w) + \prod_{k\in K} 1- p(y=1|k,c) 
\\&= log((p(y=1|c,w)) + \sum_{k\in K} log(1- p(y=1|k,c)) 
\\&=  log(\frac{1}{1+e^{-v_c v_w }})  + \sum_{k\in K} log(1-\frac{1}{1+e^{-v_c v_k}}) 
\\&=  log(\frac{1}{1+e^{-v_c v_w }})  + \sum_{k\in K} log(\frac{1}{1+e^{v_c v_k }})
\\&= log(\sigma(v_c v_w ) + \sum_{k\in K} \sigma(log(-v_c v_k )) &&\text{$ where, \sigma = \frac{1}{1+e^{-x}}$}
\end{align*}

We see that to compute our objective function we will only have to compute the sum over $K$. Which in practice is very small (2-20). Too put things in perspective lets imagine our data set consists of 100000 words, we set $K=2$ and let's say that each output neuron has weight vector $v$ with $|v| = 300$. When updating our weights we would only update  $0.2*10^{-2}$ of the 300 million weights in the output layer. 

One question remains: how do we choose our random words? Mikolov et al. \cite{mikolov2} used the following unigram distribution:
 
 \begin{equation}
P(w)=\frac{f(w)^{\frac{3}{4}}}{\sum_{w_k\in W} f(w_k)^{\frac{3}{4}}}
\end{equation}
where $f(w)$ is the frequency of $w$ in the Vocabulary $W$. The value of $\frac{3}{4}$ is set empirically.

It's quite easily observable that this approach will outperform the classical softmax in computation time. Now the question arises if the accuracy is good enough enough but according to Mikolov et al. \cite{mikolov2} the negative sampling method "is an extremely simple training method that learns accurate representations". As a matter of fact Mikolov et al. \cite{mikolov2} reported a 6\% accuracy improvement in comparison to a Hierarchical Softmax model.
We now have enough background knowledge about word2vec and the skip gram model to look at how it can be optimized. In the next section we are going to cover what has already be done.

\section{Optimization of the Skip Gram Model}
Due to the popularity of the skip gram model, a lot of research went into optimizing it. This research can actually be divided into two categories, parallelization, and the optimization of the accuracy of the algorithm by allowing words to have multiple meanings.

\subsection{Parallelization}
In the original model the optimization is done with Stochastic Gradient Descent (SGD), which is a sequential algorithm. This process does not favor a parallelization. To deal with this specific problem Mikolov et al.\cite{mikolov2} used a Hogwild tree proposed by Recht et al.\cite{hogwild}. The approach is to allow multiple threads to access a shared memory, in this case the single model. Therefore overwriting errors are bound to happen. But according to Recht et al.\cite{hogwild} the overwriting errors won't lead to a significant accuracy loss if the data access isn't too frequent. But in the case of NLP the problem seems to be a bit more significant, and especially for word embedding, as many words share the same context words. There were several attempts at solving this issue, and we are going to cover a few of them in the following subsections. 

\subsubsection{Parallization in shared and Distributed Memory}
The first parallization solution which was proposed by Ji et al. \cite{intel}, is to try to reduce the cost of our vector multiplication. The main idea in this paper is to convert the level 1-BLAS vector to vector operations to a level-3 BLAS matrix multiplication operation. This is achieved, buy using the same negative samples for each context word of a given word $w$. Instead of using for each context word a vector to vector multiplication we can transform this, under the assumption that we will not loose accuracy by sharing the same negative samples,  into a matrix multiplication. The matrix multiplication can be represented the following way.
\[
\begin{bmatrix}
w \\
w_{n_1}  \\
\vdots \\
w_{n_k}\\
\end{bmatrix}
*
\begin{bmatrix}
w_{c_1}\\
\vdots\\
w_{c_{2m}}\\
\end{bmatrix}
\]

where $w$ is our given word, $w_{n_1}...w_{n_k}$ are the shared negative samples, with $k \in [5,20]$, and $w_{c_1}...w_{c_2m}$ are the words inside of the context window $m$ of $w$, with $m \in [10,20]$, also called a batch of input context words. After each batch the model updates the weights of the used vectors. 
This model achieves a 3.6 fold increase in throughput, by only losing 1\% of accuracy. 
\subsubsection{Parallelization by the use of caching}
This idea was proposed by Vuurens et al. \cite{efficient}. The architecture used here is the basic skip gram model with an hierarchical soft max.  The general idea is to cache the most frequent used nodes of the binary tree used to memorize the probability distribution, and update them on the shared single model after a certain amounts of seen words (the paper used the number 10). The paper produced interesting results as they managed to increase execution time by increasing the number of cores used for the calculation. This is very powerful because in the original implementation the exectution time regressed after 8 cores, this seems to indicate that too much overwriting was happening, as the number of concurrent threads surpasses a certain threshold. This can be seen in \ref{fig:efficient}, where c31 is the model proposed by Vuurens et al.\cite{efficient}. The model did not suffer any accuracy loss in comparison to the original Word2vec model. 
\begin{figure}[ht]
    \centering
			\includegraphics[scale=0.3]{images/cachingEfficiency.png} 
    \caption{Comparasion of the execution time in relation with the number of used cores \cite{efficient}}
    \label{fig:efficient}
\end{figure}
%%%%%%%%%%%
\iffalse
\subsection{Restating the problem as Matrix factorization}
It was also shown in \cite{goldenberger} that the optimization problem solved by the sgd can alos be restated as a matrix factorization. In fact the partial derivative Matrix W*C where w is the word embedding matrix and C the context matrix, is nothing else then PPMI - log k. which states the optimizing problem as a pure matrix factorization problem. Also this first step showed similar results to the original word2vec. Then this was picked up by, to do it with riemman  optimization and outperformed the original word2vec in accuracy. 
\fi
%%%%%%%%%%%

\subsection{Context sensitive word embedding}
A word does not always have the same meaning according to it's context. This is a problem that is not addressed by word2vec and the general skipGram model. Some new models, that have taken this issue into consideration, were proposed. A lot of work has be done in this direction, Liu et al.\cite{topicalWE},  Bartunov et al.\cite{breaking} for example, but the the one reporting the best results is Liu et al. \cite{contextWithTensor}. The main idea is to change the way we compute the and variables we use in our conditional probability. The idea is to look if a word given a certain context word matches to a topic. Bank would match too finance given the context word money. Bank would also match too nature if river was the given context word. But bank would not match too nature with the context word money. Now one could ask himself how to achieve such a context sensitive word embedding, first we have to introduce new variables, therefore let's look at the objective function used: 
First let's take a look at the objective function:
\begin{equation}
J(\Omega) = \sum_{(w,t,c)\in D} \sum_{(w,\tilde{t},\tilde{c} \in{\tilde{D}})} max(0,1- g(w,t,c) + g(w,\tilde{t},\tilde{c})) \lambda||\Omega||_{2}^2
\end{equation}

This approach uses the same negative sample technique as described in the previous sections, $D$ is the corpus data and $\tilde{D}$ is the set of negative samples and $\lambda$is the hyperparameter used for the standard $L_2$ standarization. What is interesting here is the function $g(w,c,t)$, where $w$ is a word, $c$ the context word, and $t$ the context in which the word appears, $g$ is defined as follows: 
\begin{equation}
g(w,c,t) = u^T \sigma(w^TM^{[1:k]}t+V_c^T(w \oplus t) + b_c)
\end{equation}
where, $u, V_c, b_c$ are standard parameters for a neural network, $\oplus$ is the vector concatenation, while the most important parameter is $M^{[1:k]}$, which a tensor layer, the tensor layer is used because of its ability to model multiple interactions in the data, as this will be useful for multiple contexts. They used SGD for the optimization of this objective function.  They achieved really interesting results as shown in \ref{fig:multipleContext}.
\begin{figure}[ht]
    \centering
			\includegraphics[scale=0.7]{images/multipleContext.png} 
    \caption{"Nearest  neighbor  words  by  our  model  and  Skip-
Gram. The first line in each block is the results of Skip-Gram;
and the rest lines are the results of our model" \cite{contextWithTensor}}
    \label{fig:multipleContext}
\end{figure}

\section{Optimizers in Machine learning}
This section will give a quick overview about a few optimizers existing in machine learning. First of all let's define a few notation: 
$\alpha$ will be the learning rate, $\theta$ all the parameters of our model, $J(\theta)$ our objective function. The goal of an optimizer is to find values for $\theta$ such that $J(\theta)$ can either be minimized or maximized depending on the task. Stochastic gradient descent is the most common optimization technique used in machine learning.  We will address it in it's original form and the existing extensions. 
\subsection{Stochastic Gradient Descent}
In this work we will address stochastic gradient as mini-batach gradient descent. At each optimiation time step $t$, Stochastic gradient descent will compute the gradient of all our parameters. Denoted as $  \nabla J(\theta_{t-1})$. We will then substract this term, multiplied by our learning rate, from our weights $\theta$. The gradients will give the direction of the optimization step, whereas the learning rate will give the amplitude of that step.
\begin{equation}
\theta_t = \theta_{t-1} - \alpha \nabla J(\theta_{t-1})
\end{equation}

SGD, through it's simplicity, is very limited, therefore somme issues appear:

\begin{itemize}
\item The learning rate needs to be tuned, it's is often necessacary to try out a set of learning rate to find the optimal one for the given network. 
\item Learning rate schedules, that are very often needed, as one gets closer to the optimal a smaller learning rate is often needed, must be set before the training starts and hence cannot adapt to the learning set
\item SGD does not take the parameters into account, every parameter will be following the same update rule
\end{itemize}

Thererfore there exists numerous advanced optimizers that take those issues into consideration. Let's look into theim. 

\subsection{Momentum}
Momentum is a technique used to adress one of SGD weak points. As a matter of fact because SGD can have trouble computing the optimum of objective function that are only steep in one directions. The problem here is that SGD often osciliates in the direction that is not very steep, and only takes small steps in the steep direction. This issue is addressed by SGD with momentum. 

It does so by adding a percentage of the last gradient vector  to the update vector. By doing so the gradient that go in the same direction will get bigger (building momentum) and gradients that go in different directions will anhill themselves. 
 At the update $t$ we will comppute our update vector $v_t$ the following way:
\begin{equation}
v_t = \gamma v_{t-1} + \alpha \nabla J (\theta)
\end{equation}
The value $0.9$ for $\gamma$ has shown great results, but this, same as a learning rate, another hyper parameter that need to be tuned according to the specific task. 
And then we update our weights as usual: $\theta = \theta - v_t$ 
\subsection{Nesterov}
Momentum can be a powerful tool, but sometimes be it's own enemy. With momentum the learning algorithm often overshoots, and blows by the  optima. Hence it will never converge. This problem was addressed by Nesterov. The idea behind his algorithm is to incorporate the momentum in the computation of our gradients. We will subtract the momentum vector, or just a fraction,  from our parameters before computing the gradients. Therefore we will compute the gradients of the position where we would be with momentum, which will allow us to make a step in a better direction. The computation of the update vector will look the following way:
\begin{equation}
v_t = \gamma v_{t-1} + \alpha \nabla J (\theta -  \gamma v_{t-1})
\end{equation}

SGD wiht momentum and Nesterov accelerated gradient (NAG) has shown tremendous results in RNN's. But some of the earlier mentioned problems still remains.  NAG, still treats every parameter the same way. Therefore we need a more complex optimization algorithm, that takes the frequency of a feature into account. Adagrad does just that. 

\subsection{Adagrad}
Adagrad first introduced by..., is an optimzer that tries to apply different learning rates to different parameters according to their frequency. the idea is to give very frequent features a small learning rate, and very sparse features a high learning rate. 
For this let's define: 
$g_{t,i} = \nabla J(\theta_{t,i})$ which is the partial derivate of the loss function with respect to the parameter $\theta_i$ at time step $t$
The classic update would look like this $\theta_{t+1,i} = \theta_{t_i}-\alpha g_{t,i}$ Now we want  to change the learning rate decreasing it for very frequent words. For this we will look at the following diagonal matrix $G_{t_{i,i}}$ of the sum of the squares of the graditents. This means that the matrix for very frequent features will be very big. We know divide the learning rate through the square root of our matrix and multiply (vector matrix multiplication) it with our gradient vector. 
The equation will look the following way: 
\begin{equation}
\theta_{t+1,i} = \theta_{t_i}- \frac{\alpha}{\sqrt{G_{t_{i,i}} + \epsilon}} g_{t,i}, 
\end{equation}
where $\epsilon$ is a smoothing factor usually set to $1e-8$. There lies one weakniss in this approach: the sum of the squares of the previous gradients grows constantly. This means after a certain number of epochs the learning rate will be insufficent. 
This issue was addressed by ... in his implementation of Addadelta. 
\subsection{Adadelta}
Addadelta addresses two issues of Addagrad, first the problem of the consistent growing of the gradients. And then the unit problem. 
\subsection{Adam}


