\chapter{Background}\label{chap:background}
This section will give an overview of the SGM and related work that was done to optimize it. It will also give an understanding of the used optimizers in our experiments. 
\section{The Skip-Gram Model}
The SGM is a very simple model used to learn WE. The idea is to train the model, a neural network, on a fake task and then use the weights as embeddings. The network consists of one input and one output layer. Each of these layers will act as a lookup table for our words, i.e the matrices representing the layers will store word embeddings. The input layer will be a $T \times d$ matrix, whereas the output layer will have dimension $T \times d$, where $T$ is the number of words in our vocabulary and $d$ will be the dimension of our WE. Each row of the input Matrix will store the WE of one word. The same holds for each column of the output layer. Know the question arises, on which task the network is trained? Well, the goal of the SGM is to predict for a given word $w$ the probability of an arbitrary word $c$ to appear next to $w$ in a given window $m$. 
The training samples will be all the pairs of words with their context words. 
$m$ is a hyperparameter of the model. An important thing to notice is that instead of always using the same $m$, each word will randomly have a number $r$ that is in the range $(1,m)$ as it's window size.
The task of the skip gram model is done by maximizing the following equation:
\begin{equation} \label{basicSkip} \prod_{t=1}^T \prod_{-m<j<m}  p(w_{t+j}|w_t) \end{equation} Where T is the number of words in the corpus data, $w_t$ the $t^{th}$ word in the corpus data and $m$ is the context window. This means that the $m$ nearest words to $w$ are considered as context words.
Equation \ref{basicSkip} can be transformed into sums by using log probabilities: 
\begin{equation} \sum _{t=1}^T \sum_{-m<j<m} log( p(w_{t+j}|w_t) )\end{equation} 
    where the parameters are the same as in Equation \ref{basicSkip}.

The basic Skip-Gram Model uses a classical Softmax to calculate the conditional probability $p(w_{t+j}|w_t)$: 
   \begin{equation}
   p(w_{t+j}|w_t)=  \frac{exp( \tilde{v}_{w_{t+j}}^Tv_{w_t})}{\sum_{w=1}^v exp(\tilde{v}_w^Tv_{ w_t})}
   \end{equation}\label{eq:softmax}
  
  Here $\tilde{v}_{w_t}$ and $ v_{w_t}$ are the vector representations. The model stores vector representations of each word. One for the input words and another for the context word. Here $\tilde{v}_{w_t}$ is the output vector and  $ v_{w_t}$ the projection layer. There lies a problem in the approach with a classical softmax. As a matter of fact, it is unsuitable to compute the softmax. For the computation of $\sum_{w=1}^v exp(\tilde{v_w}^T w_t)$, the denominator in Equation \ref{eq:softmax} one has to go over the whole corpus data. As very big data sets are needed to train the model, this is not a solution. Consequently, different solutions were proposed by Mikolov et al. \cite{mikolov2}. The first one is to use a Hierarchical softmax introduced by Morin and Bengio \cite{hsoftmax}. In this model, the probability distribution of the output nodes is saved in a binary tree which gives one a logarithmic computation time for each of these probabilities and makes it suitable to compute the softmax. Another possibility is the use of negative sampling which we shall discuss in the next section. 

\section{Negative Sampling}
An alternative to the Hierarchical Softmax is Noise Contrastive Estimation (NCE) which was introduced by Gutmann and Hyv{\"a}rinen \cite{nce-original},  and first applied to NLP by Mnih and Teh \cite{mnih}. The idea behind NCE is to distinguish targets words from noise. It does so by reducing the problem to a logistic regression task and does it by maximizing the log probability. The skip-gram Model is only interested in good word representation, hence the probability of the word is not meaningful as long as the quality of the word representations remains high. Mikolov et al. \cite{mikolov2} simplified NCE and called it Negative Sampling. Let's dive into it.\\
The idea behind negative sampling is to only update the output nodes of certain words. This will obviously save an enormous amount of computation time. The idea is that given a pair $(c,w) \in D$, where $c$ is a word in the context window of $w$ and select $K$ random words $k_i$ from the corpus data, more on the random distribution later. We will assume those words do not appear in the context of $w$. We will denote the score that the $(c,w)$ wasn't drawn at random the following way: $p(y=1|c,w)$, and if $(k,w) $ is chosen at random this way: $p(y=0|k,w)$.  Now we will use logistic regression to update the weights of the $k$ selected context words and $c$. By doing so we will only have to update $k+1$ nodes.

Let's look at how we construct our objective function for a given word $w$ and one of its context words $c$: 

\begin{align*}
p(c|w) &= p(y=1|c,w) + \prod_{k\in K} p(y=0|k,c) 
\\&= p(y=1|c,w) + \prod_{k\in K} 1- p(y=1|k,c) 
\\&= log((p(y=1|c,w)) + \sum_{k\in K} log(1- p(y=1|k,c)) 
\\&=  log(\frac{1}{1+e^{-v_c \tilde{v_w }}})  + \sum_{k\in K} log(1-\frac{1}{1+e^{-v_c \tilde{v_k}}}) 
\\&=  log(\frac{1}{1+e^{-v_c \tilde{v_w } }})  + \sum_{k\in K} log(\frac{1}{1+e^{v_c \tilde{v_k} }})
\\&= log(\sigma(v_c \tilde{v_w } ) + \sum_{k\in K} \sigma(log(-v_c \tilde{v_k} )) &&\text{where, $\sigma(x) = \frac{1}{1+e^{-x}}$}
\end{align*}\label{eq:obj_neg_samples}

Where $v_c$ and $\tilde{v_w }$, can be interpreted as before in Equation \ref{eq:softmax} The goal is know to maximize this objective function. Another way, apart from logistic regression, too look at this function, and assume why it's working so well, is to assume that two vectors are similar if their dot product is high. And therefore we will maximize the dot product of similar words $(w,c)$ and minimize it for dissimilar words $(w,k_i)$.
We see that to compute our objective function we will only have to compute the sum over $K$. Which in practice is very small (2-20). Too put things in perspective lets imagine our data set consists of 100000 words, we set $K=2$ and let's say that each output neuron has weight vector $v$ with $|v| = 300$. When updating our weights we would only update  $0.2*10^{-2}$ of the 300 million weights in the output layer. 

One question remains: how do we choose our random words? Mikolov et al. \cite{mikolov2} used the following unigram distribution:
 
 \begin{equation} \label{eq:unigram}
P(w)=\frac{f(w)^{\frac{3}{4}}}{\sum_{t=0}^{T} f(w_t)^{\frac{3}{4}}}
\end{equation}
where $f(w)$ is the frequency of $w_t$. The value of $\frac{3}{4}$ is set empirically. By raising the unigram distribution to power of $\frac{3}{4}$ it makes it less likelier for a word to be drawn if it appears often in the dataset in comparison to the basic unigram distribution.  See figure \ref{fig:frequency_ex} for an example.
\begin{figure}[ht]
    \centering
            \includegraphics[scale=0.45]{images/frequency_ex} 
    \caption{Probabililty of a word, of the text8 dataset (sampled),  to be chosen at random according to its frequency and the power to which the unigram distribution is raised}
    \label{fig:frequency_ex}
\end{figure}
It's quite easily observable that this approach will outperform the classical softmax in computation time. Now the question arises if the accuracy is good enough but according to Mikolov et al. \cite{mikolov2} the negative sampling method "is an extremely simple training method that learns accurate representations". As a matter of fact, Mikolov et al. \cite{mikolov2} reported a 6\% accuracy improvement in comparison to a Hierarchical Softmax model.
We now have enough background knowledge about the SGM to look at how it can be optimized. In the next section, we are going to cover what has already be done.

\section{Optimization of the Skip Gram Model}
Due to the popularity of the skip gram model, a lot of research went into optimizing it. This research can actually be divided into two categories, optimization of the throughput and the optimization of the accuracy of the algorithm by allowing words to have multiple meanings. For our work, the optimization of the throughput is of big interest while the semantic optimization is aimed at given the reader a more holistic comprehension of the possible research directions. 
This section will first give an overview of the optimization of the throughput and then present one paper that focused on context-sensitive word embeddings. 

\subsection{Optimization of the throughput}
In the original model, the optimization is done with Stochastic Gradient Descent (SGD), which is a sequential algorithm. This process does not favor a parallelization. To deal with this specific problem Mikolov et al.\cite{mikolov2} used a Hogwild tree proposed by Recht et al.\cite{hogwild}. The approach is to allow multiple threads to access a shared memory, in this case, the single model. Therefore overwriting errors are bound to happen. But according to Recht et al.\cite{hogwild} the overwriting errors won't lead to a significant accuracy loss if the data access isn't too frequent. But in the case of NLP, the problem seems to be a bit more significant, and especially for word embedding, as many words share the same context words. There were several attempts at solving this issue, and we are going to cover a few of them in the following subsections. 

\subsubsection{Parallelization by the use of caching}
This idea was proposed by Vuurens et al. \cite{efficient}. The architecture used here is the basic skip gram model with a hierarchical softmax.  The general idea is to cache the most frequently used nodes of the binary tree used to memorize the probability distribution and update them on the shared single model after a certain amount of seen words (the paper used the number 10). The paper produced interesting results as they managed to increase execution time by increasing the number of cores used for the calculation. This is very powerful because in the original implementation the execution time regressed after 8 cores, this seems to indicate that too much overwriting was happening, as the number of concurrent threads surpasses a certain threshold. This can be seen in Figure \ref{fig:efficient}, where c31 is the model proposed by Vuurens et al.\cite{efficient}. The model did not suffer any accuracy loss in comparison to the original SGM model. 
\begin{figure}[ht]
    \centering
            \includegraphics[scale=0.3]{images/cachingEfficiency.png} 
    \caption{Comparasion of the execution time in relation to the number of used cores \cite{efficient}}
    \label{fig:efficient}
\end{figure}
This work proposes a very good way to parallelize the SGM, as in particular, it allows to use more cores during the computation. While this is very interesting, this model focuses on the hierarchical softmax approach. As we implemented the approach using negative sampling this is of less use. Therefore we will look at what has been done with negative sampling in the next section.

\subsubsection{Parallization in shared and Distributed Memory}
The first parallelized solution which was proposed by Ji et al. \cite{intel}, is to try to reduce the cost of our vector multiplication. The main idea in this paper is to convert the level 1-BLAS vector to vector operations to a level-3 BLAS matrix multiplication operation. This is achieved, by using the same negative samples for each context word of a given word $w$. Instead of using for each context word a vector to vector multiplication we can transform this, under the assumption that we will not lose accuracy by sharing the same negative samples,  into a matrix multiplication. The matrix multiplication can be represented in the following way.
\[
\begin{bmatrix}
w \\
w_{n_1}  \\
\vdots \\
w_{n_k}\\
\end{bmatrix}
*
\begin{bmatrix}
w_{c_1}\\
\vdots\\
w_{c_{2m}}\\
\end{bmatrix}
\]

where $w$ is our given word, $w_{n_1}...w_{n_k}$ are the shared negative samples, with $k \in [5,20]$, and $w_{c_1}...w_{c_2m}$ are the words inside of the context window $m$ of $w$, with $m \in [10,20]$, also called a batch of input context words. After each batch the model updates the weights of the used vectors. 
This model achieves a 3.6 fold increase in throughput, by only losing 1\% of accuracy.  Those results are interesting but cannot be considered earth-shattering. Another aspect that is not as useful to us is that the experiments were done on Cpu, as modern GPU's are often used in many machine learning libraries, as with Cuda for example, there still need work to be done to optimize it with GPUs. This was attempted by Seulki and Youngmin 2016 \cite{gpu}, which will be described in the next section.


\subsubsection{Accelleration of word2vec by Using GPU's}
This work \cite{gpu} focused on getting a better throughput on the SGM when using Gpu's. As the SGM is a sequential algorithm, the parallelization is not that obvious. Especially if one wants to parallelize the training of individual training samples. As the algorithm goes sequentially over a sentence,  the samples next to each other, in order of execution, will almost every time have the same input word.Therefore it's very hard to parallelize at this level.  Therefore the idea of Seulki and Youngmin \cite{gpu} was to parallelize the update of each dimension of the word embeding, as those are completly independent of each other. They achieved this by mapping each dimension to a CUDA thread, while mapping each sentence to a CUDA block. As each CUDA block can run independently, the training of the sentences is parallelized, and the fact that sentences have different length is of no problem. If the execution time of the GPU kernel is greater than the reading of the sentences, it could a smart choice to use multiple GPU's. The authors of the paper do note that if multiple GPU's are used, there is a need for synchronizing the model, which will hinder run time performance. They achieved their best results with 2 concurrent GPU'S. The achieved results where very good results as they achieved a 20x speedup compared to a single threaded CPU execution, which is a 3x increase in comparison to the original C code, with no loss in accuracy.  The problem with this and all the above optimization is that the code is not easily available and of use for us. Therefore we need an optimized implementation of the SGM, which Gensim \cite{gensim} provides. 
%%%%%%%%%%%
\subsubsection{Gensim}
Gensim \cite{gensim} is a pure Python library that holds a state of the art implementation of the SGM. Gensim is written in Cython, which first allowed Gensim to hold the same runtime as the original C code. It then didn't stop there. It made us of BLAS's and precomputed sigmoid tables, while also trying to parallize the training of different sentences. This finally yielded in a 4x speed up in runtime.  Gensim is an important tool as it allows us, as a python library, to compare our data rather easily. It was also used in related work \cite{intel} and is therefore of value. This will conclude our overview of the optimizations of the throughput of the SGM. In the next section, we will give a quick outlook of what has been done in the field of context sensitive word embeddigs.



\subsection{Context sensitive word embedding}
A word does not always have the same meaning according to its context. This is a problem that is not addressed by the SGM. Some new models, that have taken this issue into consideration, were proposed. A lot of work has been done in this direction, Liu et al.\cite{topicalWE},  Bartunov et al.\cite{breaking} for example, but the one reporting the best results is Liu et al. \cite{contextWithTensor}. The main idea is to change the way we compute the objective function and variables we use in our conditional probability. The idea is to look if a word given a certain context word matches to a topic. Bank would match to finance given the context word money. Bank would also match to nature if river was the given context word. But Bank would not match to nature with the context word money. Now one could ask himself how to achieve such a context sensitive word embedding? First, we have to introduce new variables, therefore let's look at the objective function used: 
First, let's take a look at the objective function:
\begin{equation}
J(\Omega) = \sum_{(w,t,c)\in D} \sum_{(w,\tilde{t},\tilde{c} \in{\tilde{D}})} max(0,1- g(w,t,c) + g(w,\tilde{t},\tilde{c})) \lambda||\Omega||_{2}^2
\end{equation}

This approach uses the same negative sample technique as described in the previous sections, $D$ is the corpus data and $\tilde{D}$ is the set of negative samples and $\lambda$is the hyperparameter used for the standard $L_2$ standarization. What is interesting here is the function $g(w,c,t)$, where $w$ is a word, $c$ the context word, and $t$ the context in which the word appears, $g$ is defined as follows: 
\begin{equation}
g(w,c,t) = u^T \sigma(w^TM^{[1:k]}t+V_c^T(w \oplus t) + b_c)
\end{equation}
where, $u, V_c, b_c$ are standard parameters for a neural network, $\oplus$ is the vector concatenation, while the most important parameter is $M^{[1:k]}$, which a tensor layer, the tensor layer is used because of its ability to model multiple interactions in the data, as this will be useful for multiple contexts. They used SGD for the optimization of this objective function.  They achieved really interesting results as shown in \ref{fig:multipleContext}.
\begin{figure}[ht]
    \centering
            \includegraphics[scale=0.7]{images/multipleContext.png} 
    \caption{"Nearest neighbor words model and Skip-
Gram. The first line in each block is the results of Skip-Gram;
and the rest lines are the results of our model" \cite{contextWithTensor}}
    \label{fig:multipleContext}
\end{figure}

\section{Gradient Descent Optimizers} \label{optimizers}
The goal of learning in machine learning is to minimize an objective function  $J(\theta)$, where $\theta$ is the set of all parameters in our model. This happens by updating the parameters $\theta$ at every training time step $t$. We will denote $\theta_{t}$ as the parameters of our model at the $t^{th}$ time step. In this work, we only examine gradient descent algorithms.
\subsection{Gradient Descent}
The idea in gradient descent optimization is to follow the path of steepest descent in the shape of the objective function. To get information about the shape of the objective function, one has to compute the gradients of all our parameters   $\nabla J(\theta_{t})$, where $J$ is our objective function, and $\theta$ all of our parameters, $t$ denotes the time step at which the parameters are taken. We will define $g_{t}$ as the gradients of our parameters $\theta_t$ according to our objective function $J$. To follow the path of steepest descent we will have to subtract a portion of this term from our parameter. The magnitude of the portion is often referred to as the learning rate, denoted as $\eta$. For illustration, this means that the gradients will give the direction of the optimization step, whereas the learning rate will give the amplitude of that step. An update at time step $t$ will result in the following equation:
\begin{equation}
\theta_t = \theta_{t-1} - \eta \nabla g_{t-1}
\end{equation}
Where all the variables can be interpreted as above. This will also be the case for further Equations. 
There are three main variations of Gradient Descent, they differ at the moment they chose to update the parameters.
\subsubsection{Stochastic Gradient Descent (SGD)} 
In this variant the the model will update the parameters after each training sample. The problem with this approach, is that the variance of the direction of the training step will be very high, as each sample will influence the step individually.

\subsubsection{Batch Gradient Descent} 
Here the update of the model happens after having gone through the entire dataset. The problem with this approach is that one updates the model way to infrequently which will lead to a high convergence time.
\subsubsection{Mini-Batch Gradient Descent} 
Update the model after having gone through a specific number of training samples. The idea is that the batch will be representative of the entire dataset. Which will allow the model to learn quickier, as the updates are more frequent (in comparison to batch gradient descent) and less variant (in comparison to SGD).

\subsubsection{Problems with Gradient Descent Algorithms}
SGD, though it's simplicity, is very limited, therefore some issues appear:

\begin{itemize}
\item The learning parameter is yet another hyperparameter to tune, as the optimum setting will largely vary depending on the training task and architecture of the network
\item Learning rate schedules, that diminish the learning rate as the training progresses, are commonly accepted technique to improve accuracy. This schedule is most often set at the beginning of the training, and will be completely independent of the training set. 
\item Every parameter has the same learning rate
\end{itemize}
To tackle those issues numerous advanced optimizers were developed. They will be covered in the next sections. 

\subsection{Momentum}
Momentum is a technique used to address one of SGD weak points. As a matter of fact because SGD can have trouble computing the optimum of objective function that are only steep in one directions. The problem here is that SGD often oscillates in the direction that is not very steep, and only takes small steps in the steep direction. This issue is addressed by SGD with momentum. 

It does so by adding a percentage of the last update vector to the current update vector. By doing so the gradient that goes in the same direction will get bigger (building momentum) and gradients that go in different directions will annul themselves. 
 At the update $t$ we will comppute our update vector $v_t$ the following way:
\begin{equation}
v_t = \gamma v_{t-1} + \eta \nabla J (\theta)
\end{equation}\label{eq:momentum}
Where $v_t$ and $v_{t-1}$, respectively are the current and the last update vector. The value $0.9$ for $\gamma$ has shown great results, but this, same as a learning rate, another hyper parameter that need to be tuned according to the specific task. 
And then we update our weights as usual: $\theta = \theta - v_t$ 
\subsection{Nesterov}
Momentum can be a powerful tool, but sometimes be its own enemy. With Momentum, the learning algorithm often overshoots and blows by the  Optima. Hence it will never converge. This problem was addressed by Nesterov. The idea behind his algorithm is to incorporate the momentum in the computation of our gradients. We will subtract the momentum vector, or just a fraction,  from our parameters before computing the gradients. Therefore we will compute the gradients of the position where we would be with momentum, which will allow us to make a step in a better direction. The computation of the update vector will look the following way:
\begin{equation}
v_t = \gamma v_{t-1} + \eta \nabla J (\theta -  \gamma v_{t-1})
\end{equation}

Where the parameters as the same used with momentum in equation \ref{eq:momentum}. SGD with momentum and Nesterov accelerated gradient (NAG) has shown tremendous results in RNN's. But some of the earlier mentioned problems still remain.  NAG, still treats every parameter the same way. Therefore we need a more complex optimization algorithm, that takes the frequency of a feature into account. Adagrad does just that. 

\subsection{Adagrad}\label{ssec:adagrad}
Adagrad first introduced by \cite{adagrad} is an optimizer that tries to apply different learning rates to different parameters, according to their frequency. The idea is to give very frequent features a small learning rate, and very sparse features a high learning rate. This can be very important for our task of word embeddings, as rare words in the corpus are more important than very frequent ones. As a matter of fact, Pennigton et al. used this algorithm for their training of Glove \cite{Glove}, another word embedding system. \\
Each parameter $\theta_i$, at time step $t$ will have it's own learning rate $\eta_{t,i}$
 \begin{equation}
\eta_{t,i} = \frac{\eta_0}{\sqrt{\sum^{t}_{i=1} g^{2}_{t,i}} \epsilon}
\end{equation}
where$g_{t,i} = \nabla J(\theta_{t,i})$  is the partial derivative of the loss function with respect to the parameter $\theta_i$ at time step $t$, and $\epsilon$ is a place holder, so that one does not divide by $0$, at the beginning of the computation.\\ We see that each parameter $\theta_{i}$ has it's one learning rate. For a very frequent feature the sum of the previous gradients will be very high, hence the learning rate low. This is how Adagrad achieves a different learning rate for each feature. 
Therefore we have $ \theta_{t+1,i} = \theta_{t_i} - \eta_{t,i} g_{t,i} $, and we can now construct our global parameter update as follows: 
\begin{equation}
\theta_{t+1,i} = \theta_{t_i}- \frac{\eta}{\sqrt{G_{t_{i,i}}} + \epsilon} g_{t,i}, 
\end{equation} 

with  $G_{t_{i,i}}$ being the diagonal Matrix of the sum of the squares of the graditents ($g_{t,i} $). 
There lies one weakness in this approach: the sum of the squares of the previous gradients grows constantly. This means that after a certain number of epochs the learning rate will be insufficient, to update the model. This issue was addressed by the Adadelta algorithm, that will be covered in the next session. 

\subsection{Adadelta}\label{ssec:adadelta}
Adadelta \cite{adadelta} not only solves the constantly growing sum problem, but also the fact that one does not have to tune the learning rate by not having one. The gist of Adadelta is that instead of taking all the gradients to compute the sum we will only take a fixed number $w$ of gradients. But instead of inefficiently storing w gradients we will take the exponentially decaying average of the squared gradient, denoted $E[g^{2}]$. The average at time step $t$ will be computed in the following way: 
\begin{equation}
E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t
\end{equation}
where $\gamma$ is a hyperparameter similar to the one used in momentum, that decides how much the past is weighted in contrast to the current gradient. Since Adadelta is an extension of Adagrad the square root of $E[g^{2}]$ is needed which become the Root Mean Squared Error (RMS):
\begin{equation}
RMS[g]_t = \sqrt{E[g^2]_t + \epsilon}
\end{equation}
Which gives us the following update rule: 
\begin{equation}
\theta_t = \theta_{t-1} - \frac{\eta}{RMS[g]_t}g_t
\end{equation}
Here we have two problems, first, the learning rate is still a hyperparameter. And the units do not match. This is a problem xyz wanted to address. That if the parameters would have units the update parameters would have the same parameters. 
Therefore they define the exponentially decaying average of squared parameter updates: 
\begin{equation}
E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t
\end{equation}
As before we can know us the root mean squared error: 
\begin{equation}
RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}
\end{equation}
As at time step $t$ $RMS[\Delta \theta]_{t} $ is unknown, we approximate it with $ RMS[\Delta \theta]_{t}-1$. Know we replace $\eta$ with  $ RMS[\Delta \theta]_{t}-1$ and get the final update rule: 
\begin{equation}
 \theta_{t+1} = \theta_t - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t}
\end{equation}
\subsection{Adam}
Adaptive Moment Estimation (Adam), is a more recent optimization algorithm. It also computes adaptive learning rates. In comparison to Adagrad and Adadelta, it does not only take into considaration the decaying average of the previous squared gradients but also the decay of the past gradients. 
Let's introduce : 
\begin{equation}
m_t = \beta_1 m_{t-1} + (1- \beta_1) g_t 
\end{equation}
as the decaying average of the previous gradients, and 
\begin{equation}
v_t = \beta_2 v_{t-1} + (1- \beta_2) g^2_t 
\end{equation}
 as the decaying average of the previous squared gradients. Where $\beta$ is a hyper parameter similar to $\gamma$ used in the previous optimizers. \\
One problem arises when using this formula, $m_t$  and $tv_t$ are initialized as vectors of zero. Therefore they are biased towards zero. therofore et al. advised to use a bias corrected version:\\
\begin{equation}
\tilde{m_t} = \frac{m_t}{1-\beta^t_1}
\end{equation}
\begin{equation}
\tilde{v_t} = \frac{v_t}{1-\beta^t_2}
\end{equation}
The general update is done exactly in the same way as in Adadelta:
\begin{equation}
\theta_{t+1} = \theta_t - \frac{\eta}{\tilde{v_t}+ \epsilon} \tilde{m_t}
\end{equation}

