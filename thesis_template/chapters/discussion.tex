\chapter{Discussion}\label{chap:discussion}


%Discuss the results. What is the outcome of your experimetns?

In this section we will compare our work to related work, try to make sense out of our results, discuss some challenges faced and finally possible future work. 
\section{Related Work}
In this section we will compare - our work to related work, we will first compare us to the baseline model, the original c implementation from Mikolov et al \cite{Mikolov}. We will then compare our self to Gensim, we do this because Gensim allowed us to easily and fastly compute the embeddings, while having access to the loss, and being a python implementaion made everything easier. 

\section{word2vec}
%TODO ask jorg
word2vec only trained their model on the very large google news datset incorporating more than 3 billio words. Therefore we have a problem to compare our work. But we will make some assumptions. 
 the original paper paer of word2vec they show results from one or 3 epochs. We accord these good results to the very large dataset. And as a matter of fact their result are better with 3 than with 1 epochs. We do not have any informtaion about the convergence. But we can talk about the quality of the word embeddings as Ji et al. reported wordsim ressults on text8 with the original c code, namely 0 .63. This is worse than our results with sgd and adam. we did not find anything that would explain those results as shuffling and wihtout shuffling VERIFY beat the original c code. It is also possible that if we would apply an advanced optimizer to the large dataset we would get a better word embedding as we achieved a wordsim score of 0.67 with adam and gensim only 0.66. 


\section{Gensim}
Gensim does propose the class w2vec with the following possible parameters\footnote{link to gensim params}, we will describe theim, show our setting, and why we chose the following, we will only describe the prametrs we changed from the default value will be explained, the rest can be found in the appendix. 
They will be presented in the following way: \\
\texttt{name} (type) - \textit{Description} - Value
\begin{itemize}

   \item \texttt{sentences} (iterable of iterables) – Dataset - we used the Original text8 document splitted into sentences of length 20
  \item \texttt{ size }(int) – Dimensionality of the word vectors - 100
\item    \texttt{window} (int) – Maximum distance between the current and predicted word within a sentence - 100
  \item  \texttt{min\_count }(int) – Ignores all words with total frequency lower than this - 5
 \item   \texttt{workers} (int) – Use these many worker threads to train the model (=faster training with multicore machines) - 4 
\item    \texttt{sg} ({0, 1}) – Training algorithm: 1 for skip-gram; otherwise CBOW. -1
  \item  \texttt{hs} ({0, 1}) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used. - 0
  \item  \texttt{negative} (int) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used - 10 
\item   \texttt{ ns\_exponent} (float) – Exponent in the unigram distribution, when choosing random samples REFERENCE EQUATION. - 0.75
\item    \texttt{alpha} (float) – The initial learning rate. - 0.025
 \item   \texttt{min\_alpha} (float) – Learning rate will linearly drop to min\_alpha as training progresses. -0.0001

 \item   \texttt{sample} (float) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5). - 1e-4
   \item \texttt{hashfxn} (function) – Hash function to use to randomly initialize weights, for increased training reproducibility. - VERIFY 
  \item  \texttt{iter} (int) – Number of iterations (epochs) over the corpus. - 10 
 \
  \item  \texttt{compute\_loss} (bool) – If True, computes and stores loss value which can be retrieved using get\_latest\_training\_loss(). - True
 \item   \texttt{callbacks} (iterable of CallbackAny2Vec) – Sequence of callbacks to be executed at specific stages during training. - see Appendix
\end{itemize}

\subsection{Gensim vs. SGD}
First we have to note that we cannot compare ourselves to gensim in computation time, this woudl not be very smart as our code is written in python and gensim uses advanced cython routines\footnote{link to tutorial thesis}. 
The first interesting thing to note is that we did not have the same values in convergence time as gensim when using sgd. There are differnet possiblilities why this could be the case. First our batched approach could hinder performance in term of convergence as some overwriting may happen. Another differnce between our implementation is the fact that gensim checks whether negative samples are real negative samples. Thererfore the learning of the input and output context is optimized. 
The first hypothesis may be confirmed by the fact that our input shuffling reduces the number of overwritings and therefore we achieved conv time of 7 epcohs which is closer to 4 epochs. and the last difference maybe explained by the eg samples thing. 

\subsection{Gensim vs. Adam}
The Adam optimizer did outperform the Gensim application in performance (only slighltlly: 0.01 corr coeff advantage) and convergence time. Adam converged in 2 epochs vs. Gensim in 4. The case to be made here is that one would have to look at the computational advantage to calculate the simple sgd vs.  the adam update rule. Another aspect to look at is that we mainly tested our implementation on the text8 dataset. We only experienced once one the enwik9 dataset. Here the argument that can be made is that, some optimizers work better on specific loss functions in comparison to others, hence we need to show that our optimizer words on amultiued on theim and that it is not just a statistical anomalie. 

\section{Challenges faced}
\subsection{Using the wrong embeddings}
\subsection{Too big of a batch size}
\section{Further Work}