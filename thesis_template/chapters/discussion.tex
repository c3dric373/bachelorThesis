\chapter{Discussion}\label{chap:discussion}


%Discuss the results. What is the outcome of your experimetns?

In this section we will compare our work to related work, try to make sense out of our results, discuss some challenges faced and finally possible future work. 
\section{Related Work}
In this section we will compare - our work to related work, we will first compare us to the baseline model, the original c implementation from Mikolov et al \cite{Mikolov}. We will then compare our self to Gensim, we do this because Gensim allowed us to easily and fastly compute the embeddings, while having access to the loss, and being a python implementaion made everything easier. 

\section{word2vec}

\section{Gensim}
Gensim does propose the class w2vec with the following possible parameters, we will describe theim, show our setting, and why we chose the following: 
\begin{itemize}

   \item sentences (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.
 \item   corpus\_file (str, optional) – Path to a corpus file in LineSentence format. You may use this argument instead of sentences to get performance boost. Only one of sentences or corpus\_file arguments need to be passed (or none of them, in that case, the model is left uninitialized).
    size (int, optional) – Dimensionality of the word vectors.
    window (int, optional) – Maximum distance between the current and predicted word within a sentence.
  \item  min\_count (int, optional) – Ignores all words with total frequency lower than this.
 \item   workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).
\item    sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.
  \item  hs ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.
    negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.
\item    ns\_exponent (float, optional) – The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper. More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, \& Royo-Letelier suggest that other values may perform better for recommendation applications.
   \item cbow\_mean ({0, 1}, optional) – If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.
\item    alpha (float, optional) – The initial learning rate.
 \item   min\_alpha (float, optional) – Learning rate will linearly drop to min\_alpha as training progresses.
    seed (int, optional) – Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization).
\item    max\_vocab\_size (int, optional) – Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit.
\item    max\_final\_vocab (int, optional) – Limits the vocab to a target vocab size by automatically picking a matching min\_count. If the specified min\_count is more than the calculated min\_count, the specified min\_count will be used. Set to None if not required.
    sample (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).
   \item hashfxn (function, optional) – Hash function to use to randomly initialize weights, for increased training reproducibility.
  \item  iter (int, optional) – Number of iterations (epochs) over the corpus.
 \item   trim\_rule (function, optional) –

    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary, be trimmed away, or handled using the default (discard if word count < min\_count). Can be None (min\_count will be used, look to keep\_vocab\_item()), or a callable that accepts parameters (word, count, min\_count) and returns either gensim.utils.RULE\_DISCARD, gensim.utils.RULE\_KEEP or gensim.utils.RULE\_DEFAULT. The rule, if given, is only used to prune vocabulary during build\_vocab() and is not stored as part of the model.

    The input parameters are of the following types:
            word (str) - the word we are examining
            count (int) - the word’s frequency count in the corpus
            min\_count (int) - the minimum count threshold.

  \item  sorted\_vocab ({0, 1}, optional) – If 1, sort the vocabulary by descending frequency before assigning word indexes. See sort\_vocab().
 \item   batch\_words (int, optional) – Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)
  \item  compute\_loss (bool, optional) – If True, computes and stores loss value which can be retrieved using get\_latest\_training\_loss().
 \item   callbacks (iterable of CallbackAny2Vec, optional) – Sequence of callbacks to be executed at specific stages during training.

\end{itemize}





\section{Challenges faced}
\section{Further Work}