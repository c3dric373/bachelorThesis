\chapter{Discussion}\label{chap:discussion}


%Discuss the results. What is the outcome of your experimetns?

In this section we will compare our work to related work, try to make sense out of our results, discuss some challenges faced and finally possible future work. 
\section{Related Work}
In this section we will compare - our work to related work, we will first compare us to the baseline model, the original c implementation from Mikolov et al \cite{Mikolov}. We will then compare our self to Gensim, we do this because Gensim allowed us to easily and fastly compute the embeddings, while having access to the loss, and being a python implementaion made everything easier. 

\section{word2vec}
%TODO ask jorg
word2vec only trained their model on the very large google news datset incorporating more than 3 billio words. Therefore we have a problem to compare our work. But we will make some assumptions. 
 the original paper paer of word2vec they show results from one or 3 epochs. We accord these good results to the very large dataset. And as a matter of fact their result are better with 3 than with 1 epochs. We do not have any informtaion about the convergence. But we can talk about the quality of the word embeddings as Ji et al. reported wordsim ressults on text8 with the original c code, namely 0 .63. This is worse than our results with sgd and adam. we did not find anything that would explain those results as shuffling and wihtout shuffling VERIFY beat the original c code. It is also possible that if we would apply an advanced optimizer to the large dataset we would get a better word embedding as we achieved a wordsim score of 0.67 with adam and gensim only 0.66. 


\section{Gensim}
Gensim does propose the class w2vec with the following possible parameters\footnote{link to gensim params}, we will describe theim, show our setting, and why we chose the following: 
\begin{itemize}

   \item \texttt{sentences} (iterable of iterables, optional) – Dataset - we used the Original text8 document splitted into sentences of length 20
 \item  \texttt{ corpus\_file} (str, optional) – None
  \item \texttt{ size }(int, optional) – Dimensionality of the word vectors - 100
\item    \texttt{window} (int, optional) – Maximum distance between the current and predicted word within a sentence - 100
  \item  \texttt{min\_count }(int, optional) – Ignores all words with total frequency lower than this - 5
 \item   \texttt{workers} (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines) - 4 
\item    \texttt{sg} ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW. -1
  \item  \texttt{hs} ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used. - 0
  \item  \texttt{negative} (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used - 10 
\item   \texttt{ ns\_exponent} (float, optional) – The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper. More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, \& Royo-Letelier suggest that other values may perform better for recommendation applications. - 0.75
   \item \texttt{cbow}\_mean ({0, 1}, optional) – If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used. - None
\item    \texttt{alpha} (float, optional) – The initial learning rate. - 0.025
 \item   \texttt{min\_alpha} (float, optional) – Learning rate will linearly drop to min\_alpha as training progresses. -0.0001
    \item seed (int, optional) – Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization). - None 
\item   \texttt{ max\_vocab\_size} (int, optional) – Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit. - None 
\item \texttt{   max\_final\_vocab} (int, optional) – Limits the vocab to a target vocab size by automatically picking a matching min\_count. If the specified min\_count is more than the calculated min\_count, the specified min\_count will be used. Set to None if not required. - None
 \item   \texttt{sample} (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5). - 1e-4
   \item \texttt{hashfxn} (function, optional) – Hash function to use to randomly initialize weights, for increased training reproducibility. - VERIFY 
  \item  \texttt{iter} (int, optional) – Number of iterations (epochs) over the corpus. - 10 
 \item\texttt{   trim\_rule} (function, optional) –Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary, be trimmed away, or handled using the default (discard if word count < min\_count). Can be None (min\_count will be used, look to keep\_vocab\_item()), or a callable that accepts parameters (word, count, min\_count) and returns either gensim.utils.RULE\_DISCARD, gensim.utils.RULE\_KEEP or gensim.utils.RULE\_DEFAULT. The rule, if given, is only used to prune vocabulary during build\_vocab() and is not stored as part of the model.

    The input parameters are of the following types:
            word (str) - the word we are examining
            count (int) - the word’s frequency count in the corpus
            min\_count (int) - the minimum count threshold. - None

  \item  \texttt{sorted\_vocab }({0, 1}, optional) – If 1, sort the vocabulary by descending frequency before assigning word indexes. See sort\_vocab(). - None
 \item   \texttt{batch\_words }(int, optional) – Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.) - None 
  \item  \texttt{compute\_loss} (bool, optional) – If True, computes and stores loss value which can be retrieved using get\_latest\_training\_loss(). - True
 \item   \texttt{callbacks} (iterable of CallbackAny2Vec, optional) – Sequence of callbacks to be executed at specific stages during training. - see Appendix

\end{itemize}

\section{Challenges faced}
\subsection{Using the wrong embeddings}
\subsection{Too big of a batch size}
\section{Further Work}