\chapter{Discussion}\label{chap:discussion}


%Discuss the results. What is the outcome of your experimetns?

In this secftion we will shortly discuss our results then extensively compare our work to related work, try to make sense out of our results, discuss some challenges faced and finally possible future work. 

\section{Our work}
In this section we will quickly discuss our findings, and try to give some explanation to it.  
\subsection{Shuffling and learning rate with SGD}
As shown in figures \ref{fig:results_sgd}, \ref{fig:results_mom} and \ref{fig:results_nag}, the model, when using SGD as an optimizer,  was able to use a higher learning rate when the input was shuffled as when not. Therefore arises the questions why those this phenomen happen. One possibility is that the model is presented with a slightly different loss function every time, which is closer to the optimal loss function, therefore the steps taken by the optimizer are closer to the optimum and can therefore be bigger. 

\subsubsection{Large differences with nag and sgd when using shuffling}
As swhon in figures \ref{fig:results_sgd} and \ref{fig:results_nag}, plain SGD and Nesterov Accelerated gradient, greatly differ in their convergence time when using shufflin in to comparison when not. We attribute these results partially to a good random initialization guess and not only input shuffling. Due to a lack of time these results where not replicate more then once. 

\section{Related Work}
In this section we will compare our work to related work. We will first compare us to the baseline model, the original c implementation from Mikolov et al \cite{Mikolov}. As Mikolov et al. did not use the same datasets as we, this comparison will loose in quality. We therefore compare ourself extensively with Gensim \cite{gensim}, a open source python implementation of the SGNS. Gensim is optimized to have a very high throuhgput which allowed us to, achieve a lot of computaions. Furthermore Gensim gave us access to the loss and the resulting word embedings, which made it easy to compare 

\subsection{word2vec}
%TODO ask jorg

As mikolov et al. published the original paper which introduced the SGNS, it is of course relevant to compare ourselves to theim. The first important thing to take under account is that Mikolov et al. only trained their model on a very large google news dataset incorporating more than 3 billion words. This makes the comparison of our work more difficult. But we will make some assumptions, as they can be interesting. 
In their original paper, Mikolov et al. reported results from computations that took 1 and 3 epochs. We accord these good results to the very large dataset  and furthermore as a matter of fact their result are better with 3 than with 1 epochs. We do not have any informtaion about the convergence. Hence it would be intersting to use their dataset for comparison. \\
One thing we can compare is the quality of our word embeddings. Mikolov et al. did not report any results on their model with the text8 dataset, but they therefore published their code, and Ji et al \cite{intel} tested the model on the text8 dataset. They reported a similarity of 0.63 on the wordsim task. This is obviously outperformed by all our models. We did not find any explanation on why those results differ as much. \\
The final assumption is that an advanced optimizer could maybe outperform SGD in terms of quality on a large data set. This will be discussed in further work.
 


\subsection{Gensim}
Gensim proposes a class \texttt{W2vec}\footnote{param}. The constructor has a lot of possible parameter, an extended list can be found in the appendix. In this section we will only describe the parameters we changed from the default setting.  The description will be presented in the following way: \\
\texttt{name} (type) -- \textit{Description} -- Value\\
Parameters:
\begin{itemize}
   \item \texttt{sentences} (iterable of iterables) -- \textit{Dataset} -- text8 document splitted into sentences of 20 words 
  \item \texttt{size}(int) â€“ \textit{Dimensionality of the word vectors } -- 5
\item \texttt{window} (int) --\textit {Maximum distance between the current and predicted word within a sentence }-- 100
  \item  \texttt{min\_count }(int) --\textit{ Ignores all words with total frequency lower than this }-- 5
 \item   \texttt{workers} (int) -- \textit{ Use these many worker threads to train the model (=faster training with multicore machines) }-- 4 
\item    \texttt{sg} ({0, 1}) --\textit{ Training algorithm: 1 for skip--gram; otherwise CBOW. }--1
  \item  \texttt{negative} (int) --\textit{Number of negative samples}-- 10 
\item   \texttt{ ns\_exponent} (float) --\textit{ Exponent in the unigram distribution, when choosing random samples, as shown in Equation \ref{eq:unigram} }-- 0.75
\item    \texttt{alpha} (float) --\textit{ The initial learning rate. }-- 0.025
 \item   \texttt{min\_alpha} (float) --\textit{ Learning rate will linearly drop to min\_alpha as training progresses. }--0.0001
 \item   \texttt{sample} (float) --\textit{ Treshold for subsampling as described in \ref{eq:sampling}. } -- 1e--4
  \item  \texttt{iter} (int) --\textit{ Number of iterations (epochs) over the corpus. }-- 10 
 \
  \item  \texttt{compute\_loss} (bool) --\textit{If True, loss is stored at the end of each batch}-- True
 \item   \texttt{callbacks} (iterable of CallbackAny2Vec) --\textit{ Set of functions that will be executed at given training times, in order to follow the loss and the progress of the model in word similarity }-- see Appendix
\end{itemize}

\subsubsection{Gensim vs. SGD}
First we have to note that we cannot compare ourselves to gensim in computation time, this woudl not be very smart as our code is written in python and gensim uses advanced cython routines\footnote{link to tutorial thesis}. 
The first interesting thing to note is that we did not have the same values in convergence time as gensim when using sgd. There are differnet possiblilities why this could be the case. First our batched approach could hinder performance in term of convergence as some overwriting may happen. Another differnce between our implementation is the fact that gensim checks whether negative samples are real negative samples. Thererfore the learning of the input and output context is optimized. 
The first hypothesis may be confirmed by the fact that our input shuffling reduces the number of overwritings and therefore we achieved conv time of 7 epcohs which is closer to 4 epochs. and the last difference maybe explained by the eg samples thing. 

\subsubsection{Gensim vs. Adam}
The Adam optimizer did outperform the Gensim application in performance (only slighltlly: 0.01 corr coeff advantage) and convergence time. Adam converged in 2 epochs vs. Gensim in 4. The case to be made here is that one would have to look at the computational advantage to calculate the simple sgd vs.  the adam update rule. Another aspect to look at is that we mainly tested our implementation on the text8 dataset. We only experienced once one the enwik9 dataset. Here the argument that can be made is that, some optimizers work better on specific loss functions in comparison to others, hence we need to show that our optimizer words on amultiued on theim and that it is not just a statistical anomaly.

\begin{figure}[h]
    \centering
			\includegraphics[scale=0.45]{images/gensim_vs_adam} 
    \caption{Training time Stochastic Gradient Descent with input Shuffling}
    \label{fig:gensim_vs_adam}
\end{figure}


\section{Challenges faced}
\subsection{Using the wrong embeddings}
To start our comutation we did choose some wrong, at leat not in coordinance with gensim, initialization embeddings. The context vectors were set between (-1,1) distributed normally. Gensim does it buy putting theim all to 0. As we did the latter the first time our model did not train, but this is possably due to a learning rate that was too low. So we chosed theim to (-1,1). With theim we did not acheive the same similarity as gensim. But when we changed theim back we performed the same results as gensim. Therefore this is a recommendation to future work to not set the initialization to (-1,1)
\subsection{Too big of a batch size}
Another challenge we faced was finding a good batch size. We trained our batch size for a long time with a bs of 5000. But this seemed to worked fine in the beginning but a high lr seemed to work too which was very suspect. We then decided to take a batch size of 2000. Which slightly decreased the learning time, and increased the convergence time and adjusted the learning rate. 
\subsection{Correct loss function}
We expirimented with different loss function as we did not have exactly the same as introduced by mikolov et al. We therefore first ook the average of each score in our tensor. But we then took the sum which seemed to work  better. Is this because the shape of the loss function suits bettter advanced optimizers. Or because it approaches the original loss function more. 


\section{Further Work}
This work only focused on testing the approach of using advanced optimizers and input shuffling to improve the convergence time of the SGNS. While we did show that in theory it could work there dstill needs a bit of work to be done to show that this claim holds consistently. The first thing to do is to use another data set we only tried our approach on only one and a very small dataset. Both of these aspects are problematic. By using a very small dataset we do not use the model in the condition it is mostly needed for as the dataset used in practice a usually 3b words. There is a small argument that can be made for machine translation as the use of small parllel corpus is not unusual . The second one is that some optimizers have shown to perform better on some specific shapes of loss functions. Therefore it could be possible that adam and adagrad only outperform the sgd on this specific loss function for this dataset. Therefore there still needs to be more extensive work done to confirm to confirm our interesting early results. Antoher issue with our implementation is that we didn't care about the throughput of words, so in this matter our implementation is inefficient. One would have to improve one of the already efficient implementation in order to show that we can imporve the convergence time while at the same time maintaining the same convergence time.



