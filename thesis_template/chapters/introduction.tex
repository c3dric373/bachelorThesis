\chapter{Introduction}\label{chap:introduction}

%Insert a short introduction to word2vec. and quickly talk about machine learning blablbablah. 
Representing words as vectors, i.e word embeddings (WE) is a fundamental aspect of Natural Language Processing (NLP). There are two ways to create such WE, either arbitrarly or with the purpose of capturing a semantic meaning, i.e. vector representations of words that ar syntactilly or semantically similar will be near to each other. By capturing semantic or syntactic meaning WE have shown to facilitate a lot of subsequent NLP tasks, such as entity recognition, machine translation or sentence classification.  The first attempt at creating WE with neural networks was mad by Bengio et al. \cite{bengio}, but more recently Mikolov et al. \cite{mikolov} introduced a software package called w2vec that uses a simpler network and produces state of the art results. One of the proposed algorithms in this software package is the Skip-Gram Model (SGM). The SGM is an algorithm, that trains a network, on the task of predicting the neighboring words in a sentence. The weights of this network are then used as WE. 

The SGM gained a lot of attention, as it achieved very good results for a very simplistic model. As a consequence a lot of effort went into optimizing  it. Most of this effort was trying to improve the throughput of the model, i.e the number of words that are processed per second by the model. The SGM uses Stochastic Gradient Descent as its optimization algorithm, and is therefore inheritly sequential. To remedy this problem Mikolov et al. used Hogwild \cite{hogwild}, where different threads can acces the shared model and update it. As this is not an optimal solution Yi et al. \cite{intel} tried to optimize it, by using a mini-batch like approach and converting vector to vector multiplications into matrix matrix multiplications. This yielded two consequences: First the model is updated less frequently leading to less overwriting and offering the possibility to parallelize more. Sencondly it transformed level-1 BLAS operation into level-3 BLAS operations, and the algorithm could therefore effectively use computational resources.

Another attempt at optimizing the throughput was made by Seulki and Youngmin \cite{gpu} who parallelized the update of each dimension of the embeddings on GPU's. We see that a lot of work went into optimizing the throughput of the model. As this is very interesting, not much work went into improving the convergence time of the model. Therefore one could ask himself if the convergence time of the SGM can be optimized by the use of advanced optimizers and input shuffling, while at the same time maintaining its accuracy?  Therefore our work focused on this task.\\

This work will first give a short introduction to the Skip-Gram model, then discuss related work and the numerous successful attempts that went into optimizing the throughput of the model. Then we will describe our implementation of the SGM followed by the description of our results. In the Section Results, we will first describe our dataset, the measure we used to compare the quality of word embeddings. And finally, we will discuss our work by comparing it to the state of the art implementation gensim \cite{gensim}. 















