\chapter{Introduction}\label{chap:introduction}

Insert a short introduction to word2vec. and quickly talk about machine learning blablbablah. 


Machine learning is the science of algorithms the execution of a certain task, for example the classification of images into categories. The goal is that after a training period the model will be able to execute the task. The training is done on a so called training dataset, after sufficient training samples the model will have learned the appropriate task and will be able to perform well on the wanted task. An application for machine learning are word embeddings. Word embeddings are vectors that represent words. This is usually done in a one hot manner. For a vocabulary of size $n$, we will create $n$ vectors with dimension $n$. Each word will recieve an integer as an $id$. And the vector representation of the word will be as follows: every dimension of the vector will be $0$, except for the one wich is equal to the $id$ which will be $1$. There lies a Problem in this approach, the dimensionality of the vector is very big, a typical size of a vocabulary is 3b. This representation does not capture any meaning /. Each vector has the same distance to each other. To tackle this issue, a lot of attempt were meade to create better word embeddings. And these have shown to facilitate a lot of tasks in natural language processing, such as machine translation and senteces classfication. Here there is a little trick at work to create word embeddings, as there is no real task to learn. Therefore the approaches to create word embeddings have used the following trick: Use a fake taks on a neural network, and then take the or some weights from the network and use these weights as embeddings. This was also the technique used by Mikolov et al., which ntroduced the Skip Gram Model, an algorithm to create word embedings. 
This work focused on improving the convergence time of the algorithm. \\
We will first give a short introduction to the Skip-Gram model, then discuss related work, and see that a lot of work has got into optimizeing the throughput of the skip gram model but not in getting a better convergence time. Then we will describe our implementation of the SGNS followed by the description of our results. There we will first describe our dataset, the measure we used to compare the quality of word embedings. And finaly we will discuss our work by comparing it to the state of the art implementation gensim. 

  














