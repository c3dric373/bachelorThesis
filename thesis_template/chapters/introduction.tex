\chapter{Introduction}\label{chap:introduction}

%Insert a short introduction to word2vec. and quickly talk about machine learning blablbablah. 

Machine learning (ML) is a subpart of Artifficial Intelligence (AI), in which models poses the ability to lean to solve a task without having been explicitly programmed to solve this task. Supervised learning is on of the many different ways ML can achieve it's goal. The task of unsupervised learning is to learn a prediction function from labeled training data. We suppose from the training data that it's representative of a larger sample population, this would mean that our trained model generalizes well, i.e the predictive function will give accurate results on unseen data. Neural networks are models used in Supervised learning. Neural networks, will take a vector as an input an return a vector as an output. In between their are the so called hidden layers. What will happen is that given in input each layer will react in a different way whith each other. The goal of ML is to find a way such that the output vector will be closer to the labeled training data. Here their are lot of ways to learn, but gradient learning has imposed itself as the go to optimization technique. The network can be seen as a function with a lot of parameters, at each time step the optimizer will compute the gradient, and update the parameters to make the predictive function more true. An example of a task where this technique is used is Word Embeddings (WE). Word embeddings are vectors that represent words. There are two ways of creating theim, use an AI to create WE, that capture some sense of meaning or create arbitrary ones. Two create arbitrary ones there are two ways of doing so, create a random vector of fixed dimension for each word, or use a one-hot encoding. A one-hot encoding is the following: each word vector will have the length of the numbers of words in the vocabulary, and each vector will hold the value 0 in every dimension except one specific. As this representation does not capture any meaning, because each vector has the same distance to each other, a lot of effort went into  creating better word embeddings. Those efforts have shown to facilitate a lot of tasks in natural language processing (NLP), such as machine translation and sentence classification. The main problem with creating WE is that there is no real task to learn, in comparison to classic ML tasks, therefore the approach is to create a fake task and then use the or some weights of the network as WE. This was also the technique used by Mikolov et al. who  introduced the Skip Gram Model(SGM), an algorithm to create word embedings. The SGM used the task of predicting the probability of a word appearing in the context of another word, what is meant by appearing in the context of another word, is appearing in a certain window next to the word in a sentence. The SGM gained a lot of attention, as it achieved very good results for a very simplistic Model. Therefore a lot of effort was made to optimize it. Most of the optimizing work went into improving the throughput of the model. As this is very interesting, not much work went into improving the convergence time of the model. Therefore our work focused on this task.  \\
We will first give a short introduction to the Skip-Gram model, then discuss related work, and see that a lot of work has got into optimizeing the throughput of the skip gram model but not in getting a better convergence time. Then we will describe our implementation of the SGNS followed by the description of our results. There we will first describe our dataset, the measure we used to compare the quality of word embedings. And finaly we will discuss our work by comparing it to the state of the art implementation gensim. 

  














