\chapter{Results}\label{chap:results}


Describe the experimental setup, the used datasets/parameters and the experimental results achieved

\section{Dataset}
In this implementation we used the following datasets: text8\footnote{matt mahoney} and ewik9\footnote{matt}, these are both repsectively the first 30 and 100MB of clean text from Wikipedia. We chose the first dataset because it is very small, hence giving us a fast computation time, but at the same time used in a lot of related work. \cite{intel} \cite{gpu}. We needed the second dataset to compute the analogy task (more on this in results). 
Both of these datasets contain documents of 10k senteces. Therefore we needed to split the dataset into sentences. We arbitrarly chose a length of 20. Furthermore we applied subsampling (described in the next subsection)  too both of these datasets. We also applied a Statistics regarding the dataset can be found in the following subsection. 
\subsection{Subsampling}
Subsection is a technique introduced by Mikolov et al. in \cite{mikolov}. Certain words that appear very frequently in the dataset such as: "the, as, it, of". Do not give an intrinsic value to the words that appear in it's context. Therefore the idea of subsampling is to delete theim from the dataset. This will increase the computation time and should in theory increase the accuracy of the model. Another interesting fact of subsampling is that words that would not have appeard in the context of the word will now be it. 
Subsampling will be the first step of our preprocing. A word $w$ will be eliminated with the following probability:
\begin{equation}
P(w) = 1- \sqrt{{\frac{t}{f(w)}}}
\end{equation}
where $f(w)$ is the frecquency of w, and $t$ is a treshhold set empirecally. (Mikolov et al.)  recommends a value between $0$ and $1e-5$. We experimented with different values and $1e-4$, seemed the most suted. We did this analysis by looking at and random set of sentences and analyzed the best context pairs. 
\subsubsection{Min count}
We also applied a techniqued, called min count, we deleted every word thatt did not appeared more than 5 times in our dataset. We got this idea from gensim \cite{gensim}. Therer were a few words that had no meaning (aldk, ahdfo) VERIFY in our dataset. and a word that only appears one time will be very dependent on it's original setting which will influence the words tha appear in its context and will therefore not be a true value. Therefore we applied this technique. It should in theory, such as subsampling, improve both the accuracy and computation. 
\subsection{Specification of datasets}
todo add number of subsampled word number of words deleted by min count.
\begin{table}[]
\begin{tabular}{|l|l|l|l|}
\hline
Dataset & Voc size & Numb of sentences (20) & Number of Sentences(length 20) with sampling \\ \hline
text8   & 250k     & 4000                   & 2000                                         \\ \hline
enwik9  & 750k     & 100000                 & 5000                                         \\ \hline
\end{tabular}
\caption{Specifications of the dataset}
\end{table}

\section{Word similarity}
Evaluating word embedding is not an easy task. We cannot split our data set into train and test set. Therefore we need to verify that our embedings are of quality with other techniques. We therefore define the principle of word similarity. We need to define a measure for our word embedings, if they are similar. We will use the cosine distance for this task. The cosine distance of vectors $v$ and $w$ is 1 minus the cosine of the angle between the two vectors. The cosine distance is 0 if two vectors are pointing in the same direction. It's 1 if they are 90 deg aparts from each other and 2 if they are pointing exactly in the opposited direction. It's calculated by taking the dot product of $v$ and $w$ and deviding it by the magnitude of $v$ and $w$ divided by each other. (equation x). We get
\begin{equation}
cos_dis(v,w) =1 - \frac{v \cdot w}{|v| |w|} 
\end{equation}
If we normalize the vectors then their the cosine angle becomes the dot product which allows us to compute the distance much faster using gpu's. 
\subsection{wordsim353}
To rate our word embedings we will need a dataset that gives us the similarity of words. This is exactly what wordsim353 is. It's 353 pairs of words rated by humans on their similarity. The score goes from 10 and 1, 10 being the most similar. We will rank our embedings on the pearson corelation coefficient between the cosine distance and the score attributed by humans. 

\section{Configuration of the network}
The skip-gram model contains a lot of hyper parameters that can be tuned. 
Let's have a look at theim and how they potentially could influence the network
\begin{itemize}
\item Negative Samples: the recommended range in \cite{mikolov} is 10-20. The training accuracy should increase as the sample increases, therefore the computation time should go up. There is a tradeoff to to find. 
\item Context Window: The bigger the window the more training examples the network will have, but if the window is to big the semantic meaning of the window will be erased. The recommended setting was 2-10. 
\item Dimension of the embedding: here the choice is less obvious, in their paper mikolov et al choosed an embedding size of 300. their dataset was pretty huge, with 1 billion words. We therefore choosed a smaller one. 
\item Batch size: more on the batch size in section intel description. Too big of a batch size will lead to too much overwirting and therefor not getting the best resulsts. If we turn dhte batch size down too much we will get a higher computation time. 
\item Alpha: learnng rate. We tried out a variety of lr in the range of 0.00001 to 2 
\end{itemize}

\section{Input Shuffling}
We used input shuffling as a technique to optimize the skip gram model. We will first describe input shuffling in a general way and then explain why we suppose that input shuffling could work well on the skip gram model. 
Let $X = {x_1...x_n}$ be our input data set. Input Shuffling describes the process of taking a random permutation of the dataset as an input at each epoch. 
The idea behind this technique, is the same as the use of mini batches. We want to present our optimizer with different loss surfaces, so that it's able to find the best optimum. But both combined can be a very powerful, there always lies a risk that a mini-batch isn't a good representative of the true gradient. This way, by shuffling the input, one would avoid this bias.
There are two reasons why we thnik that input shuffling is particulary well suited for the skip gram model. The first one has to do with the fact that when we read our words sequentially that words that only appear very early will not benefit from the context words being already updated from others. The second thing is that we used the special batch technique described in section x.  When using this technique and not using shuffling we will always have words that appear next to each other in a batch and will therefore updat similar words at the same time. We then loose some accuracy. But if instead we would use input shuffling then in one batch the words would likely not be similar and therefore overwriting will be less likely. 
One thing to consider is that when using input shuffling we cannot use a sliding window size. 

\section{Convergence time} 
To optimize convergence time we have to define convergence time first. Therefore we used the already available implementation gensim. we know from \cite{intel}. that a score of $0.66$ in the task of word similarity is the state of the art. We also know after testing Gensim ( more on this process in section discussion), that it takes 4 epochs to converge. Therefore we defined the following criteria for convergence: \\
$\rho - \rho_{prev} < 0.009 \vee or \rho = 0.66$ \\
where $\rho$ is the pearson coefficient with the wordsim353 task. 

\section{Results by optimizer}
\subsection{SGD}
The first think we had two manage was finding a correct learning rate for our model. We tried the learning rate from gensim first then expirmanted with different learning rates. As expected a bell curve shape resulted. the optimal setting that we found is $0.0075$. We converged in 11 epochs. 
We then added input shuffling to the equation. We see that for all the learning rates this helped to increase the convergenge time. Our models know converges in 7 epochs. An interesting point to notice is that we achieve this with a higher learning rate. 
\subsection{Momentum and Nesterov}
Momentum and nesterov accelerated gradient both have an additional hyper parameter $\gamma$, that, as described in secction background, defines the importance of the previous gradient taking in the equation. We set $\gamma = 0.9|$ this is the typical value. Let's talk results: Momentum and NAG only slighltly respectively decrease or increasethe convergence time. NAG convergences in 12 epochs while momentum took 8 epochs. 
\subsection{Adagrad}
Adagrad is a very interesting tool for learning word embedding as they decrease the loearning rate for very frequent occuring features, and vice versa for low frequent words. Because words that appear very frequently often do not have a real semantic gain to their context words, it's good to have a low learning rate. this is also confirmed by the results adagrad converges in 4 epochs. When combined with shuffling adagrad only takes 3 epochs to converge. This shows the tendency of the skip gram model to converge faster with input shuffling.
\subsection{Adadelta}
Adadelta was the Advanced optimizer that showed the most disapointing results. Because it didn't has any learning rate to tune, we only did 2 experiments, with and without input shuffling. the results where very much disapointing. Adadelta converged in 10 epochs with shuffling and 7 epochs without. Therefore barely beating sgd. This is interesting as Adadelta is an expansion of Adagrad, and is supposed, in theory to work better then the latter. 
\subsection{Adam}
Adam is the most advanced of all the optimizers used in our experiemnts. Did it therefore yield the best results? Indeed this was the case, as seen in figure 2, Adam converged in 3 epochs without shuflfling and 2 with. This are the best result that we got. 