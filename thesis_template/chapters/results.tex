\chapter{Results}\label{chap:results}


Describe the experimental setup, the used datasets/parameters and the experimental results achieved

\section{Dataset}
In this implementation we used the following datasets: text8\footnote{matt mahoney} and ewik9\footnote{matt}, these are both repsectively the first 30 and 100MB of clean text from Wikipedia. We chose the first dataset because it is very small, hence giving us a fast computation time, but at the same time used in a lot of related work. \cite{intel} \cite{gpu}. We needed the second dataset to compute the analogy task (more on this in results). 
Both of these datasets contain documents of 10k senteces. Therefore we needed to split the dataset into sentences. We arbitrarly chose a length of 20. Furthermore we applied subsampling (described in the next subsection)  too both of these datasets. We also applied a Statistics regarding the dataset can be found in the following subsection. We also deleted every word that did not occur more than 5 times in the dataset. 
\subsection{Subsampling}
Subsection is a technique introduced by Mikolov et al. in \cite{mikolov}. Certain words that appear very frequently in the dataset such as: "the, as, it, of". Do not give an intrinsic value to the words that appear in it's context. Therefore the idea of subsampling is to delete theim from the dataset. This will increase the computation time and should in theory increase the accuracy of the model. Another interesting fact of subsampling is that words that would not have appeard in the context of the word will now be it. 
Subsampling will be the first step of our preprocing. A word $w$ will be eliminated with the following probability:
\begin{equation}
P(w) = 1- \sqrt{{\frac{t}{f(w)}}}
\end{equation}
where $f(w)$ is the frecquency of w, and $t$ is a treshhold set empirecally. (Mikolov et al.)  recommends a value between $0$ and $1e-5$. We experimented with different values and $1e-4$, seemed the most suted. We did this analysis by looking at and random set of sentences and analyzed the best context pairs. 
\subsection{Specification of datasets}
todo add number of subsampled word number of words deleted by min count.
\begin{table}[]
\begin{tabular}{|l|l|l|l|}
\hline
Dataset & Voc size & Numb of sentences (20) & Number of Sentences(length 20) with sampling \\ \hline
text8   & 250k     & 4000                   & 2000                                         \\ \hline
enwik9  & 750k     & 100000                 & 5000                                         \\ \hline
\end{tabular}
\caption{Specifications of the dataset}
\end{table}

\section{Word similarity}
Evaluating word embedding is not an easy task. We cannot split our data set into train and test set. Therefore we need to verify that our embedings are of quality with other techniques. We therefore define the principle of word similarity. We need to define a measure for our word embedings, if they are similar. We will use the cosine distance for this task. The cosine distance of vectors $v$ and $w$ is 1 minus the cosine of the angle between the two vectors. The cosine distance is 0 if two vectors are pointing in the same direction. It's 1 if they are 90 deg aparts from each other and 2 if they are pointing exactly in the opposited direction. It's calculated by taking the dot product of $v$ and $w$ and deviding it by the magnitude of $v$ and $w$ divided by each other. (equation x). We get
\begin{equation}
cos(v,w) =1 - \frac{v \cdot w}{|v| |w|} 
\end{equation}


\section{Configuration of the network}
The skip-gram model contains a lot of hyper parameters that can be tuned. 
Let's have a look at theim and how they potentially could influence the network
\begin{itemize}
\item Negative Samples: the recommended range in \cite{mikolov} is 10-20. The training accuracy should increase as the sample increases, therefore the computation time should go up. There is a tradeoff to to find. 
\item Context Window: The bigger the window the more training examples the network will have, but if the window is to big the semantic meaning of the window will be erased. The recommended setting was 2-10. 
\item Dimension of the embedding: here the choice is less obvious, in their paper mikolov et al choosed an embedding size of 300. their dataset was pretty huge, with 1 billion words. We therefore choosed a smaller one. 
\item Batch size: more on the batch size in section intel description. Too big of a batch size will lead to too much overwirting and therefor not getting the best resulsts. If we turn dhte batch size down too much we will get a higher computation time. 
\item Alpha: learnng rate. We tried out a variety of lr in the range of 0.00001 to 2 
\end{itemize}
