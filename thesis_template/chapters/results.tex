
\chapter{Results}\label{chap:results}


%Describe the experimental setup, the used datasets/parameters and the experimental results achieved

This section will give an overview over the used datasets, the used metric to evaluate our models, the configuration of our model and finally the experimental results achieved.

\section{Dataset}\label{sec:dataset}
In this implementation we mainly used the text8 \footnote{http://mattmahoney.net/dc/enwik8.zip} dataset. We chose this dataset for two reasons. First of all it's a very small dataset, more on exact numbers later, that allowed us to do a lot of computations. Secondly this data set was used in a lot of related work (cite gpu, cpu,caching) hence giving us a very good benchmark. The text8 dataset consists of 1702 lines of 1000 words, with a vocabulary of roughly 63000 words, there is no punctuation in the dataset. Therefore we had to choose between building arbitrary sentences and keeping the dataset as it is. We chose the first option because it gives us a faster computation time, and did not show any significant loss in quality empirically, as shown in Table \ref{table:with_20}. We chose a sentences length of 20. Furthermore we applied a technique called subsampling that reduces the data set size. 
We needed a second more larger dataset to confrim our results.  We therefore chose the enwik9 dataset\footnote{http://mattmahoney.net/dc/enwik9.zip}.This dataset needed more preprocessing as it's plain html. We therefore used the a preprocessing script.\footnote{http://mattmahoney.net/dc/textdata.html, Appendix A}. We also splitted this dataset in sentences of length 20, and applied subsampling. The next section will give an exmplanation on the sampling process.
\begin{table}[]
\centering
\begin{tabular}{|l|l|l|}
\hline
Model            & 20       & 1 Document \\ \hline
Training Time for one batch  & 10min    & 18min      \\ \hline
Convergence Time & 3 epochs & 3 epochs   \\ \hline
Word Similarity  & 0.66     & 0.66       \\ \hline
\end{tabular}
\caption{Training and Convergence time according to choice of the length of sentences in text8 dataset}
    \label{table:with_20}
\end{table}
 
 
\subsection{Subsampling}
Subsampling is a technique introduced by Mikolov et al. \cite{mikolov} to reduce the dataset size while at the same time increasing the quality of the dataset, i.e getting better word embeddings with it. The idea behind sub sampling is that words appear very frequently in the dataset such as: "the, as, it, of" but do not give an intrinsic value to the words that appear in it's context. Therefore the goal of subsampling is to delete such words from the dataset. This will decrease the computation time, as it will reduce the number of training samples, and should in theory increase the accuracy of the model. The increase in accuracy can also be explained by the fact that words that would not have appeared in the context of each other, may know do, because words between have been deleted.
Know the question arises, how one chooses to delete a word. Mikolov et al. chose the following equation to compute the deletion of a word $w$ in the data set:
\begin{equation} \label{eq:sampling}
P(w) = 1- \sqrt{{\frac{t}{f(w)}}}
\end{equation}

where $f(w)$ is the frequency of w, and $t$ is a threshold set empirically. Mikolov et al.  recommends a value between $0$ and $10^{-5}$, depending on the size of the dataset. We experimented with different values and $10^{-4}$ seemed the most suited. We did this by simply looking at a random set of sentences and humanly judging the results. An example of the first sentence with different sampling tresholds can be found in Figure \ref{fig:treshold_examples}. Stats about subsampling can be found in Table \ref{table:treshold}. 
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Sampling Treshhold &  0      &      $ 1^{-1}$&$   1^{-2}$& $1^{-3}     $ &$1^{-4}   $    \\ \hline
Number of words in Dataset    & 16 mio & 15mio & 11 mio & 8mio & 4 mio \\ \hline
\end{tabular}
\caption{Size of preproccessed text8 dataset according to sampling treshold}
\label{table:treshold}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{treshold1}
        \caption{Treshold = $10^{-1}$}
        \label{fig:treshold1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{treshold2}
        \caption{Treshold = $10^{-2}$}
        \label{fig:treshold2}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
     \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{treshold3}
        \caption{Treshold = $10^{-3}$}
        \label{fig:treshold3}
    \end{subfigure}
   \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{treshold4}
        \caption{Treshold = $10^{-4}$}
        \label{fig:treshold4}
    \end{subfigure}
     \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{treshold5}
        \caption{Treshold = $10^{-5}$}
        \label{fig:treshold5}
    \end{subfigure}
    \caption{Example of a sentence with different sampling tresholds}\label{fig:treshold_examples}
\end{figure}

\subsubsection{Min count}
We also deleted every word that did not appear more than 5 times in our dataset. We got this tecnique from gensim \cite{gensim}, that introduced this parameter into their training. This is a good technique because of two reasons: first certain words of our data sets do not appear in a common lexicon (twigleg, melqu), or words from a foreign language (Volksvereinigung), or names and acronyms. Secondly a word that only appears one time in our dataset will be very dependent on it's original initialization as it will only be updated with it's context pairs, which is only a dozen times. Therefore we applied this technique. It should, such as subsampling, in theory improve the quality of the word embeddings and  will decrease computation time. 

\section{Evaluating word embedings}
Evaluating word embedding is not an easy task. We cannot split our data set into train and test set. As the task that the network is learning, is of no interest to us. Therefore we need to verify that our embedding are of quality with other techniques. To define quality we first need to define a measure of similarity between two vectors. Let us introduce the cosine distance for this task. 
\subsection{Cosine distance}
The cosine similarity, this is not the cosine distance, of vectors $v$ and $w$ is the cosine of the angle between the two vectors It can be calculated by taking the dot product of $v$ and $w$ and dividing it by the magnitude of $v$ and $w$ multiplied with each other. We get:
\begin{equation}
cos\_sim(v,w) = \frac{v \cdot w}{|v| |w|} 
\end{equation}
The cosine of 0\textdegree is 1,  it's 0 for two vectors that are orthogonal to each other and vectors that point in the opposite direction will have a cosine of their angle of -1. This is not a good distance measure as -1 is smaller then 0, and therefore two vectors pointing away from each other would be closer then two orthogonal vectors, but by subtracting 1 from the cosine of the angle we can create a good distance measure  between the two vectors. This distance does not take into account any order of magnitude. Hence for our tasks, two vectors will be considered equal if they are of different magnitude but point in the same direction. 
This technique a part from being shown empirically to work very well to measure the quality of word embedding has another advantage. By normalizing the vectors the calculation of the cosine angle becomes the dot product of the two vectors. Which can be computed very fast on modern gpu's. 
Know that we have a measure to compute the similairty of two vectors let us introduce a way to rate the quality of our embedings.


\subsection{Word similarity and wordsim353}
To measure the quality of our word embedding we will need a dataset to compare our results too. We chose  wordsim353\footnote{http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip} for this task, as it's the most used in the related literature. The data set consits of 353 pairs of words rated by humans on their similarity. The similarity score is in the range of 1 and 10, an example for two of such pairs can be found in Figure \ref{fig:ws353_ex}. We will rank our embeddings on the pearson corelation coefficient between the cosine distance and the scores attributed by humans, as again this is what is done in the literature. 
\begin{figure}[ht]
    \centering
			\includegraphics[scale=0.7]{images/wordsim353_example} 
    \caption{Example of pairs and their rating in wordsim353}
    \label{fig:ws353_ex}
\end{figure}

\section{Configuration of the network}
The skip gram model, has a lot of possible parameters, that can be tuned. We experimented with different models, and finally decided for one that we tried to optimize. This section will give a short overview over each parameter, where we will explain the process in which we chose the value of the given parameter. The explanation of the parameters will be structured as follows: 
\texttt{Parameter} - Description and tuning -  \textit{Value}
\begin{itemize}
\item \texttt{Negative Samples} Here we have to find a trade off between, setting the parameter too high which will result in increased accuracy but a longer computation time. For smaller data sets a higher negative samples is often needed. In their original paper Mikolov et al. recommend a value of 5-20. We tested a few values in the range of 5 to 15, as 10 yielded state of the art results we chose this value. - \textit{10}
\item \texttt{Context Window:} The bigger the window the more training examples the network will have, but if the window is to big the semantic meaning of the window will be erased. Mikolov et al. proposed a setting between 2-10, as all our sentences are of size 20 we chose 5. - \textit{5}
\item\texttt{ Dimension of the embedding}: Here the choice is less obvious, the higher the dimension the better the embedding should be.( cite paper dimension vectors talk about gensim ) but when tested on gensim 100 yielded better results than 300 - \textit{100}
\item \texttt{Batch size}: As described in section \ref{ssec:b_SGM}, there is a trade off to find between quality and training time. We first used a batch size of 5000, but then decide after non conclusive results (see \ref{ssec:bs_lf}) that 2000 would be better - \textit{2000}
\item \texttt{Alpha}: learning rate, this hyper parameter was tuned in every optimizer therefore only the range will be indicated - \textit{(1e-5,1)}
\end{itemize}

\section{Input Shuffling}
We used input shuffling as a technique to optimize the skip gram model. We will first describe input shuffling in a general way and then explain why we suppose that input shuffling could work well on the skip gram model. \\
Let $X = {x_1...x_n}$ be our input data set. Input Shuffling describes the process of taking a random permutation of the dataset as an input at each epoch. 
The idea behind this technique, is  to present our optimizer with different loss surfaces, so that it's able to find the best optimum. Therefore it's easier for the neural network to escape a local minimum. As for example if a network had converged to a local minimum after one epoch it could not escape it as all the parameters are the same. But if we change the shape of the loss function, by input shuffling, then their would be a greater probability for the network to escape the local minimum.
\\
There are two reasons why we think that input shuffling is particulary well suited for the skip gram model. The first one has to do with the fact that when we read our words sequentially that words that only appear very early will not benefit from the context words being already updated from others. The second idea is that we used the special batch technique described in Section \ref{ssec:b_SGM}. When using this technique and not using shuffling we will always have words that appear next to each other in a batch and will therefore update the same words at the same time. We therefore loose some quality. But if instead we would use input shuffling then in one batch the words would likely not be similar and therefore only taking the average of a small part of pairs with the same words will be less likely. 
One thing to consider is that when using input shuffling we cannot use a random window size. 

\section{Convergence time} 
To optimize convergence time we have to define it first. Therefore we used the already available implementation Gensim \cite{gensim}. Gensim is an open source software that propoeses an implementation of the SGNS in python. It is also written in cython,therefore it has a fast computation time, but can be used inside a python implementation. Together with the knowledge from Ji et al.\cite{intel} that a score of $0.66$ in the task of word similarity, with the text8 dataset is the state of the art, we tested Gensim (more on this process in Chapter \ref{chap:discussion} and found out  that it took 4 epochs to converge. Therefore we defined the following criteria for convergence: \\
$\rho - \rho_{prev} < 0.009 \vee \rho = 0.66$ \\
where $\rho$ is the pearson coefficient on the wordsim353 task. 
We also stopped computation, if it took more then 20 epochs to converge. 

\section{Results by optimizer}
We ran multiple experiments for each optimizer. This section will only give an overview over the achieved results. Each section will give an explanation over the achieved result with a specific optimizer.

\subsection{SGD}
The first challenge for each optimizer was to find a correct learning rate. As SGD is the optimizer used in Gensim \cite{gensim} we first tried the same learning rate as Gensim \cite{gensim}, and then performed a random search to find a better one. As expected a bell curve shape resulted, a learning rate that is to high leads to diversion and  a learning rate that is two low leads to a training time that is too slow. The best value that we found for the learning rate is $0.0075$. With this setting SGD converged in 11 epochs. The second experiment was to add input shuffling. 
As seen is figure \ref{fig:results_sgd}, for every learning rate the convergence time decreased. Our model, with it's best setting, know converges in only 7 epochs. Another interesting  fact to point out from Figure \ref{fig:results_sgd} is that with input shuffling we achieved better results with higher learning rates. As for learning rates of $0.01$ and $0.025$ we did converge in 11 epochs with input shuffling but did not converge in 20 epochs without it.

\begin{figure}[h]
    \centering
			\includegraphics[scale=0.45]{images/results_sgd_shuffle} 
    \caption{Training time Stochastic Gradient Descent with input Shuffling}
    \label{fig:results_sgd}
\end{figure}
\subsection{Momentum and Nesterov}
Momentum\cite{momentum} and NAG \cite{nag} both have an additional hyper parameter $\gamma$, that, as described in Section \ref{optimizers}, defines the percentage of the previous gradient that will be added to the current gradients. We set $\gamma = 0.9$ as this is the typical value and did not alter it during our experiments. Momentum and Nesterov alone respectively only slightly decrease or increase the convergence time. As the first one optimally converges in 9 epochs and the second one in 13. If we combine these optimizers with input shuffling, interestingly the same phenomena as with plain SGD appear. The convergence time gets better, 8 epochs for Momentum  and 3 epochs for NAG. The phenomena that a higher learning rat yields better results also happens with both of the optimizers. As Momentum does not converge in 20 epochs with a learning rate of 0.002 but does in 8 with input shuffling. 
\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  	\includegraphics[scale=0.3]{images/results_mom_shuffle} 
    \caption{Training time  Momentum with input Shuffling}
    \label{fig:results_mom}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
	\includegraphics[scale=0.3]{images/results_nag_shuffle} 
    \caption{Training time  Nesterov with input Shuffling}
    \label{fig:results_nag}
\end{minipage}
\end{figure}
\subsection{Adagrad}
Adagrad \cite{adagrad} is a very interesting tool for learning word embeddings as it decrease the learning rate for very frequent occuring features, and vice versa (this is explained in detail in Section \ref{ssec:adagrad}). Because words that appear very frequently often do not have a semantic gain that is as important as words that appear less frequently  to their context words, it's good to have a lower learning rate. So in theory Adagrad is particularly well suited for our task. This was confirmed empirically as our model converged in 4 epochs. When combined with shuffling adagrad only took 3 epochs to converge. This shows the tendency of the skip gram model to converge faster with input shuffling and the big impact of having different learning rate for each feature. 
Here it's interesting to notice that a higher learning rate combined with input shuffling did not yield better results then without shuffling. Both of our best results happened with a learning rate of $0.1$, as can be seen in Figure \ref{fig:results_adagrad_shuffle}.
\begin{figure}[h]
    \centering
			\includegraphics[scale=0.45]{images/results_adagrad_shuffle} 
    \caption{Training time Adagrad with input Shuffling}
    \label{fig:results_adagrad_shuffle}
\end{figure}
\subsection{Adadelta}
In theory Adadelta \cite{adadelta} should outperform Adagrad as it's an extension of the former. Because it didn't has any learning rate to tune, we only did 2 experiments, with and without input shuffling. We are aware of the fact thta their are additional hyper parameter by did, for simplicity reason, and because their effect is not as high as the learning rate in other optimizers, decide no to tune it. We left it to it's default value $\rho = 0.9$. This parameter defines the percentage taken when calculating the exponantially decaying average of past gradients, as explained in \ref{ssec:adadelta}Adadelta did not manage to achieve a word similarity of 0.66. It only converged to a similarity of 0.59. It did this in 20 epochs without input shuffling and in 3 with input shuffling, as can be seen in Table \ref{table:results_adadelta}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
Adadelta Model    & Convergence Time & Word similarity \\ \hline
Without Shuffling & 20               & 0.59            \\ \hline
With Shuffling    & 3                & 0.59            \\ \hline
\end{tabular}
\caption{Convergence Time and Quality with Adadelta}
\label{table:results_adadelta}
\end{table}
\subsection{Adam}
Adam is the most advanced of all the optimizers used in our experiments, and did it  yield the best results as seen in Figure \ref{fig:results_adam_shuffle}. Adam converged in 3 epochs without shuffling and 2 with. This are the best result that we got with any optimizer.  It's also intersting to note that as same as with Adagrad it did not react to input shuffling the same way as SGD did. In fact it worked in the opposite direction, as we achieved our best result with input shuffling while having a lower learning rate $0.001$ then we used to achieve the best result without input shuffling $0.05$.
\begin{figure}[h]
    \centering
			\includegraphics[scale=0.45]{images/results_adam_shuffle} 
    \caption{Training time Adam with input Shuffling}
    \label{fig:results_adam_shuffle}
\end{figure}