\chapter{Implementation}\label{chap:impl}
%Describe the method/software/tool/algorithm you have developed here

In this work our goal is to optimize the skip-gram model, we, therefore, implemented our own version of it. As we implemented it in python, computation is slow, we, therefore, altered the original implementation. 
This chapter will illustrate our proceeding. First, it will give a short introduction to PyTorch, and then cover our implementation. In the latter part, we will talk about the modified version of the skip gram model that we used and the challenging parts of our implementation.
\section{PyTorch}
For our implementation, we chose the open source library PyTorch.\footnote{pytorch}. Though it's the simplicity of use it's one of the most used libraries for machine learning. One of the most important features is the calculation of gradients by Pytroch. All gradients are calculated online, therefore there is no need for us to implement the calculation of those gradients. The second important feature that we used, is the large variety of optimizes already implemented in pyTorch. They are proposed in the package \texttt{torch.optim}\footnote{https://pytorch.org/docs/stable/optim.html}. The last important feature of Pytorch that we used are the classes \texttt{Dataset}\footnote{\label{note_data}https://pytorch.org/docs/stable/data.html} and \texttt{Dataloader}\footnotemark[\ref{note_data}]. Both of these classes are meant to work closely together.   The Dataset interface has two functions to offer namely: "\texttt{\_\_len\_\_}" and "\texttt{\_\_getitem\_\_}". Those are then used by a data loader object, that will construct a batch based on those two functions. The loader object will facilitate the training process, as we can simply iterate over it, and get the batches. Furthermore, the data loader can shuffle our dataset before each epoch. 
\section{Implementation}
This section will give an overview of our implementation. First, it will give a broad overview of the implementation idea and process. Then it will go into detail, explaining the forward process of our model and the construction of our dataset, by the use of the \texttt{Dataset} class. 
\subsection{Batched SkipGramModel}\label{ssec:b_SGM}
As we implemented the SGNS in python, computation time was slow. We, therefore, altered the original implementation to get a faster computation, while taking the risk to have slightly less good quality. Our idea was, to compute the loss for multiple words and context pairs at the same time. Therefore some overwriting may happen at the backpropagation. The exact process will be described in the following paragraph. 
\subsubsection{Forwarding}
\textit{Input:}\\
The forwarding method will take two vectors $v$ and $c$, and a matrix $A$ as an input. The first vector represents all the center words in a batch, the second one the context words. The Matrix represents the negative samples. The two vectors need to have the same length, defined as $n$. Our training batch $X$ can be seen as in the following way:  $X = \{(v_i, c_i)\|\i \in \{1, ..., n\}, n = |v| \}$. Where $(v_i,c_i)$ is a context pair. The matrix must be of dimension $n \times k$, with $k$, being the number of negative samples per pair. This means the $i^{th}$ row will store the negative samples for the $i^{th}$ word context pair.\\
\textit{Concatenation of samples:}\\
First, we will concatenate $w$ and our Matrix $A$. This will result in a Matrix $\tilde{A}$.\\
\textit{Embeddings:}\\
 Know we need to get our Embeddings. Therefore we will create a Matrix $E_v$ of dimensionality $n \times d$ where $d$ is the dimension of our word embedding which will store the word embedding of the $i^{th}$ word from our input vector $v$ in it's $i^{th}$ row. We will do the same for the our Matrix $\tilde{A}$ This will result in a $n \times k+1 \times d$ Array $E_v$. \\
 \textit{Batch Multiplication and negation of samples:}\\ 
 To compute the dot product of each word vector with it's pair and the negative samples, exactly as done as in the original loss function of Mikolov et al. shown in Equation \ref{eq:obj_neg_samples}, we will need some definitions: let $A_j$ be the $j^th$ row of the matrix A, know let $E_c(i,j)$ be the $d$ dimensional embedding of the word stored in $\tilde{A}(i,j)$. To know compute the dot product we will do a so-called batch multiplication\footnote{https://pytorch.org/docs/stable/torch.html\#torch.bmm} which will result in a matrix $S$ where $S(i,j) = E_c(i,j) \cdot A_j$. This will result in a $n\times k+1$ Matrix $S$. Know we only have to multiply the last $k$ rows with minus one. The sum of each row represents the loss computed in Equation \ref{eq:obj_neg_samples}, for each word context pair. But this is not what we are wishing to do as the computation time would be too long, therefore we construct our own loss function. \\
\textit{Loss function:}\\
We sum the resulting matrix of and multiply it with $-1$  hence we get a less for our entire batch, as some words may appear more then once in the batch this will more be an average of it then as with Mikolov et al the exact update per pair. \\
The process can be best understood with an example, let our training batch be:\\ $X = {(v_1,c_1),(v_2,c_2),(v_3,c_3)}$
Input:\\
 $v = \begin{bmatrix}
v_1 & v_2 & v_3
\end{bmatrix}, c = \begin{bmatrix}
c1\\
c2\\
c3\end{bmatrix}$ and $A = 
\begin{bmatrix}
k_{1,1} & k_{2,1} & k_{3,1}\\
k_{1,2} & k_{2,2} & k_{3,2}\\
k_{1,3} & k_{2,3} & k_{3,3}\\
\end{bmatrix}$\\

 We then concatenate $c$ and $A$, 
resulting in: \\
$\tilde{A} = \begin{bmatrix}
c_1 & k_{1,1} & k_{2,1} & k_{3,1}\\
c_2 & k_{1,2} & k_{2,2} & k_{3,2}\\
c_3 & k_{1,3} & k_{2,3}& k_{3,3}\\
\end{bmatrix}$

Embeddings:\\
$E_v = \begin{bmatrix}
\tilde{v_1}_1 & \ldots & \tilde{v_1}_d\\
\tilde{v_2}_1 & \ldots & \tilde{v_2}_d\\
\tilde{v_3}_1 & \ldots & \tilde{v_3}_d\\
\end{bmatrix}
$, where $\tilde{v_i} = \begin{bmatrix}
\tilde{v_i}_1 & \ldots & \tilde{v_i}_d \end{bmatrix}$ is the embedding of $v_i$.  \\$E_c = \begin{bmatrix}
\tilde{c_1 }& \tilde{k_{1,1}} & \tilde{k_{2,1}} \\
\tilde{c_2 }& \tilde{k_{1,2}}& \tilde{k_{2,2}} \\
\tilde{c_3 }&\tilde{ k_{1,3} }& \tilde{k_{2,3}}\\
\end{bmatrix}$,
where each entry of the matrix is a vector of dimension $d$\\

Batch multiplication and negation of samples:\\
$S = \begin{bmatrix}
\tilde{v_1} \cdot  \tilde{c_1} & -\tilde{v_1} \cdot \tilde{k_{1,1}} & -\tilde{v_1} \cdot  \tilde{k_{2,1}}& -\tilde{v_1} \cdot  \tilde{k_{3,1}}\\
\tilde{v_2} \cdot \tilde{c_2} & -\tilde{v_2} \cdot \tilde{k_{1,2}} & -\tilde{v_2} \cdot \tilde{k_{2,2}} & -\tilde{v_2} \cdot \tilde{k_{3,2}}\\
\tilde{v_3} \cdot \tilde{c_3} &-\tilde{v_3} \cdot c_3  \tilde{k_{1,3}} & -\tilde{v_3} \cdot c_3 \tilde{k_{2,3}}&-\tilde{v_3} \cdot \tilde{k_{3,3}}\\
\end{bmatrix}$\\

Loss computation: \\
 L = $- \sum_{(i,j) \in k \times n} S(i,j) $
 
As we now have a way to compute our loss, we need to access the context-pairs, therefore we also had to create our own way of doing it. We will explain the process in the next paragraph.

\subsection{Creating the context pairs}
We needed to provide a way for the \texttt{dataloader} to access each word context pair. The straight forward way would be to go once over the whole dataset and create a list that stores all those pairs. The Problem with this approach is that it would not be possible to do so for a very large dataset. The amount needed to store a list of all the possible pairs for a small dataset (text8 dataset, more on the dataset in Section \ref{sec:dataset}) is roughly 4GB.  As some used datasets are 100x bigger in practice, this is not a suitable solution. Therefore we propose a way to compute the $i^{th}$ word-context pair of the dataset, by only storing the dataset in the RAM. Here we had two tasks, compute the number of possible pairs in our dataset, and given an index $i$ return the wanted pair. Both of these task were respectively done in the methods \texttt{\_\_len\_\_} and \texttt{\_\_getitem\_\_}.

\subsubsection{Number of pairs in the dataset}
For the first task, calculating the number of pairs in the dataset, we knew that every sentence except the last one had a length of 20 words (more on this in Section \ref{sec:dataset}). As the number was fixed we only needed to compute once the number of pairs in a sentence of length 20. This was done in the following way. We will distinguish two types of words in a sentence: first center words, those have the maximum amount of possible context words, and the border words that do not have this property because they are too close to the start or end of the sentence.
We know that every center word has exactly 2 times the amount of the window size as context words. To compute the number of border pairs in a sentence one only has to subtract the length of the sentence with the double of the window size. The border pairs can be computed by the following equation. 
\begin{equation}
\sum_{i=0}^{ctx\_window -1} ctx\_window + i
\end{equation}
Finally, we have to compute the number of pairs in the last sentence, here the challenge is to compute it if the sentence is shorter then twice the length of our window because then the equation described previously does not work. A pseudo-code description can be found in Algorithm \ref{alg2}. We will iterate over the length and add each time step we will compute the number of context pairs the given word has. First, we check if the index is smaller then our window. If this is the case we have to distinguish one special case, namely if our sentence is smaller then where we would be if we added the maximum number of the window. If this is the case we add the length of our sentence -1 to the number of pairs (line 4). A short explanation: Instinctively one would first add the $j$ pairs that are left of our word, and then compute the number of pairs right of the word. For this one would do $len\_last\_sen -1 -j$ because len\_last\_sen -1  is the index of the last word in our sentence and to get the number of words between the last and the current $j^{th}$ word one needs to subtract $j$. Therefore the number of pairs is $len\_last\_sen -1 -j + j = len\_last\_sen -1$.\\ Then we have to check if our word is to close to the end of the last sentence (line 7) to add the context pairs. Here we apply the same procedure as above, add the window amount context pairs from the left, we can do this because we know that $j \geq window$ from line2, we also need to add the words right to the current word, therefore, we take the same difference as before: $len\_last\_sen -1 -j$.
The final case is simply calculating the number of pairs per center word if there are any. \\
Know we can simply add the number of pairs in our last sentence to the previously computed number, and return the number of pairs in our dataset.

%TODO think and give comment about random window size
\begin{algorithm}[h]
\caption{Computing the number of pairs in the last sen}
\label{alg2}
\begin{algorithmic}[1]
\FOR{ $j=0$ \TO len\_last\_sen -1}
\IF{$j<window$}
\IF{$j+window \geq len\_last\_sen$}
\STATE pairs\_last\_sen += len\_last\_sen-1
\ELSE
\STATE pairs\_last\_sen += j+window
\ENDIF
\ELSIF {$j \geq len\_last\_sen - window$}
\STATE pairs\_last\_sen += len\_last\_sen -1 -j + window
\ELSE
\STATE pairs\_last\_sen += 2*window
\ENDIF
\ENDFOR
\RETURN pairs\_last\_sen
\end{algorithmic}
\end{algorithm}

 \subsubsection{Accessing each pair individually}
Now we need to provide a way for the dataloader to access each pair individually, a pseudo code description can be found in \ref{alg1}. 
Given an item index $idx$ we need to find in which sentence the pair is. Because we know how many pairs there are in each sentence, except the last one, we only have to divide $idx$ by the number of pairs that can be built within one sentence (line2). This division also holds true for the last sentence, as it's the only one with a different number of words in it and is the last one.  Once this is done we have access to the sentence where our pair is located. (line3). We have to find the index of our pair within our sentence. We know the number of pairs in all sentence before our sentence (this also holds true if our sentence is the last one), therefore we can subtract the number of pairs that are in all the sentences before our sentence from $idx$, and will get the index of our pair within it's sentence (line4). Once this is done we only have to iterate over all the possible pairs in our sentence keep count and return when we find the correct pair (line 6-15).
\begin{algorithm}
\caption{Getting the context pair from the id}
\label{alg1}
\begin{algorithmic}[1]

\STATE n\_pairs\_in\_sen = border\_pairs + center\_pairs
 \STATE id\_sen = $\lfloor \frac{idx}{n\_pairs\_in\_sen} \rfloor$
\STATE  sen  = dataset[id\_sen]
\STATE  pair\_id\_in\_sen = $idx - id\_sen*(n\_pairs\_in\_sen)$
\STATE counter = 0

\FOR{$i=0$ \TO $len\_sen $}
\FOR{$j=0$ \TO $window$}
\IF{$i+j < len\_sen $}
\IF{counter == pair\_id\_sen}
\RETURN (word2idx[sen[i]], word2idx[sen[i+j]])
\ENDIF
\STATE counter += 1
\ENDIF
\IF{$i-j \geq 0$}
\IF{counter == pair\_id\_sen}
\RETURN (word2idx[sen[i]], word2idx[sen[i-j]])
\ENDIF
\STATE counter += 1
\ENDIF
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Adaptability to other datasets}
The above described implementation is limited to datasets formatted as ours, namely having all sentences except the last of the same length. But our model can be quickly modified to achieve the same results on any given dataset. To compute the length of the dataset one would have to use Algorithm \ref{alg1} to compute the length of each sentence, and take the sum over all the length of each sentence. To access the context-pairs the challenge lies in getting the id of the sentences. Therefore one would have to go over the dataset and compute the numbers of pairs for each sentence, sum them up and wait until the sum is greater than the searched pair. Once this is done the algorithm stays the same. 

