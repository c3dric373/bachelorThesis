\chapter{Methods}\label{chap:methods}
Describe the method/software/tool/algorithm you have developed here

We developped a  full replication of the original w2vec implementation of the Skip Gram Model in Pytorch. And then tried to optimize it. This chapter will illustrate our proceeding. First it will give a short introduction to PyTorch, and then cover the most interesting parts of our implementation. 
\section{PyTorch}
For our implementation we choosed the open source library PyTorch. It offers a few  implementation features that enormlhy facilitates the implemenatation process. First of all it computes the gradient of the loss function online, therofore there is no need for us to implement the calcluation of htose gradients. For the learning ew will only have to use the already implemented optimizers, that can be found in the module torch.optim. Another feature of Pytorch that we used are the classes dataset and dataloader. Both of these classes works closely together.   The dataset has to function to offer namely: "__len__" and " __getitem__". Those are then used by a data loader object. it will construct a batch based on those two function. The loader object has a very nice feature it can shuffle the datsaet before each epochs. 
\section{Implementation}
\subsection{class SkipGramModel}
This class is responsable for the forward pass of our data, and represents the neural network, that will result in the word embeddings. We used the class torch.embeddings to generate our word embedings. This class allows us to give a single integer to the embeding and it will return our embedding.  
\subsubsection{Method forward()}
. We will take as an input a number of word and their context and then use the same negative samples for all of those words. We will create a matrix made out contex words and samples. Create matrix containing all our words. Multiply the two matrices. this will result in a matrix having each score of equation \ref{obj_neg_samples}. And we will then sum all our scores to return our loss. 
Let's examine an example: $(w_1,c_1)(w_2,c_2)(w_3,c_3)$ be our batch. 
Therefore $pos_u = \begin{bmatrix}
w_1 & w_2 & w_3
\end{bmatrix}, pos_v = \begin{bmatrix}
c1\\
c2\\
c3\end{bmatrix}$ and $neg_v = 
\begin{bmatrix}
k_{1,1} & k_{2,1} & k_{3,1}\\
k_{1,2} & k_{2,2} & k_{3,2}\\
k_{1,3} & k_{2,3} & k_{3,3}\\
\end{bmatrix}$\\
 We then concatenate $pos_v$ and $neg_v$, while negating $neg_v$
resulting in: \\
$samples = \begin{bmatrix}
c_1 & -k_{1,1} & -k_{2,1} & - k_{3,1}\\
c_2 &- k_{1,2} & -k_{2,2} & -k_{3,2}\\
c_3 & -k_{1,3} & -k_{2,3}& - k_{3,3}\\
\end{bmatrix}$

We then multiply $pos_u$ and $samples$ resulting in: \\
$scores = \begin{bmatrix}
w_1 \cdot c_1 & -w_1 \cdot k_{1,1} & -w_1 \cdot  k_{2,1} & -w_1 \cdot  k_{3,1}\\
w_2 \cdot c_2 & -w_2 \cdot k_{1,2} & -w_2 \cdot k_{2,2} & k_{3,2}\\
w_3 \cdot c_3 &-w_3 \cdot c_3  k_{1,3} & -w_3 \cdot c_3 k_{2,3}& k_{3,3}\\
\end{bmatrix}$
Finnally we sum up the score and multiply it with minus one to make it a minimizing problem: \\
 $-(\sum_{i=1}^3 w_i \cdot c_i - \sum_{j=0}^3 -w_i \cdot -k_i,j)$
\subsection{class Dataset}
data loader, pairs neg samples etc.
\subsection{class W2Vec}
tst
\subsubsection{method train\_with\_loader}
training

