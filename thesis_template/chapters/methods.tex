\chapter{Methods}\label{chap:methods}
Describe the method/software/tool/algorithm you have developed here

In this work we replicated, the skip gram models in a python implementation. We did modify it to achieve a faster computation. The following task, was then to try to optimize it. This chapter will illustrate our proceeding. First it will give a short introduction to PyTorch, and then cover our implementation. In the latter part we will talk about the modified version of the skip gram model that we used and the interesting parts of our implementation
\section{PyTorch}
For our implementation we chosed the open source library PyTorch.\footnote{pytorch}. Thruogh it's simplicity of use it's one of the most used libraries for machine learning. One of the most important features is the calculation of gradients by Pytroch. All gradients are calculated online, therofore there is no need for us to implement the calcluation of those gradients. The second important feature that we used, is the large variety of optimizes already implemented in pyTorch. They are proposed in the package torch.optim. This means that we did not need to change anything in our implementation except for one line. The last important feature of Pytorch that we used are the classes \texttt{Dataset} and \texttt{Dataloader}. Both of these classes are meant to work closely together.   The Dataset interfacea has to function to offer namely: "\texttt{\_\_len\_\_}" and " \texttt{\_\_getitem\_\_}". Those are then used by a data loader object. it will construct a batch based on those two function. The loader object has a very nice feature it can shuffle the dataset before each epoch. 
\section{Implementation}
\subsection{Batched SkipGramModel}
We implemented the model in the \texttt{Model} interface from torch.model. We used two embeddings as they are proposed in torch.embedding. they do esactly what we need, given an integer they return a vector. Hence they can be seen as the look up matrix. The real interesting part of our implementation is the way we modified the loss function to get a faster computation. 
We wanted to achieve a faster computation therefore we  had the idea to use a kind of a batch. The idea is to update the weights of different vectors. 
\subsubsection{Forwarding}
The forwarding method will take two vectors $v$ and $c$, and a matrix $A$ as an input. The first vector represents all the center words in a batch, the second one the context words. The Matrix represents the negative samples. The two vectors need to have the same length $n$. Our training batch $X$ can be seen as in the following way:  $X = \{(v_i, c_i)\|\i \in \{1, ..., n\}, n = |v| \}$. Where $(v_i,c_i)$ is a context pair. The matrix must be of dimension $|w|\times k$, with $k$, being the number of negative samples per pair.  First we will concatenate $w$ and our Matrix $A$. This will result in a Matrix $\tilde{A}$.  Know we need to get our Embedings. Therefore we will create a Matrik $E_v$ of dimensionality $n \times d$ where $d$ is the dimension of our word embeding which will store the word embeding of the $i^{th}$ word from our input vector $v$ in it's $i^{th}$ row. We will do the same for the our Matrix $\tilde{A}$ This will result in a $n \times k+1 \times d$ Array $E_v$. To compute the dot product of each word vector with it's pair and the negative samples we only have to compute $E_v E_u$. This will result in a $n\times k+1$ Matrix $S$. Know we only have to multply the last $k$ rows with minus one.  We then sum the resulting matrix of and multiply it with $-1$  and we get the wanted loss. It is best understand with an example. 

Let's examine an example: let $(v_1,c_1)(v_2,c_2)(v_3,c_3)$ be our batch. \\
Input:\\
 $v = \begin{bmatrix}
v_1 & v_2 & v_3
\end{bmatrix}, c = \begin{bmatrix}
c1\\
c2\\
c3\end{bmatrix}$ and $A = 
\begin{bmatrix}
k_{1,1} & k_{2,1} & k_{3,1}\\
k_{1,2} & k_{2,2} & k_{3,2}\\
k_{1,3} & k_{2,3} & k_{3,3}\\
\end{bmatrix}$\\
 We then concatenate $c$ and $A$, 
resulting in: \\
$\tilde{A} = \begin{bmatrix}
c_1 & k_{1,1} & k_{2,1} & k_{3,1}\\
c_2 & k_{1,2} & k_{2,2} & k_{3,2}\\
c_3 & k_{1,3} & k_{2,3}& k_{3,3}\\
\end{bmatrix}$

Embeddings:\\
$E_v = \begin{bmatrix}
\tilde{v_1}_1 & \ldots & \tilde{v_1}_d\\
\tilde{v_2}_1 & \ldots & \tilde{v_2}_d\\
\tilde{v_3}_1 & \ldots & \tilde{v_3}_d\\
\end{bmatrix}
$, where $\begin{bmatrix}
\tilde{v_i}_1 & \ldots & \tilde{v_i}_d \end{bmatrix}$ is the embedding of $v_i$.  \\$E_c = \begin{bmatrix}
\tilde{c_1 }& \tilde{k_{1,1}} & \tilde{k_{2,1}} \\
\tilde{c_2 }& \tilde{k_{1,2}}& \tilde{k_{2,2}} \\
\tilde{c_3 }&\tilde{ k_{1,3} }& \tilde{k_{2,3}}\\
\end{bmatrix}$,
where each entry of the matrix is a vector of dimensionality $d$\\
Batch multiplication \footnote{torch.bmm}:\\
$S = \begin{bmatrix}
v_1 \cdot c_1 & -v_1 \cdot k_{1,1} & -v_1 \cdot  k_{2,1} & -v_1 \cdot  k_{3,1}\\
v_2 \cdot c_2 & -v_2 \cdot k_{1,2} & -v_2 \cdot k_{2,2} & -v_2 \cdot k_{3,2}\\
v_3 \cdot c_3 &-v_3 \cdot c_3  k_{1,3} & -v_3 \cdot c_3 k_{2,3}&-v_3 \cdot k_{3,3}\\
\end{bmatrix}$\\
Loss computation: \\
 L = $-(\sum_{i=1}^3 v_i \cdot c_i - \sum_{j=0}^3 -v_i \cdot -k_i,j)$

\subsection{Creating the context pairs}
This class should provide a way for the dataloader to access each word context pair. Here the challenge is that one cannot store all the pairs in a list as this would not be feasble for a large dataset. We therefore propose a way to compute th i-th word-context pair of the dataset. we therofore had to implement the len method. Here we knew that every sentence expcept the last would have a length of 20 words. therefore it was easy to compute the number of pairs in those sentences.
We will distinguish two types of wordes in the sentences: first center words, those have the maxiumum amount of possible context words. The border pairs are the words that do not have this proprety because they are two close to the start or end of the sentence. The amount of context pairs per center pair is easy to compute, it's $2*ctx\\_window$. Therefore we get that for sentences with length $LEN\\_SEN$, with $LEN\_SEN  > ctx\\_window*2$:
\begin{lstlisting}[language=python]
number\_center\_pairs = ((LEN\_SEN - \\ ctx\_window*2)*self.ctx\_window*2)
\end{lstlisting}
We then need to compute the amount of borde words. For this we will only need to compute the following:
\begin{equation}
\sum\_{i=0}^{ctx\\_window -1} ctx\\_window + i
\end{equation}
 Then we had to add the number of pairs from the last sentence and are done. First  we have to check if the sentence is longer then $2 ctx\_window$. 
 If it's the case we can apply the samDe principle as shown above. If not we need to do the following:\\
 %TODO \\
Now we need to access each pair individually, so first we have to define an order. We will go over the Dataset sequentially adding first the pairs from the left then from the right. Now that we have defined an order, let's look at how we can acces theim. Given an item index $x$ we need to find in which sentence the pairs is. Because we know how many pairs per sentence their is we can. We only have to devide $x$ by the number of pairs that can be built within one sentence. Know we can access this sentence because we have the list that stores our dataset. then we have to iterate over all the possible pairs that we can builld within this sentence count theim, and stop when we arrived at the wished id. S

