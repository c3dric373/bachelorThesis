\chapter{Methods}\label{chap:methods}
%Describe the method/software/tool/algorithm you have developed here

In this work our goal is to optimize the skip-gram model, we therefore implemented our own version of it. As we implemented it in python, computation is slow, we therefore altered the original implementation. 
This chapter will illustrate our proceeding. First it will give a short introduction to PyTorch, and then cover our implementation. In the latter part we will talk about the modified version of the skip gram model that we used and the challenging parts of our implementation.
\section{PyTorch}
For our implementation we chose the open source library PyTorch.\footnote{pytorch}. Thruogh it's simplicity of use it's one of the most used libraries for machine learning. One of the most important features is the calculation of gradients by Pytroch. All gradients are calculated online, therofore there is no need for us to implement the calculation of those gradients. The second important feature that we used, is the large variety of optimizes already implemented in pyTorch. They are proposed in the package \texttt{torch.optim}\footnote{https://pytorch.org/docs/stable/optim.html}. The last important feature of Pytorch that we used are the classes \texttt{Dataset}\footnote{\label{note_data}https://pytorch.org/docs/stable/data.html} and \texttt{Dataloader}\footnotemark[\ref{note_data}]. Both of these classes are meant to work closely together.   The Dataset interface has two functions to offer namely: "\texttt{\_\_len\_\_}" and "\texttt{\_\_getitem\_\_}". Those are then used by a data loader object, that will construct a batch based on those two functions. The loader object will facilitate the training process, as we can simply iterate over it, and get the batches. Furthermore the data loader can shuffle our dataset before each epoch. 
\section{Implementation}

\subsection{Batched SkipGramModel}
We implemented the model in the \texttt{Model} interface from torch.model. We used two embeddings as they are proposed in torch.embedding. they do esactly what we need, given an integer they return a vector. Hence they can be seen as the look up matrix. The real interesting part of our implementation is the way we modified the loss function to get a faster computation. 
We wanted to achieve a faster computation therefore we  had the idea to use a kind of a batch. The idea is to update the weights of different vectors. 
\subsubsection{Forwarding}
The forwarding method will take two vectors $v$ and $c$, and a matrix $A$ as an input. The first vector represents all the center words in a batch, the second one the context words. The Matrix represents the negative samples. The two vectors need to have the same length $n$. Our training batch $X$ can be seen as in the following way:  $X = \{(v_i, c_i)\|\i \in \{1, ..., n\}, n = |v| \}$. Where $(v_i,c_i)$ is a context pair. The matrix must be of dimension $n \times k$, with $k$, being the number of negative samples per pair.  First we will concatenate $w$ and our Matrix $A$. This will result in a Matrix $\tilde{A}$.  Know we need to get our Embedings. Therefore we will create a Matrik $E_v$ of dimensionality $n \times d$ where $d$ is the dimension of our word embeding which will store the word embeding of the $i^{th}$ word from our input vector $v$ in it's $i^{th}$ row. We will do the same for the our Matrix $\tilde{A}$ This will result in a $n \times k+1 \times d$ Array $E_v$. To compute the dot product of each word vector with it's pair and the negative samples we will need some definitions: let $A_j$ be the $j^th$ row of the matrix A, know let $E_c(i,j)$ be the $d$ dimensional embeding of the word stored in $\tilde{A}(i,j)$. To know compute the dot product we will do a so called batch multiplication\footnote{torch.bmm} which will result in a matrix $S$ where $S(i,j) = E_c(i,j) \cdot A_j$. This will result in a $n\times k+1$ Matrix $S$. Know we only have to multply the last $k$ rows with minus one.  We then sum the resulting matrix of and multiply it with $-1$  and we get the wanted loss. It is best understand with an example. 

Let's examine an example: let $(v_1,c_1)(v_2,c_2)(v_3,c_3)$ be our batch. \\
Input:\\
 $v = \begin{bmatrix}
v_1 & v_2 & v_3
\end{bmatrix}, c = \begin{bmatrix}
c1\\
c2\\
c3\end{bmatrix}$ and $A = 
\begin{bmatrix}
k_{1,1} & k_{2,1} & k_{3,1}\\
k_{1,2} & k_{2,2} & k_{3,2}\\
k_{1,3} & k_{2,3} & k_{3,3}\\
\end{bmatrix}$\\

 We then concatenate $c$ and $A$, 
resulting in: \\
$\tilde{A} = \begin{bmatrix}
c_1 & k_{1,1} & k_{2,1} & k_{3,1}\\
c_2 & k_{1,2} & k_{2,2} & k_{3,2}\\
c_3 & k_{1,3} & k_{2,3}& k_{3,3}\\
\end{bmatrix}$

Embeddings:\\
$E_v = \begin{bmatrix}
\tilde{v_1}_1 & \ldots & \tilde{v_1}_d\\
\tilde{v_2}_1 & \ldots & \tilde{v_2}_d\\
\tilde{v_3}_1 & \ldots & \tilde{v_3}_d\\
\end{bmatrix}
$, where $\tilde{v_i} = \begin{bmatrix}
\tilde{v_i}_1 & \ldots & \tilde{v_i}_d \end{bmatrix}$ is the embedding of $v_i$.  \\$E_c = \begin{bmatrix}
\tilde{c_1 }& \tilde{k_{1,1}} & \tilde{k_{2,1}} \\
\tilde{c_2 }& \tilde{k_{1,2}}& \tilde{k_{2,2}} \\
\tilde{c_3 }&\tilde{ k_{1,3} }& \tilde{k_{2,3}}\\
\end{bmatrix}$,
where each entry of the matrix is a vector of dimension $d$\\

Batch multiplication and negation of samples:\\
$S = \begin{bmatrix}
\tilde{v_1} \cdot  \tilde{c_1} & -\tilde{v_1} \cdot \tilde{k_{1,1}} & -\tilde{v_1} \cdot  \tilde{k_{2,1}}& -\tilde{v_1} \cdot  \tilde{k_{3,1}}\\
\tilde{v_2} \cdot \tilde{c_2} & -\tilde{v_2} \cdot \tilde{k_{1,2}} & -\tilde{v_2} \cdot \tilde{k_{2,2}} & -\tilde{v_2} \cdot \tilde{k_{3,2}}\\
\tilde{v_3} \cdot \tilde{c_3} &-\tilde{v_3} \cdot c_3  \tilde{k_{1,3}} & -\tilde{v_3} \cdot c_3 \tilde{k_{2,3}}&-\tilde{v_3} \cdot \tilde{k_{3,3}}\\
\end{bmatrix}$\\

Loss computation: \\
 L = $- \sum_{(i,j) \in k \times n} S(i,j) $
 
As we know have a way to compute our loss, we need to access the context pairs, therefore we also had to create our own way of doing it. We will explaine the process in the next paragraph.

\subsection{Creating the context pairs}
We needed to provide a way for the dataloader to access each word context pair. The straight forward way would be to go once over the whole dataset and create a list that stores all those pairs. The Problem with this approach is that it would not be possible to do so for a very large dataset. As one would get a list of over 10 billion entries, therefore we propose a way to compute th $i^{th}$ word-context pair of the dataset. Here we had two tasks, compute the number of possible pairs in our dataset, and given an index $i$ return the wanted pair. Both of these task were respectively done in the methods \texttt{\_\_len\_\_} and \texttt{\_\_getitem\_\_}.
For the first task we knew that every Senetece except the last one had a length of 20 words (more on this in Chapter \ref{chap:results}). 
\subsubsection{Number of pairs in dataset}
 As the number was fixed we only needed to compute once the number of pairs in a sentece of length 20. This was done in the following way. We will distinguish two types of words in a sentence: first center words, those have the maxiumum amount of possible context words, and the border words that do not have this property because they are two close to the start or end of the sentence.
We know that every center word has exactly 2 times the amount of the window size as context words. To compute the the number of border pairs in a sentence one only has to substract the length of the sentence with the double of the window size. The border pairs can be computed by the following equation. 
\begin{equation}
\sum_{i=0}^{ctx\_window -1} ctx\_window + i
\end{equation}
Finally we have to compute the number of pairs in the last sentence, here the challenge is to compute it if the sentence is shorter then twice the length of our window, because then the equation described previously does not work.A pseudo code description can be found in Algorithm \ref{alg2}. We will iterate over the length and add each time step we will compute the number of context pairs the given word has. First we check if the idex is smaller then our window. If this is the case we have to distinguish one special case, namely if our sentence is smaller then where we would be if we added the maximum number of window. If this is the case we add the length of our sentence -1 to the number of pairs (line 4). A short explanation: Instinctively one would first add the $j$ pairs that are left of our word, and then compute the number of pairs right of the word. For this one would do $len\_last\_sen -1 -j$, because len\_last\_sen -1  is the index of the last word in our sentence and to get the amount of words between the  last and the current $j^{th}$ word one needs to substract $j$. Therefore the number of pairs is $len\_last\_sen -1 -j + j = len\_last\_sen -1$.  Then we have to check if our word is to close to the end of the last sentence (line 7) to add the context pairs. Here we apply the same procedure as above, add the window amount context pairs from the left, we can do this because we know that $j \geq window$ from line2, we also need to add the words right to the current word therefore we take the same difference as before: $len\_last\_sen -1 -j$.
The final case is simply calculating the amount of pairs per center word, if there is any. 
\begin{algorithm}
\caption{Computing the number of pairs in the last sen}
\label{alg2}
\begin{algorithmic}[1]
\FOR{ $j=0$ \TO len\_last\_sen -1}
\IF{$j<window$}
\IF{$j+window \geq len\_last\_sen$}
\STATE pairs\_last\_sen += len\_last\_sen-1
\ELSE
\STATE pairs\_last\_sen += j+window
\ENDIF
\ELSIF {$j \geq len\_last\_sen - window$}
\STATE pairs\_last\_sen += len\_last\_sen -1 -j + window
\ELSE
\STATE pairs\_last\_sen += 2*window
\ENDIF
\ENDFOR
\RETURN pairs\_last\_sen
\end{algorithmic}
\end{algorithm}
 
 \subsubsection{Accessing each pair individually}
Now we need to access each pair individually,a pseudo code description can be found in \ref{alg1}.
Given an item index $idx$ we need to find in which sentence the pair is. Because we know that how many pairs their is in each sentence, excpet the last one this is not a Problem. We only have to devide $idx$ by the number of pairs that can be built within one sentence (line2). Once this is done we have access to the sentence where our pair is located. (line3). We have to find the id of our pair within our sentence. We know the number of pairs in all sentence before our sentence(this also holds true if our sentence is the last one), we can substract the number of pairs that are in all the snetnces before our sentence. (line4). Once this is done we only have to iterate over all the possible pairs in our sentence and keep count and return when we find the good pair (line 6-15).
\begin{algorithm}
\caption{Getting the context pair from the id}
\label{alg1}
\begin{algorithmic}[1]

\STATE n\_pairs\_in\_sen = border\_pairs + center\_pairs
 \STATE id\_sen = $\lfloor \frac{idx}{n\_pairs\_in\_sen} \rfloor$
\STATE  sen  = dataset[id\_sen]
\STATE  pair\_id\_in\_sen = $idx - id\_sen*(n\_pairs\_in\_sen)$
\STATE counter = 0

\FOR{$i=0$ \TO $len\_sen $}
\FOR{$j=0$ \TO $window$}
\IF{$i+j < len\_sen $}
\IF{counter == pair\_id\_sen}
\RETURN (word2idx[word], word2idx[sen[i+j]])
\ENDIF
\STATE counter += 1
\ENDIF
\IF{$i-j \geq 0$}
\IF{counter == pair\_id\_sen}
\RETURN (word2idx[word], word2idx[sen[i-j]])
\ENDIF
\STATE counter += 1
\ENDIF
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

