\begin{frame}\frametitle{Background}\framesubtitle{Softmax}
\begin{Large}
It's unsuitable to compute the softmax
\end{Large}
   \begin{equation}
  p(c|w) =  \frac{exp( {v^{'}_c}^\intercal v_w )}{\sum_{i=1}^T exp({v^{'}_i}^\intercal v_{w})}
   \end{equation}
   \begin{itemize}
   \item $v_w$ and $v^{'}_c$ are the "input" and "output" representation of $w$
\item For each pair we have to go over the whole training corpus. (Billions of word in practice) 
   \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Network Architecture} 
\begin{columns}
    \column{0.6\textwidth}
\begin{itemize}
\item Dimension of input and output vectors = 100
\item Context window  = 5
\end{itemize}
    \column{0.6\textwidth}
    \begin{itemize}
    \item Negative Samples = 10 
\item Coded in Pytorch 1.0
\end{itemize}
  \end{columns}
  \bigskip
\begin{columns}
    \column{0.4\textwidth}
    		First 10 pairs of training:
        \includegraphics[scale=0.35]{images/pairs_example}    \column{0.6\textwidth}
        Negative Samples:\\
        \includegraphics[scale=0.35]{images/neg_samples_example}
  \end{columns}
\end{frame}

\begin{frame}
Each parameter $\theta_i$, at time step $t$ will have it's own learning rate $\eta_{t,i}$
 \begin{equation}
\eta_{t,i} = \frac{\eta_0}{\sqrt{\sum^{t}_{i=1} g^{2}_{t,i}} \epsilon}
\end{equation}
where
\begin{itemize}
\item $g_{t,i} = \nabla J(\theta_{t,i})$  is the partial derivative of the loss function with respect to the parameter $\theta_i$ at time step $t$. 
\item each parameter $\theta_{i}$ has it's one learning rate
\item  $ \theta_{t+1,i} = \theta_{t_i} - \eta_{t,i} g_{t,i} $
\end{itemize}
We can now construct our global parameter update as follows: 
\begin{equation}
\theta_{t+1,i} = \theta_{t_i}- \frac{\eta}{\sqrt{G_{t_{i,i}}} + \epsilon} g_{t,i}, 
\end{equation} 
with  $G_{t_{i,i}}$ being the diagonal Matrix of the sum of the squares of the graditents ($g_{t,i} $).
\end{frame}

\begin{frame}\frametitle{Background}\framesubtitle{Softmax}
\centerline{
\includegraphics[scale=0.5]{images/skip_gram_example.pdf}}
\end{frame}