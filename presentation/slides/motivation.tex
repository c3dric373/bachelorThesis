
\section{Motivation}
\begin{frame}\frametitle{Motivation}
\textbf{How can we encode vectors for machine learning? }\\
\bigskip 
\centerline{
He =$
\left(
\begin{array}{c}
\rowcolor{green! 20}
1\\
0\\
0\\
\end{array}\right)
$
is = $
\left(
\begin{array}{c}
0\\
\rowcolor{blue! 20}
1\\
0\\
\end{array}\right)
$
King =
$
\left(
\begin{array}{c}
0\\
0\\
\rowcolor{red! 20}
1\\
\end{array}\right)
$}
\bigskip
\begin{itemize}
    \item \texttt{PROBLEMS:}
    \begin{itemize}
    \item All vectors have the same distance to each other
    \item Very high dimension 
    \end{itemize}
\end{itemize}
\end{frame}
\begin{frame}\frametitle {Motivation}
    \framesubtitle{Why are word embeddings necessary?}
    \textbf{We need a new system to create word embeddings }
      \begin{itemize}
 \item $\Rightarrow $Skip-Gram Model  (Mikolov et al. 2013) \cite{mikolov}
 \item Low dimension
 \item Captures meaning
 \item Speeds up and improves other NLP tasks, for example machine translation. 
 \end{itemize}
  \end{frame}
